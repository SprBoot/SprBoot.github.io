<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>日常操作工具类库</title>
      <link href="/2025/08/30/ri-chang-cao-zuo-gong-ju-lei/"/>
      <url>/2025/08/30/ri-chang-cao-zuo-gong-ju-lei/</url>
      
        <content type="html"><![CDATA[<h3 id="日常操作工具类库">日常操作工具类库</h3><ul><li><p>HDF5文件格式内容查看</p><pre class=" language-language-python"><code class="language-language-python">import h5pydef print_hdf5_structure(hdf_file, indent=0):    """递归打印 HDF5 文件的结构和内容"""    for key in hdf_file.keys():        item = hdf_file[key]        print(' ' * indent + f"{key}: {type(item)}")        if isinstance(item, h5py.Group):            # 递归调用来遍历组            print_hdf5_structure(item, indent + 4)        elif isinstance(item, h5py.Dataset):            print(' ' * (indent + 4) + f"数据集内容: {item[:]}")  # 读取整个数据集            print(' ' * (indent + 4) + f"数据集形状: {item.shape}")            print(' ' * (indent + 4) + f"数据类型: {item.dtype}")        # 替换为你的 HDF5 文件路径file_path = '/Volumes/Untitled/episode_6.hdf5'with h5py.File(file_path, 'r') as hdf_file:    print("HDF5 文件的结构和内容：")    print_hdf5_structure(hdf_file)</code></pre></li><li><p>相机离线标定</p><pre class=" language-language-python"><code class="language-language-python">#coding:utf-8import cv2import numpy as npimport globdef get_cam2global_matrix(rvec, tvec):    # 将旋转向量转换为旋转矩阵    R, _ = cv2.Rodrigues(rvec)    # 创建 4x4 齐次变换矩阵    cam2global = np.eye(4)    cam2global[:3, :3] = R  # 旋转部分    cam2global[:3, 3] = tvec.flatten()  # 平移部分    return cam2global# 找棋盘格角点# 阈值criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)#棋盘格模板规格w = 11h = 8# 世界坐标系中的棋盘格点,例如(0,0,0), (1,0,0), (2,0,0) ....,(8,5,0)，去掉Z坐标，记为二维矩阵objp = np.zeros((w*h,3), np.float32)objp[:,:2] = np.mgrid[0:w,0:h].T.reshape(-1,2)# objp = 0.9 * objp# 储存棋盘格角点的世界坐标和图像坐标对objpoints = [] # 在世界坐标系中的三维点imgpoints = [] # 在图像平面的二维点images = glob.glob('./robot_camera/*.jpg')i=0for fname in images:    img = cv2.imread(fname)    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)    # 找到棋盘格角点    ret, corners = cv2.findChessboardCorners(gray, (w,h),None)    # 如果找到足够点对，将其存储起来    if ret == True:        cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)        objpoints.append(objp)        imgpoints.append(corners)        # 将角点在图像上显示        cv2.drawChessboardCorners(img, (w,h), corners, ret)        i+=1# 标定ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)print(len(imgpoints))print("ret:", ret)print("mtx:\n", mtx) # 内参数矩阵print("dist:\n", dist)  # 畸变系数   distortion cofficients = (k_1,k_2,p_1,p_2,k_3)print("rvecs:\n", rvecs)  # 旋转向量  # 外参数print("tvecs:\n", tvecs ) # 平移向量  # 外参数rvecs = [np.random.randn(3, 1) for _ in range(len(images))]tvecs = [np.random.randn(3, 1) for _ in range(len(images))]# 遍历每一组 rvec 和 tvec，将其转换为 cam2global 的 4x4 变换矩阵for i in range(len(rvecs)):    rvec = np.float32(rvecs[i])    tvec = np.float32(tvecs[i])    # 将旋转向量转换为旋转矩阵    rotation_matrix, _ = cv2.Rodrigues(rvec)    # 构建4x4的齐次变换矩阵 cam2global    cam2global = np.eye(4)    cam2global[:3, :3] = rotation_matrix  # 旋转部分    cam2global[:3, 3] = tvec.ravel()      # 平移部分    print(f"Transform matrix for image {i+1}:\n", cam2global)# print("-----------------------------------------------------")# # 去畸变# img2 = cv2.imread("./15-22-31.png")# h,  w = img2.shape[:2]# newcameramtx, roi=cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),0,(w,h)) # 自由比例参数# dst = cv2.undistort(img2, mtx, dist, None, newcameramtx)# print("newcameramtx:\n",newcameramtx)# cv2.imwrite('calibresult.png',dst)# # 反投影误差# total_error = 0# for i in range(len(objpoints)):#     imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)#     error = cv2.norm(imgpoints[i],imgpoints2, cv2.NORM_L2)/len(imgpoints2)#     total_error += error# print("total error: ", total_error/len(objpoints))</code></pre></li><li><p>大模型远程调用Restful接口</p><pre class=" language-language-python"><code class="language-language-python">import requestsimport jsonimport cv2from PIL import Imageimport subprocessimport base64# 定义请求的URLurl = "http://10.18.37.92:8000/v1/chat/completions"# 本地将图片信息转换为base64编码上传image_url = './circle.png'f = open(image_url, 'rb')  # 二进制方式打开图文件image = base64.b64encode(f.read())# 定义请求体data = {    "model": "glm-4v",    "messages": [{"role": "user", "content": [{"type": "text", "text": "Now you are a robotic arm. The get_names_on_table() function is used to get the names of all objects on the table and returns a collection. The variable is defined as nams. The get_box_postion() function obtains the positions where all objects on the table need to move. The variable is defined as position, which is fixed. The get_location_by_name(name) function gets the current location of each object. The move_tool(xyz) function is used to move the robotic arm. The grasp() function is used to grasp objects. The ungrasp() function is used to drop objects. Analyze the picture and design the plan method to move the objects on the desktop to the same position on the other side according to the provided function. Please only provide the python code implementation of the plan method without outputting relevant comments."}, {"type": "image_base64",                                                                                      "image_base64": {                                                                                          "base64": image.decode('utf-8')}}]}],    "max_tokens": 1024,    "temperature": 0.5}# 将字典转换为JSON格式headers = {'Content-Type': 'application/json'}data_json = json.dumps(data)# 发送POST请求response = requests.post(url, data=data_json, headers=headers)llm_generated_func = None# 检查响应状态码if response.status_code == 200:    # 如果响应成功，打印响应内容    # print(response.json()['choices'][0]['message']['content'].replace('<|endoftext|>',''))    llm_generated_func = response.json()['choices'][0]['message']['content'].replace('<|endoftext|>','')else:    # 如果响应失败，打印错误信息    print(f"Error: {response.status_code}, {response.text}")execute="./tasks/exec_llm_code.py"subprocess.run(['python', execute, llm_generated_func.replace("```python", "").replace("```", "").strip()])# 采集到的图片以文件形式存储，rest api进行调用# 两种模式，一种是直接以服务形式暴露图片接口# 一种是客户端解析图片，以base64编码形式上传</code></pre></li><li><p>将视频转换为帧图像</p><pre class=" language-language-python"><code class="language-language-python">import osimport cv2def convert_videos_to_images(video_folder_path):    # 检查目录是否存在    if not os.path.exists(video_folder_path):        print("目录不存在。")        return    # 遍历目录中的所有文件    for filename in os.listdir(video_folder_path):        # 检查文件是否为视频        if filename.endswith(".mp4") or filename.endswith(".avi") or filename.endswith(".mov"):            video_path = os.path.join(video_folder_path, filename)            # 加载视频            cap = cv2.VideoCapture(video_path)            if not cap.isOpened():                print(f"无法打开视频：{filename}")                continue            # 根据视频名称创建图片保存目录            video_name = os.path.splitext(filename)[0]            image_folder_path = os.path.join(video_folder_path, video_name)            if not os.path.exists(image_folder_path):                os.makedirs(image_folder_path)            frame_count = 0            # 从视频中读取帧            while True:                ret, frame = cap.read()                if not ret:                    break                # 将帧保存为图片                image_filename = f"{video_name}_{frame_count}.jpg"                image_path = os.path.join(image_folder_path, image_filename)                cv2.imwrite(image_path, frame)                frame_count += 1            cap.release()    print("视频到图片的转换完成。")# 使用示例video_folder_path = "//Users/apple/Documents/"convert_videos_to_images(video_folder_path)</code></pre></li><li><p>RealSense相机与Yolo探距</p><pre class=" language-language-python"><code class="language-language-python">import cv2import numpy as npimport pyrealsense2 as rsfrom ultralytics import YOLOpipeline = rs.pipeline()config = rs.config()config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)  # 配置depth流config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)  # 配置color流pipe_profile = pipeline.start(config)align = rs.align(rs.stream.color)def get_aligned_images():    frames = pipeline.wait_for_frames()    aligned_frames = align.process(frames)    aligned_depth_frame = aligned_frames.get_depth_frame()    aligned_color_frame = aligned_frames.get_color_frame()    intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics    img_color = np.asanyarray(aligned_color_frame.get_data())    return intrin, img_color, aligned_depth_frameif __name__ == '__main__':    model = YOLO("yolov8l.pt")    try:        while True:            intrin, img_color, aligned_depth_frame = get_aligned_images()            source = [img_color]            results = model.predict(source, save=False, show_conf=False)            for result in results:                boxes = result.boxes.xywh.tolist()                im_array = result.plot()                for i in range(len(boxes)):                    ux, uy = int(boxes[i][0]), int(boxes[i][1])                    dis = aligned_depth_frame.get_distance(ux, uy)                    camera_xyz = rs.rs2_deproject_pixel_to_point(                        intrin, (ux, uy), dis)                    camera_xyz = np.round(np.array(camera_xyz), 3)                    distance = np.sqrt(camera_xyz[0] ** 2 + camera_xyz[1] ** 2 + camera_xyz[2] ** 2)                    camera_xyz = list(camera_xyz)                    cv2.circle(im_array, (ux, uy), 4, (255, 255, 255), 5)  # 标出中心点                    cv2.putText(im_array, f'Distance: {distance:.2f}m', (ux + 20, uy + 10), 0, 0.5,                                [225, 255, 255], thickness=1, lineType=cv2.LINE_AA)  # 标出坐标            cv2.namedWindow('detection', flags=cv2.WINDOW_NORMAL |                                               cv2.WINDOW_KEEPRATIO | cv2.WINDOW_GUI_EXPANDED)            cv2.resizeWindow('detection', 640, 480)            cv2.imshow('detection', im_array)            if cv2.waitKey(1) & 0xFF == ord('q'):                cv2.destroyAllWindows()                pipeline.stop()                break    finally:        pipeline.stop()</code></pre></li><li><p>UMI数据处理</p><p>可用视频生成xlsx文件</p><pre class=" language-language-python"><code class="language-language-python"># show_pkl.pyimport pandas as pdimport picklepath = './dataset_plan.pkl'  # path='/root/……/aus_openface.pkl'   pkl文件所在路径f = open(path, 'rb')data = pickle.load(f)data_ = {'name': []}print(len(data))for i in data:    camera = i['cameras']    name_ = camera[0]['video_path'].split('/')[0]    name = camera[1]['video_path'].split('/')[0]    data_['name'].append(name_ + ',' + name)df = pd.DataFrame(data_)# 写入 Excel 文件df.to_excel('name.xlsx', index=False)</code></pre><p>移动可用视频到新文件夹下，剔除不可用视频</p><pre class=" language-language-python"><code class="language-language-python">import pandas as pdimport osimport shutil# 定义读取 Excel 文件并移动文件夹的函数def process_excel_and_move_folders(excel_file, folder_path, target_directory):    # 读取 Excel 文件    df = pd.read_excel(excel_file)    # 假设 Excel 文件中只有一列，且列名为 'name'    if 'name' not in df.columns:        print("Excel 文件中没有名为 'name' 的列")        return    # 遍历每一行，并以逗号分割获取名称    for index, row in df.iterrows():        names = row['name'].split(',')        for name in names:            name = name.strip()  # 去掉空格            source_folder = os.path.join(folder_path, name)            if os.path.exists(source_folder) and os.path.isdir(source_folder):                # 移动文件夹到目标目录                destination_folder = os.path.join(target_directory, name)                print(f"移动文件夹: {source_folder} 到 {destination_folder}")                shutil.move(source_folder, destination_folder)  # 移动目录到新位置            else:                print(f"文件夹不存在: {source_folder}")# 定义主函数if __name__ == "__main__":    excel_file = './name.xlsx'  # 替换为你的 Excel 文件路径    folder_path = '/home/gongcheng/Downloads/example_demo_session/demos'  # 替换为你的文件夹路径    target_directory = '/home/gongcheng/Downloads/sure'  # 目标文件夹路径    # 确保目标文件夹存在    os.makedirs(target_directory, exist_ok=True)    process_excel_and_move_folders(excel_file, folder_path, target_directory)</code></pre></li><li><p>L515相机对应的python版本库</p><p>pip install pyrealsense2==2.54.2.5684</p></li><li><p>ubuntu上传文件跳过已有文件</p><p>rsync -avzu --progress /home/jack/run/clothes/ &nbsp; <a href="mailto:gongcheng@10.18.39.244">gongcheng@10.18.39.244</a>:/home/gongcheng/</p></li><li><p>ubuntu挂载NAS共享文件夹</p><p>sudo mount -t cifs <a href="https://192.168.1.105/robot_data" target="_blank" rel="noopener">//192.168.1.105/robot_data</a> your_path -o username=username,<a href="http://password=dgc123.com/" target="_blank" rel="noopener">password=</a>password,uid=1000</p></li><li><p>ubuntu启动Alas</p><pre class=" language-language-bash"><code class="language-language-bash">sudo docker run -itd --rm --privileged --pull always -v /dev:/dev -v ~/data:/data -e REDROID_EXTRA_SYS_PROPERTIES="ro.hardware.wifi=1 ro.radio.noril=1 wifi.interface=wlan1 wlan.interface=wlan1" -p 5555:5555 redroid/redroid:12.0.0_64only-latest androidboot.redroid_width=1280 androidboot.redroid_height=720 androidboot.redroid_dpi=480</code></pre><pre class=" language-language-bash"><code class="language-language-bash">sudo modprobe binder_linux devices="binder,hwbinder,vndbinder"sudo modprobe ashmem_linux</code></pre><pre class=" language-language-bash"><code class="language-language-bash">adb connect 10.118.23.53:5555</code></pre><pre class=" language-language-bash"><code class="language-language-bash">cd /home/jack/scrcpy/nohup scrcpy -s 10.118.23.53:5555 &</code></pre><pre class=" language-language-bash"><code class="language-language-bash">cd /home/jack/AzurLaneAutoScript/nohup podman-compose up &</code></pre></li><li><p>Docker重新封装镜像</p><pre class=" language-language-bash"><code class="language-language-bash">docker commit 0d4c63a1bf61 ubuntu_20.04_act:v1.0docker save -o ubuntu_20.04_act_v1.tar ubuntu_20.04_act:v1.0</code></pre></li><li><p>D405相机查看图像</p><pre class=" language-language-python"><code class="language-language-python">import pyrealsense2 as rsimport numpy as npimport cv2pipeline = rs.pipeline()config = rs.config()config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 15)  # 10、15或者30可选，其他分辨率可以参考官方datasheetconfig.enable_stream(rs.stream.infrared, 1, 640, 480, rs.format.y8, 15)config.enable_stream(rs.stream.infrared, 2, 640, 480, rs.format.y8, 15)config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 15)profile = pipeline.start(config)# Getting the depth sensor's depth scale (see rs-align example for explanation)depth_sensor = profile.get_device().first_depth_sensor()depth_scale = depth_sensor.get_depth_scale()clipping_distance_in_meters = 1  # 1 meterclipping_distance = clipping_distance_in_meters / depth_scale# Create an align object# rs.align allows us to perform alignment of depth frames to others frames# The "align_to" is the stream type to which we plan to align depth frames.align_to = rs.stream.coloralign = rs.align(align_to)try:    while True:        frames = pipeline.wait_for_frames()        # Align the depth frame to color frame        # 一般彩色图和深度图需要配准，所以数据从aligned_frames读出来        aligned_frames = align.process(frames)        # Get aligned frames        aligned_depth_frame = aligned_frames.get_depth_frame()  # aligned_depth_frame is a 640x480 depth image        if not aligned_depth_frame:            continue        depth_frame = np.asanyarray(aligned_depth_frame.get_data())        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_frame, alpha=0.03), cv2.COLORMAP_JET)        cv2.imshow('1 depth', depth_colormap)        # color frames        color_frame = aligned_frames.get_color_frame()        video_profile = rs.video_stream_profile(color_frame.get_profile())        instrinsics = video_profile.get_intrinsics()        fx = instrinsics.fx        fy = instrinsics.fy        cx = instrinsics.ppx        cy = instrinsics.ppy        print("Width:", instrinsics.width)        print("Height:", instrinsics.height)        print("PPX:", instrinsics.ppx)        print("PPY:", instrinsics.ppy)        print("FX:", instrinsics.fx)        print("FY:", instrinsics.fy)        print("Distortion Model:", instrinsics.model)        print("Coefficients:", instrinsics.coeffs)        # 如果需要打印其他内参 可以去官方文档查看instrinsics的具体参数，以及其他配置文件        if not color_frame:            continue        color_frame = np.asanyarray(color_frame.get_data())        cv2.imshow('2 color', color_frame)        # 读取左右眼图片，可能是为了通信方便的原因，原先的rgb和红外图像都会以灰度图的形式传输        # left　frames        left_frame = frames.get_infrared_frame(1)        if not left_frame:            continue        left_frame = np.asanyarray(left_frame.get_data())        cv2.imshow('3 left_frame', left_frame)        # right frames        right_frame = frames.get_infrared_frame(2)        if not right_frame:            continue        right_frame = np.asanyarray(right_frame.get_data())        cv2.imshow('4 right_frame', right_frame)        c = cv2.waitKey(1)        # 如果按下ESC则关闭窗口（ESC的ascii码为27），同时跳出循环        if c == 27:            cv2.destroyAllWindows()            breakfinally:    # Stop streaming    pipeline.stop()</code></pre></li><li><p>D405设置相机白平衡</p><p>白平衡值为3600</p></li><li><p>Docker启动镜像参数</p><pre class=" language-language-bash"><code class="language-language-bash">#!/bin/bashxhost + docker run -it \--net=host \--env="DISPLAY" \--env="QT_X11_NO_MITSHM=1" \-v /tmp/.X11-unix:/tmp/.X11-unix \--security-opt apparmor:unconfined \--gpus all \-v /home:/home \-v /dev:/dev \--privileged=true \--name="ros2_ubuntu2404" \ubuntu_24.04_ros2:v1.0 /bin/bash</code></pre></li><li><p>Python查看话题频率</p><pre class=" language-language-python"><code class="language-language-python">        current_time = time.time()        if self.prev_time is not None:            dt = current_time - self.prev_time            if dt > 0:                self.frame_rate = 1.0 / dt                print(f"Current frame rate: {self.frame_rate:.2f} Hz")        self.prev_time = current_time</code></pre></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具身智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>π0复现</title>
      <link href="/2025/08/30/p0-fu-xian/"/>
      <url>/2025/08/30/p0-fu-xian/</url>
      
        <content type="html"><![CDATA[<h3 id="π0复现">π0复现</h3><ul><li><p>π0论文</p><p><a href="https://arxiv.org/html/2410.24164v1" target="_blank" rel="noopener">pi0.pdf</a></p></li><li><p>π0代码仓库</p><p><a href="https://github.com/Physical-Intelligence/openpi.git" target="_blank" rel="noopener">GitHub - Physical-Intelligence/openpi</a></p></li><li><p>π0论文解读</p><ul><li><p>多模态PaliGemma2</p><p>PaliGemma 是一个开放的视觉语言模型（VLM），基于 SigLIP–So400m 视觉编码器和 Gemma-2B 语言模型</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/pi0_image.png" alt="image.png"></p><p><img src="https://i-blog.csdnimg.cn/direct/44f146a005dc431bb1b73cf8a22adeb2.png" alt=""></p><p><strong>SigLIP：一个图像编码器，使用SigLIP，其shape optimized ViT-So400m图像编码器，该模型通过sigmoid损失在大规模上进行了对比预训练，且其在小尺寸上也表现出色</strong></p><p><strong>gemma-2B：一个仅解码器的语言模型，使用gemma-2B v1.0，该模型可以匹配或超越使用相对更大些的语言模型的VLMs的性能，包括之前的PaLIs</strong></p><p><strong>线性层：一个线性层，将SigLIP的输出投影到与gemma-2B的词汇token相同的维度，以便它们可以被连接</strong></p><p>第一个多模态预训练阶段保持图像编码器冻结。这部分是由于LiT [132]中的研究发现，多模态调优预训练图像编码器会降低其表示能力</p><p>作者对图像编码器的学习率采用缓慢的线性预热（如下图图3所示），以确保图像编码器的质量不因最初未对齐的梯度通过LLM而恶化</p><p>作者训练了两个进一步的模型检查点以提高分辨率，首先是448 × 448，然后是896 × 896像素分辨率</p><p>作者发现，当批量大小小于16k 时，sigmoid 损失比softmax 损失表现显著更好。随着训练批量大小的增加，这一差距缩小。重要的是，sigmoid 损失是对称的，只需一次遍历，并且典型实现所需内存比softmax 损失少。这使得在一百万的批量大小下成功训练SigLiT 模型成为可能</p></li><li><p>预训练的视觉-语言模型VLM主干</p><p><strong>先在高度多样化的机器人数据上进行预训练——变成了相比不二次预训练情况下更强大的VLA(<em><strong>类似7-RT-2、24-OpenVLA、55-Tinyvla</strong></em>)，然后针对具体任务进行微调</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/pi0_image%201.png" alt="image.png"></p><p>利用一个预训练的视觉-语言模型(VLM)来导入互联网规模的经验，<strong>进一步训练他们的模型以整合机器人动作，使其成为一个视觉-语言-动作(VLA)模型</strong></p><p>能够执行高度灵巧和复杂的物理任务，作者使用<strong>带有流匹配的动作分块架构</strong></p><ul><li><p>流匹配</p><p>扩散模型广泛应用于DALLE、stable diffusion等文生图的模型中，但一直以来扩散模型的一个缺点就是<strong>采样速度较慢</strong>，通常需要100-1000的评估步骤才能抽取一个不错的样本</p><p>一致性模型直接把随机的噪声映射到复杂图像上，输出都是同一轨迹上的同一点，所以实现了一步生成</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/pi0_image%202.png" alt="image.png"></p><p>表示流 v 将初始分布 p1 转移到最终分布 p0 的过程。在我们的气体类比中，这意味着如果我们从按照 p1 分布的粒子气体开始，并且每个粒子按照流 v 定义的轨迹运动，那么最终粒子的分布将是 p0</p><p>流匹配的最终目标是以某种方式学习一个流 v*，它将易于采样的基础分布 q（例如高斯分布）转移到目标分布 p</p><p>如果我们有了这个 v*，我们可以通过首先从 q 中采样 x1，然后运行我们的流，以 x1 作为初始点，并输出结果的最终点 x0，从而生成目标分布 p 的样本</p><p>我们可以选择任意一个 q 和 p 之间的耦合 Π_(q,p)，并考虑与点级流 {v^[x1,x0]}，(x1,x0)∼Π_(q,p) 对应的粒子。 这给了我们一个点级流的分布（即一组粒子轨迹），在总体上表现出所需的行为。【q 和 p 之间的一个耦合 Π_(q,p)，指定了如何联合采样源点和目标点对 (x1, x0)，使得 x0 边际分布为 p，x1 边际分布为 q</p><p>一个源分布 q，我们可以有效地从中采样（例如标准高斯分布）</p><p>源分布 q 和目标分布 p 之间的耦合 Πq,p，它指定了如何联合采样一对源点和目标点 (x1,x0)，分别具有边际分布 q 和 p。标准选择是独立耦合，即独立地从 q 中采样 x1 和从 p 中采样 x0</p><p>对于所有点对 (x1,x0)，显式点对点流 v^[x1,x0] 将 x1 转移到 x0。我们必须能够在所有点上有效地计算向量场 v^[x1,x0]_t</p></li></ul><p><strong>流匹配直接对数据和噪声分布之间的映射场(vector field)进行建模，训练目标是匹配这一映射场</strong></p><p>而扩散模型通常学习的是每个去噪步骤的条件分布</p><p>图像和状态通过相应的编码器进行编码，然后通过线性投影层投影到与语言token相同的嵌入空间中对于动作块中的每个动作，有一个相应的动作token，通过动作专家action expert来处理</p><p><a href="https://latex.csdn.net/eq?%5Cmathbf%7BI%7D_%7Bt%7D%5E%7Bi%7D" target="_blank" rel="noopener"></a></p><p><a href="https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D_%7Bt%7D" target="_blank" rel="noopener"></a></p><p><strong>不是直接把自己的参数塞进VLM模型中，变成一个整体大模型来输出动作</strong>而是通过"注意力机制"去关注VLM的参数——即<strong>动作专家的每一层都与VLM的所有层进行注意力交互</strong></p><p><a href="https://latex.csdn.net/eq?%5Cmathbf%7BA%7D_%7Bt%7D" target="_blank" rel="noopener"></a></p><p><a href="https://latex.csdn.net/eq?%5Cmathbf%7Ba%7D_%7Bt%5E%7B%5Cprime%7D%7D" target="_blank" rel="noopener"></a></p></li><li><p>模型改动</p><p>添加了一个输入，用于机器人的本体状态，通过线性投影映射到transformer嵌入维度</p><p>对于图像和语言提示：，被路由到较大的VLM主干——其由PaliGemma初始化</p><p>VLM预训练期间未见的输入：机器人本体状态和噪声动作，则被路由到动作专家</p><p>配置向量和动作向量始终具有数据集中最大机器人的维度大小</p><p>对于配置和动作空间维度较低的机器人，对配置和动作向量进行零填充。对于少于三张图像的机器人，还会屏蔽掉缺失的图像槽</p></li></ul></li><li><p>π0代码解析</p></li><li><p>π0微调</p><ul><li><p>数据集转换代码</p><p>原始的aloha数据集包含四个摄像头，我们这里将low视角的摄像头去除即可</p><p>L59</p><pre class=" language-language-python"><code class="language-language-python">cameras = [   "cam_high",   "cam_left_wrist",   "cam_right_wrist",]</code></pre><p>L182</p><pre class=" language-language-python"><code class="language-language-python">[    "cam_high",    "cam_left_wrist",    "cam_right_wrist",]</code></pre><p>数据集转换</p><pre class=" language-language-bash"><code class="language-language-bash">uv run examples/aloha_real/convert_aloha_data_to_lerobot.py --raw-dir /home/jack/Docker/aloha/act-main/aloha_data/lay_clothes --repo-id lay_clothes</code></pre><p>数据集转换完成后进行数据统计</p><pre class=" language-language-python"><code class="language-language-python">uv run scripts/compute_norm_stats.py --config-name pi0_aloha_lay_clothes</code></pre><p>若当前磁盘空间不足，可将lerobot数据集存放位置更改为机械硬盘</p><p>修改lerobot_dataset.py即可</p><p>L71</p><pre class=" language-language-python"><code class="language-language-python">LEROBOT_HOME = Path(os.getenv("LEROBOT_HOME", "/home/jack/run/lerobot")).expanduser()</code></pre></li><li><p>微调策略aloha修改</p><p>修改aloha_policy中的AlohaInputs与AlohaOutputs</p><p>相机配置修改，去除cam_low视角相机信息，修改关节信息</p><p>L13</p><pre class=" language-language-python"><code class="language-language-python">"state": np.ones((16,)),"images": {    "cam_high": np.random.randint(256, size=(3, 224, 224), dtype=np.uint8    "cam_left_wrist": np.random.randint(256, size=(3, 224, 224), dtype=np.uint8),    "cam_right_wrist": np.random.randint(256, size=(3, 224, 224), dtype=np.uint8),}"prompt": "Fold the red fabric twice from bottom to top and once from right to left, finally folding it into a rectangle."</code></pre><p>L42</p><pre class=" language-language-python"><code class="language-language-python">EXPECTED_CAMERAS: ClassVar[tuple[str, ...]] = ("cam_high", "cam_left_wrist", "cam_right_wrist")</code></pre><p>L105</p><pre class=" language-language-python"><code class="language-language-python"> actions = np.asarray(data["actions"][:, :16])</code></pre><p>adapt_to_pi设置为False</p><p>aloha_policy</p><p>L38</p><pre class=" language-language-python"><code class="language-language-python">adapt_to_pi: bool = False</code></pre><p><a href="http://config.py" target="_blank" rel="noopener">config.py</a></p><p>L209</p><pre class=" language-language-python"><code class="language-language-python">adapt_to_pi: bool = False</code></pre><p>L235</p><pre class=" language-language-python"><code class="language-language-python">delta_action_mask = _transforms.make_bool_mask(7, -1, 7, -1)</code></pre><p>model pi0 action dim修改</p><p>L74</p><pre class=" language-language-python"><code class="language-language-python">action_dim: int = 16</code></pre></li><li><p>微调配置</p><p>nohup bash -c 'XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi0_pusht --exp-name=pusht --overwrite’ &gt; pusht.log 2&gt;&amp;1 &amp;</p><pre class=" language-language-python"><code class="language-language-python">TrainConfig(        name="pi0_aloha_lay_clothes",        # 使用lora进行微调        model=pi0.Pi0Config(paligemma_variant="gemma_2b_lora", action_expert_variant="gemma_300m_lora"),        data=LeRobotAlohaDataConfig(            repo_id="lay_clothes",            assets=AssetsConfig(                # 计算完stats后，这里修改为自己的路径                assets_dir="/home/jack/workspace/openpi/assets/pi0_aloha_lay_clothes",                asset_id="lay_clothes",            ),            default_prompt="Fold the red fabric twice from bottom to top and once from right to left, finally folding it into a rectangle",            repack_transforms=_transforms.Group(                inputs=[                    _transforms.RepackTransform(                        {                            "images": {                                "cam_high": "observation.images.cam_high",                                "cam_left_wrist": "observation.images.cam_left_wrist",                                "cam_right_wrist": "observation.images.cam_right_wrist",                            },                            "state": "observation.state",                            "actions": "action",                        }                    )                ]            ),            base_config=DataConfig(                local_files_only=True,  # Set to True for local-only datasets.                 prompt_from_task=True,            ),        ),        num_train_steps=20_000,        freeze_filter=pi0.Pi0Config(            paligemma_variant="gemma_2b_lora", action_expert_variant="gemma_300m_lora"        ).get_freeze_filter(),        ema_decay=None,    )</code></pre></li></ul></li><li><p>π0实机评估</p><p>π0的实机评估代码基于aloha，之前在测试RDT和ACT时已经对aloha的评估代码进行了修改，这里直接使用即可。</p><pre class=" language-language-python"><code class="language-language-python"># Ignore lint errors because this file is mostly copied from ACT (https://github.com/tonyzhaozh/act).# ruff: noqa# import IPython# import rospyimport matplotlib.pyplot as pltimport collectionsimport timefrom typing import Optional, Listimport dm_env# from interbotix_xs_modules.arm import InterbotixManipulatorXS# from interbotix_xs_msgs.msg import JointSingleCommandimport numpy as npimport constantsimport robot_utilsfrom realsense_camer import RealSenseCamera, get_device_idsfrom constants import DTimport argparsefrom dnx_utils import *import cv2# This is the reset position that is used by the standard Aloha runtime.# DEFAULT_RESET_POSITION = [0, -0.96, 1.16, 0, -0.3, 0]# e = IPython.embedleft_gripper_val_obs = 0right_gripper_val_obs = 0num = 0ROBOT_PORTS = {'leader': '/dev/serial/by-id/usb-1a86_USB_Single_Serial_585A008389-if00','follower': '/dev/serial/by-id/usb-1a86_USB_Single_Serial_5837053167-if00'}follower = robot_utils.Robot(device_name=ROBOT_PORTS['follower'],servo_ids=[0, 1, 2, 3, 4, 5])cfg = TASK_CONFIG = {'dataset_dir': "/media/jack/Lenovo/put_duck_checkpoints/checkpoints/put_duck/my_experiment/199999",'episode_len': 450,'state_dim': 6,'action_dim': 6,'cam_width': 640,'cam_height': 480,'camera_names': ['right', 'left'],'camera_port': 0}def move_to_reset(reset, robot):    curr = robot.read_position()    max_delta = (np.abs(curr - reset)).max()    steps = min(int(max_delta / 0.01), 500)    for jnt in np.linspace(curr, reset, steps):        robot.set_goal_pos(jnt.astype(int))class RealEnv:    """    Environment for real robot bi-manual manipulation    Action space:      [left_arm_qpos (6),             # absolute joint position                        left_gripper_positions (1),    # normalized gripper position (0: close, 1: open)                        right_arm_qpos (6),            # absolute joint position                        right_gripper_positions (1),]  # normalized gripper position (0: close, 1: open)    Observation space: {"qpos": Concat[ left_arm_qpos (6),          # absolute joint position                                        left_gripper_position (1),  # normalized gripper position (0: close, 1: open)                                        right_arm_qpos (6),         # absolute joint position                                        right_gripper_qpos (1)]     # normalized gripper position (0: close, 1: open)                        "qvel": Concat[ left_arm_qvel (6),         # absolute joint velocity (rad)                                        left_gripper_velocity (1),  # normalized gripper velocity (pos: opening, neg: closing)                                        right_arm_qvel (6),         # absolute joint velocity (rad)                                        right_gripper_qvel (1)]     # normalized gripper velocity (pos: opening, neg: closing)                        "images": {"cam_high": (480x640x3),        # h, w, c, dtype='uint8'                                   "cam_low": (480x640x3),         # h, w, c, dtype='uint8'                                   "cam_left_wrist": (480x640x3),  # h, w, c, dtype='uint8'                                   "cam_right_wrist": (480x640x3)} # h, w, c, dtype='uint8'    """    def __init__(self, init_node, *, reset_position: Optional[List[float]] = None, setup_robots: bool = True):        # reset_position = START_ARM_POSE[:6]        self._reset_position = [2048]*6         self.camera_names = ['right', 'left']        device_ids = get_device_ids() # 获取相机的设备ID        self.device_ids = device_ids        # print(f"Found {len(device_ids)} devices: {device_ids}")        cameras = [RealSenseCamera(device_id=device_id) for device_id in device_ids]        # self.cam = RealSenseCamera(device_ids=device_ids)        self.cam = cameras        # self.cam = [robot_utils.ImageRecorder(device_ids=device_id) for device_id in device_ids]        print(f"Found {len(device_ids)} devices: {device_ids}")        # self.cam = RealSenseCamera(device_id=self.device_ids[0])                # 小机械臂类        follower = robot_utils.Robot(device_name=ROBOT_PORTS['follower'],servo_ids=[0, 1, 2, 3, 4, 5])        self.follower = follower        # 初始化小机械臂        self.puppet_bot_right = robot_utils.Robot(device_name=ROBOT_PORTS['follower'],servo_ids=[0, 1, 2, 3, 4, 5])        # 重置小机械臂到初始位置        move_to_reset(self._reset_position,follower)        # 从传感器读取数据并将其记录下来        obs = {                'qpos': pwm2pos(self.follower.read_position()),                # 'qvel': vel2pwm(follower.read_velocity()),                # 'images': {cn: self.cam.read() for cn in cfg['camera_names']}                'images': {f'camera_{i}': camera.read() for i, camera in enumerate(self.cam)}                }        self.recorder_right = robot_utils.Recorder("right", init_node=False)    def setup_robots(self):        # robot_utils.setup_puppet_bot(self.puppet_bot_left)        robot_utils.setup_puppet_bot(self=None)    # def initialize_people_robot_config(self):            # def initialize_small_robot_config(self):    def get_qpos(self):        # 自研人形机器人        # left_qpos_raw = self.recorder_left.qpos        # right_qpos_raw = self.recorder_right.qpos        # left_gripper_qpos = [self.recorder_left.gripper] # this is position not joint        # right_gripper_qpos = [self.recorder_right.gripper] # this is position not joint        # 小机械臂                obs = {        'qpos': pwm2pos(self.follower.read_position()),        # 'qvel': vel2pwm(follower.read_velocity()),        'images': {f'camera_{i}': camera.read() for i, camera in enumerate(self.cam)}                }         self.qpos = obs['qpos']        right_qpos_raw = self.qpos         # left_arm_qpos = left_qpos_raw[:6]        # right_arm_qpos = right_qpos_raw[:6]        # left_gripper_qpos = [        #     constants.PUPPET_GRIPPER_POSITION_NORMALIZE_FN(left_qpos_raw[7])        # ]  # this is position not joint        # right_gripper_qpos = [        #     constants.PUPPET_GRIPPER_POSITION_NORMALIZE_FN(right_qpos_raw[7])        # ]  # this is position not joint        # return np.concatenate([left_arm_qpos, left_gripper_qpos, right_arm_qpos, right_gripper_qpos])        right_qpos_raw = np.array(right_qpos_raw)        # print("right_qpos_raw:", right_qpos_raw)        return np.concatenate([right_qpos_raw])    def get_qvel(self):        left_qvel_raw = self.recorder_left.qvel        right_qvel_raw = self.recorder_right.qvel        left_arm_qvel = left_qvel_raw[:6]        right_arm_qvel = right_qvel_raw[:6]        left_gripper_qvel = [constants.PUPPET_GRIPPER_VELOCITY_NORMALIZE_FN(left_qvel_raw[7])]        right_gripper_qvel = [constants.PUPPET_GRIPPER_VELOCITY_NORMALIZE_FN(right_qvel_raw[7])]        return np.concatenate([left_arm_qvel, left_gripper_qvel, right_arm_qvel, right_gripper_qvel])    def get_effort(self):        left_effort_raw = self.recorder_left.effort        right_effort_raw = self.recorder_right.effort        left_robot_effort = left_effort_raw[:7]        right_robot_effort = right_effort_raw[:7]        return np.concatenate([left_robot_effort, right_robot_effort])    def get_images(self):        camera_names = ['right', 'left']        return {camera_name: camera.read()[0] for camera_name, camera in zip(camera_names, self.cam)}        # return {cn: self.cam.read(camera_id=device_id)[0] for cn, device_id in zip(self.camera_names, self.device_ids)}        # return self.image_recorder.get_images()        # return {cn: self.cam.read()[0] for cn in cfg["camera_names"]}    def get_observation(self):        obs = {            'qpos': pwm2pos(self.follower.read_position()),            'images': {f'camera_{i}': camera.read() for i, camera in enumerate(self.cam)}        }        obs["qpos"] = self.get_qpos()        obs["images"] = self.get_images()        # cv2.imshow('image', obs["images"]["right"])        # cv2.imshow('image', obs["images"]["left"])        # cv2.waitKey(10000)        return obs          # 'qvel': vel2pwm(follower.read_velocity()),            # obs["qvel"] = self.get_qvel()        # obs["effort"] = self.get_effort()    def get_reward(self):        return 0    def reset(self, *, fake=False):        # if not fake:            # Reboot puppet robot gripper motors            # self.puppet_bot_left.dxl.robot_reboot_motors("single", "gripper", True)            # self.puppet_bot_right            # self._reset_joints()            # self._reset_gripper()        return dm_env.TimeStep(            step_type=dm_env.StepType.FIRST, reward=self.get_reward(), discount=None, observation=self.get_observation()        )    def step(self, action):        state_len = int(len(action) / 2)        # left_action = action[:state_len]        right_action = action[state_len:]        # print("right_action type:", type(right_action))        # print("right_action content:", right_action)        # self.puppet_bot_left.arm.set_joint_positions(left_action[:6], blocking=False)        # self.puppet_bot_right.arm.set_joint_positions(right_action[:6], blocking=False)        # self.set_gripper_pose(left_action[-1], right_action[-1])        # time.sleep(constants.DT)        # 这里是向左右臂以及夹爪赋值        # 应该调用ros        # action_left = Float64MultiArray()        # gripper_left = Float64()        obs = {        'qpos': pwm2pos(self.follower.read_position()),        # 'qvel': vel2pwm(follower.read_velocity()),        'images': {f'camera_{i}': camera.read() for i, camera in enumerate(self.cam)}        }         action_right = obs["qpos"] = self.get_qpos()        curr_images = obs["images"] = self.get_images()        # gripper_right = Float64()        # action_left.data = np.array(left_action)[:7]        # self.pub_left_arm.publish(action_left)        # gripper_left.data = np.array(left_action)[-1]        # self.pub_left_gripper.publish(gripper_left)        # action_right.data = right_action[:6]        # print("action_right type:", type(action_right))        # print("action_right content:", action_right)        # self.pub_right_arm.publish(action_right)        # gripper_right.data = np.array(right_action)[-1]        # self.pub_right_gripper.publish(gripper_right)        return dm_env.TimeStep(            step_type=dm_env.StepType.MID, reward=self.get_reward(), discount=None, observation=self.get_observation()        )def get_action(master_bot_left, master_bot_right,master_left_gripper_val, master_right_gripper_val):    # action = np.zeros(14)  # 6 joint + 1 gripper, for two arms    # # Arm actions    # action[:6] = master_bot_left.dxl.joint_states.position[:6]    # action[7 : 7 + 6] = master_bot_right.dxl.joint_states.position[:6]    # # Gripper actions    # action[6] = constants.MASTER_GRIPPER_JOINT_NORMALIZE_FN(master_bot_left.dxl.joint_states.position[6])    # action[7 + 6] = constants.MASTER_GRIPPER_JOINT_NORMALIZE_FN(master_bot_right.dxl.joint_states.position[6])    # 这里传入的参数默认包含了夹爪，即最后一位    action = np.zeros(6) # 6 joint + 1 gripper, for two arms    # Arm actions    # action[:7] = master_bot_left    action[:6] = master_bot_right    # action[7] = master_left_gripper_val    action[6] = master_right_gripper_val    return actiondef make_real_env(init_node=False, reset_position: Optional[List[float]] = None,setup_robots=True):    env = RealEnv(init_node=False,reset_position=reset_position,setup_robots=True)    return env</code></pre></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具身智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>树莓派开发板</title>
      <link href="/2025/08/30/shu-mei-pai-kai-fa-ban/"/>
      <url>/2025/08/30/shu-mei-pai-kai-fa-ban/</url>
      
        <content type="html"><![CDATA[<h3 id="树莓派开发板">树莓派开发板</h3><p>转发自https://ivonblog.com/posts/redroid-on-raspberry-pi/</p><p>ReDroid為一款開源雲手機解決方案，讓你將Android系統部署在x86與ARM架構的Linux電腦，遠端控制Android玩遊戲。</p><p><img src="https://ivonblog.com/posts/redroid-on-raspberry-pi/images/Screenshot_20240825_143650.webp" alt=""></p><p>使用Scrcpy遠端連線到ReDroid容器畫面，執行遊戲</p><p>ReDroid得透過Docker部署，啟動速度十分之快，待機狀態下不太吃資源，效能可跟基於LXC的Waydroid相比，還支援多開。</p><p>為何要用ReDroid呢？因為它本來就是設計給遠端存取用的，較適合headless伺服器環境，不像<a href="https://ivonblog.com/posts/raspberry-pi-waydroid/" target="_blank" rel="noopener">樹莓派跑Waydroid</a>需要顯示器才能輸出。</p><p>之前寫過<a href="https://ivonblog.com/posts/redroid-android-docker/" target="_blank" rel="noopener">x86電腦部署ReDroid的教學</a>，這裡要用ARM架構的樹莓派。相較於x86架構電腦，ARM架構的優勢是不需要libhoudini轉譯器就能執行大多數手機的APP。並且從<a href="https://ivonblog.com/posts/raspberry-pi-lineageos/" target="_blank" rel="noopener">樹莓派5跑LineageOS系統</a>的跑分來看，樹莓派5的處理器效能已接近一台中低階Android手機，玩一些不吃效能的手遊完全是夠的。</p><p>影片演示：</p><p>另一種用法：樹莓派除了當作隨身攜帶的小電腦之外，也可以用來當隨身Android機用喔！對iPhone用戶來說彷彿就有了一台Linux伺服器 + Android的 APP測試用小雞，或可達成在iOS使用Play商店APP的操作。</p><h2 id="1-環境">1. 環境<a href="#1-%E7%92%B0%E5%A2%83">#</a></h2><ul><li>Raspberry Pi 5 8GB</li><li>Raspberry Pi OS (Debian 12 Bookworm)</li><li>核心版本：Linux raspberrypi5 6.6.42-v8+</li><li>無接上顯示器</li></ul><p>考慮到Raspberry Pi常常是headless運作的，所以得準備另一台電腦或手機，利用Scrcpy無線控制的方式遠端存取Android桌面。</p><p>不過在開始下一步之前，建議執行以下指令確認GPU加速是否正常，驅動應會顯示<code>V3DV Mesa</code>。</p><h2 id="2-前置作業">2. 前置作業<a href="#2-%E5%89%8D%E7%BD%AE%E4%BD%9C%E6%A5%AD">#</a></h2><p>根據<a href="https://github.com/remote-android/redroid-doc/blob/master/deploy" target="_blank" rel="noopener">ReDroid文件的部署需求</a>，Linux必須啟用binder核心模組，並且要啟用4K Pages、PSI。</p><ol><li>從<code>zcat /proc/config.gz</code>指令的輸出來看，Raspberry Pi 5的核心已經啟用BINDER設定了，所以不用安裝binder核心模組</li></ol><pre class=" language-language-bash"><code class="language-language-bash">sudo modprobe configszcat /proc/config.gz | grep BINDER# 輸出CONFIG_ANDROID_BINDER_IPC=yCONFIG_ANDROID_BINDERFS=yCONFIG_ANDROID_BINDER_DEVICES="binder,hwbinder,vndbinder"</code></pre><ol start="2"><li>Raspberry Pi 5預設是使用16K Pages，透過調整開機設定檔啟用4K Pages</li></ol><pre class=" language-language-bash"><code class="language-language-bash">echo '# 4k pageskernel=kernel8.img'| sudo tee -a /boot/firmware/config.txt</code></pre><ol start="3"><li>啟用PSI</li></ol><pre class=" language-language-bash"><code class="language-language-bash">sudo sed --follow-symlinks -i 's/quiet/psi=1 quiet/g' /boot/firmware/cmdline.txt</code></pre><ol start="4"><li>重開機，確認4K Pages與PSI狀態</li></ol><pre class=" language-language-bash"><code class="language-language-bash">getconf PAGESIZEls /proc/pressure</code></pre><ol start="5"><li>如果有裝Waydroid，不要讓它開機自動啟動</li></ol><pre class=" language-language-bash"><code class="language-language-bash">sudo systemctl stop waydroid-containersudo systemctl disable waydroid-container</code></pre><h2 id="3-給ReDroid映像檔安裝GApps">3. 給ReDroid映像檔安裝GApps<a href="#3-%E7%B5%A6redroid%E6%98%A0%E5%83%8F%E6%AA%94%E5%AE%89%E8%A3%9Dgapps">#</a></h2><p>ReDroid作者上傳的映像檔全部都是原生系統，GApps要自己裝。</p><p>關於安裝GApps的指令稿，我原本是用這個安裝<a href="https://github.com/ayasa520/redroid-script" target="_blank" rel="noopener">ayasa520/redroid-script</a>，但作者似乎只有考慮到x86_64的狀況，所以改用這個比較新的指令稿<a href="https://github.com/abing7k/redroid-script/" target="_blank" rel="noopener">abing7k/redroid-script</a>。但是這個指令稿還有小問題，它抓的是Android 10的GAPPS，而我比較慣用Android 11，所以要稍微修改一下：</p><pre class=" language-language-bash"><code class="language-language-bash">git clone https://github.com/abing7k/redroid-script.gitcd redroid-scriptvim stuffs/gapps.py# 填入GApps的下載網址和MD5    dl_links = {            "arm64-v8a": ["https://nchc.dl.sourceforge.net/project/opengapps/arm64/20220215/open_gapps-arm64-11.0-pico-20220215.zip", "7790055d34bbfc6fe610b0cd263a7add"]            }</code></pre><p>執行以下指令安裝GApps，會得到<code>redroid/redroid:11.0.0_gapps</code>映像檔：</p><pre class=" language-language-bash"><code class="language-language-bash">sudo apt install lzip python3 python3-venv python3-pippython3 -m venv venvvenv/bin/pip install -r requirements.txtvenv/bin/python3 redroid.py -a 11.0.0 -g</code></pre><h2 id="4-部署ReDroid容器">4. 部署ReDroid容器<a href="#4-%E9%83%A8%E7%BD%B2redroid%E5%AE%B9%E5%99%A8">#</a></h2><ol><li><p><a href="https://ivonblog.com/posts/install-docker-engine-on-linux/" target="_blank" rel="noopener">安裝Docker</a></p></li><li><p>新增存放資料的目錄</p></li></ol><pre class=" language-language-bash"><code class="language-language-bash">mkdir ~/redroidcd redroid</code></pre><ol start="3"><li><p>新增docker-compose.yml</p></li><li><p>內容如下：</p></li></ol><pre class=" language-language-yaml"><code class="language-language-yaml">services:  redroid:    image: redroid/redroid:11.0.0_gapps # 使用剛剛建立的內含GApps的ReDroid映像檔    stdin_open: true    tty: true    privileged: true    ports:      - "5555:5555" # ADB通訊埠    volumes:      - ./redroid-11-data:/data # 資料目錄    command:      - androidboot.redroid_width=1080 # 解析度      - androidboot.redroid_height=2160      - androidboot.redroid_dpi=439      - androidboot.redroid_fps=60      - androidboot.redroid_gpu_mode=host # 啟用GPU硬體加速</code></pre><ol start="3"><li>啟動容器</li></ol><pre class=" language-language-bash"><code class="language-language-bash">sudo docker compose up -d</code></pre><ol start="4"><li><p>取得樹莓派的區域IP</p></li><li><p>有使用UFW防火牆的話，開放5555通訊埠允許連入</p></li></ol><pre class=" language-language-bash"><code class="language-language-bash">sudo ufw allow 5555sudo ufw reload</code></pre><h2 id="5-使用Scrcpy存取ReDroid桌面">5. 使用Scrcpy存取ReDroid桌面<a href="#5-%E4%BD%BF%E7%94%A8scrcpy%E5%AD%98%E5%8F%96redroid%E6%A1%8C%E9%9D%A2">#</a></h2><p>這裡我使用電腦版的Scrcpy，若要方便其他裝置存取，可於樹莓派部署網頁版的<a href="https://ivonblog.com/posts/ws-scrcpy/" target="_blank" rel="noopener">ws scrcpy</a>，透過瀏覽器連線。</p><ol><li><p>安裝<a href="https://ivonblog.com/posts/scrcpy-usage/" target="_blank" rel="noopener">Scrcpy與ADB工具</a></p></li><li><p>讓電腦與樹莓派位於同一個區網，將ADB與樹莓派連線</p></li><li><p>啟動Scrcpy，遠端連線可能會lag，所以限制畫質</p></li></ol><pre class=" language-language-bash"><code class="language-language-bash">scrcpy -m 1080 -b 5M --audio-codec=aac</code></pre><ol start="4"><li><p>這樣就能看到Android桌面了。</p><p><img src="https://ivonblog.com/posts/redroid-on-raspberry-pi/images/Screenshot_20240825_144338.webp" alt=""></p></li><li><p>如果GPU加速有正常運作，那麼「AIDA64」APP應當能抓到樹莓派的GPU型號。</p><p><img src="https://ivonblog.com/posts/redroid-on-raspberry-pi/images/Screenshot_20240825_142130.webp" alt=""></p></li><li><p>Google Play服務可能會跳出「裝置未驗證」的錯誤訊息。執行以下指令取得Android裝置ID，到<a href="https://www.google.com/android/uncertified" target="_blank" rel="noopener">Google網站註冊裝置</a>，等個30分鐘後重新啟動Redroid容器，才能登入Google Play。</p></li></ol><pre class=" language-language-bash"><code class="language-language-bash">adb -s 樹莓派IP:5555 rootadb -s 樹莓派IP:5555 shell 'sqlite3 /data/data/com.google.android.gsf/databases/gservices.db \    "select * from main where name = \"android_id\";"'</code></pre><ol start="7"><li>現在Scrcpy只能透過區網連線，接下來只要再於樹莓派<a href="https://ivonblog.com/posts/my-self-hosting-setup/" target="_blank" rel="noopener">部署內網穿透服務</a>，就能從外面的網路隨時連線到樹莓派上的雲手機了！</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 嵌入式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDT复现</title>
      <link href="/2025/08/30/rdt-fu-xian/"/>
      <url>/2025/08/30/rdt-fu-xian/</url>
      
        <content type="html"><![CDATA[<h3 id="RDT复现">RDT复现</h3><ul><li><p>RDT论文</p><p><a href="https://arxiv.org/pdf/2410.07864" target="_blank" rel="noopener">2410.07864v1.pdf</a></p></li><li><p>RDT仓库</p><p><a href="https://github.com/thu-ml/RoboticsDiffusionTransformer.git" target="_blank" rel="noopener">GitHub - thu-ml/RoboticsDiffusionTransformer: RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</a></p></li><li><p>RDT算法解析</p><ul><li><p>模型的obs</p><p>形式上，给定一个语言指令，策略在时间时接收到一个观测，然后它生成一个动作来控制两个机器人手臂，以实现语言指令指定的目标</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%201.png" alt="image.png"></p><p>大小为T的RGB观测历史，Zt是机器人的低维本体感知，c是控制频率</p><p>动作at，通常是期望本体感知Zt的一个子集</p></li></ul><p>将扩散模型应用于机器人任务面临独特的挑战，一个是机器人数据的不稳定性、一个是机器人数据的非线性特征，而这也是如果用扩散模型作为RDT架构所需要解决的两大问题</p><p>图像和视频数据虽然是高维的，但通常表现出一定程度的时间和空间连续性，<strong>机器人物理量的特征是其非线性动态</strong></p><p><strong>作者倾向于一次性预测一系列动作，即一个动作块，以促进时间一致性，并通过减少任务中的决策次数来缓解错误累积</strong></p><ul><li><p>解决数据异构性问题，且兼容多个模态的输入</p><p>他们使用适配器将每种模态的token维度对齐到2048，且所有多模态编码器的适配器均使用GeLU激活</p><ul><li><p>第一种输入模态</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%202.png" alt="image.png"></p><p>低维输入是表示机器人物理量的低维向量，包括本体感知、动作块和控制频率，为了对它们进行编码，使用「<em>带有傅里叶特征（Tancik等人，2020））</em>」的MLP，可以有效捕捉低维空间中的高频变化</p><p><strong>首先</strong>，将本体感知，和噪声动作块嵌入到统一的动作空间。该空间用于统一不同机器人中本体感知和噪声动作块的表示</p><p>通过根据其物理意义将原始动作向量的每个元素填充到统一动作空间向量的相应位置，将机器人的动作空间嵌入到这个统一空间中，剩余的位置则进行填充</p><p>为了将各种机器人的动作嵌入到一个包含所有主要物理量的统一空间中，先将这个统一动作空间的维度是128</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%203.png" alt="image.png"></p><p><strong>然后</strong>，由于它们具有相似的物理意义，它们通过一个共享的MLP被编码到token空间中。这种连续编码可以避免离散编码所导致的精度损失</p><p>对于<strong>频率</strong>「<em>之所以将控制频率输入模型，如论文附录D最后所述，是为了在解决数据集中不同控制频率带来的挑战时，使模型在做出决策时能够考虑到这种变化</em>」，<strong>以及扩散时间步</strong>，作者<strong>分别通过两个MLP将它们编码到token空间中</strong></p><p><strong>之后</strong>，所有这些在长度方向上被拼接在一起，最后得到的长度为本体感知噪音块+统一空间+控制频率+时间步</p><p><strong>最后</strong>，添加位置嵌入以区分不同的模态并注入中的时间信息</p><p><em><strong>在预处理上，通过统一物理量的单位，而非归一化处理</strong></em></p><p><em><strong>采用6D表示法（Zhou et al., 2019）来解决EEF旋转中的万向节锁问题</strong></em></p></li><li><p>第二种输入模态</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%204.png" alt="image.png"></p><p><strong>图像输入</strong>是高维的(图像token空间维度为1152，RDT的token空间维度为2048)，包含丰富的空间和语义信息，为了提取紧凑的表示，作者使用了一个图像-文本对齐的预训练视觉编码器：SigLIP</p><p>具体而言，使用冻结的 SigLIP (Zhai et al., 2023) 对 RGB 图像进行编码，并利用额外的 MLP 将输出投射到token空间，为了增强模型根据视角和时间步区分图像的能力，作者将传统的正弦位置嵌入扩展到多维网格</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%205.png" alt="image.png"></p><p>这里，Ncam 代表相机的数量，在他们的配置中设置为三个，Npatch 表示每幅图像被 ViT 基于图像编码器划分的补丁数量，D 表示嵌入维度</p><p>在预处理上，他们采用固定长度的图像输入策略。为所有机器人固定图像输入顺序和格式，共有三个视图：一个静态外部视图，一个右手腕视图和一个左手腕视图，这被认为足以满足大多数双手任务的要求</p><p>当输入模型时，每张图像被填充成一个正方形并调整为384×384的大小，保持其原始纵横比，此外，选择T=2(<strong>相当于图像历史大小</strong>)，因为两帧的历史长度对于大多数情况来说已经足够，在效率和性能之间达到了平衡</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%206.png" alt="image.png"></p></li><li><p>第三种输入</p><p><strong>语言输入</strong>(语言token的空间维度为4096)的长度各异且高度抽象，由于其复杂性和模糊性，带来了集成挑战，为了对其进行编码，作者使用了基于Transformer的预训练语言模型T5-XXL(Raffel等，2020)，且作者还在训练过程中固定其权重以节省GPU内存</p><p>具体而言，语言指令由一个冻结的T5-XXL(Raffel等，2020)进行编码，并使用一个MLP将输出投射到词元空间。在计算语言词元的注意力时，作者应用语言注意力掩码以掩盖在批处理过程中附加的填充词元</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%207.png" alt="image.png"></p><p>在预处理上，会针对这部分对原始文本进行简单清理，例如去除非法字符和多余空格，将句子开头大写，并在句末添加句号，且保留文本为可变长度</p><p><strong>模型可能会学习到一种捷径：只关注外部视图而忽略腕部视图，从而失去感知深度的能力为了解决这个问题，作者在编码过程中以一定的概率比如10%——随机独立地屏蔽每个多模态输入，以防止模型过度依赖特定的输入</strong></p></li></ul></li><li><p>再解决架构问题：修改网络结构DiT(解决不稳定性和非线性特征)</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%208.png" alt="image.png"></p><p>DiT结合了视觉 transformer 和 diffusion 模型的优点，即DiT = DDPM + ViT，相当于之前一般更多是DDPM + CNN，将扩散过程中的U-Net 换成ViT(2D图像生成，带文本条件融合)</p><ul><li><p>第一个修改，QKNorm和RMSNorm，解决机器人数据不稳定的问题</p><p><strong>由于传感器失灵等原因，机器人数据中往往会出现极端值。这种极端值可能导致梯度不稳定和数值溢出等问题。研究者采用更加先进的 QKNorm 和 RMSNorm 来进行缓解</strong></p><p>输入机器人物理量的不稳定数值范围可能导致梯度不稳定和数值溢出等问题，尤其是在训练大型基础模型时。为了解决这个问题，作者添加了<strong>QKNorm</strong>（Henry等,2020）以避免计算注意力时的数值不稳定</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/main/RDT_image%209.png" alt="image.png"></p><p>原始DiT中的LayerNorm的中心化操作可能导致token偏移和注意力偏移，从而破坏时间序列的对称性，因此，他们用没有中心化操作的<strong>RMSNorm</strong>（Zhang &amp; Sennrich,2019）替换了LayerNorm。图4a显示，如果没有这种修改，大规模预训练往往会非常不稳定，甚至爆炸</p></li><li><p>第二个修改，MLP解码器，解决机器人数据的非线性特征</p><p>为了提高对非线性机器人动作的逼近能力，作者用非线性MLP解码器替换了最终的线性解码器，将其作为从潜在空间回到物理空间的投影</p></li><li><p>第三个修改，交替条件注入(即Alternating Condition Injection，简称ACI)：图像和语言token，而非同时注入两者</p><p>作者采用交叉注意力来适应不同长度的条件，避免在进一步压缩中信息丢失</p><p>由于图像token通常比文本token多，同时注入两种模态往往会掩盖与文本相关的信息「<em><strong>说白了，图像的维度通常远高于文本的维度。同时将这两种模态注入到主干网络中时，往往图像会淹没文本</strong></em>」，从而削弱模型的指令遵循能力</p><p>作者在连续层的交叉注意力中策略性地交替注入图像和文本token，而不是在每一层中同时注入这两种token</p></li></ul></li></ul><p>为了将一个特定的机器人动作嵌入128维的统一动作空间，作者需要填充不可用的动作元素。通常的做法是用0值或特定值进行填充。但“0”实际上具有物理意义。例如，速度为“0”通常表示相对于地面静止。这可能会让模型感到困惑：“0”是代表静止还是填充值？</p><p>为了解决这个问题，他们将动作和本体感受与一个0-1向量连接，指示每个维度在编码到token空间之前是否被填充，最终形成一个256维的向量。这可以补充缺失的可用性信息并消除混淆</p><p>进行图像增强，包括颜色抖动和图像损坏，并在输入本体感受中添加高斯噪声，信噪比（SNR）为40dB，利用数据增强技术来解决过拟合</p></li><li><p>RDT代码阅读</p><p><a href="https://raw.githubusercontent.com/SprBoot/image/main/RDT_RoboticsDiffusionTransformer-main.zip" target="_blank" rel="noopener">RoboticsDiffusionTransformer-main.zip</a></p></li><li><p>RDT代码复现</p><ul><li><p>复现bug及错误记录</p><p><code>pip install flash-attn --no-build-isolation</code></p><p>安装<code>flash-attn</code>报错提示只支持cuda11.7以上的版本，自身安装的cuda为11.8，这里需要再bashrc中配置一下自己安装的最新的cuda版本环境变量即可。</p><pre class=" language-language-bash"><code class="language-language-bash">export PATH=/usr/local/cuda-11.8/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH# source ./bashrc</code></pre><p>RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key ‘0’, but store-&gt;get(‘0’) got error: Connection reset by peer. This may indicate a possible application crash on rank 0 or a network set up issue.</p><pre class=" language-language-bash"><code class="language-language-bash"># 查看自己的网卡ifconfig</code></pre><p>这里需要再finetune中修改</p><p>export NCCL_SOCKET_IFNAME=[前面得到的网卡名]</p><p>Error catched when processing sample from pick: ‘NoneType’ object has no attribute 'shape’</p><p>直接从hdf5中读取图像的话不需要对图象在进行编码操作</p><pre class=" language-language-python"><code class="language-language-python">imgs.append(cv2.imdecode(np.frombuffer(img, np.uint8), cv2.IMREAD_COLOR))直接替换为imgs.append(img)</code></pre></li><li><p>RDT复现流程</p><p>相机的图像输入顺序[ext_{t-1}, right_wrist_{t-1}, left_wrist_{t-1}, ext_{t}, right_wrist_{t}, left_wrist_{t}]</p><p>这里推荐在采集数据的时候将相机的顺序写死，防止模型输入时在对顺序进行转换</p><p>软链接语言和图像模型</p><pre class=" language-language-bash"><code class="language-language-bash"># Under the root directory of this repomkdir -p google# Link the downloaded encoders to this repoln -s /home/jack/workspace/t5-v1_1-xxl google/t5-v1_1-xxlln -s /home/jack/workspace/siglip-so400m-patch14-384 google/siglip-so400m-patch14-384</code></pre><p>微调自己的模型</p><p>软链接自己的数据集</p><pre class=" language-language-bash"><code class="language-language-bash"># Under the root directory of this repocd datamkdir -p datasets# Link the downloaded dataset to this repoln -s /home/jack/workspace/clothes_data datasets/clothes_data</code></pre><p>在dataset_control_freq.json文件中添加自己的数据集并设定控制频率</p><pre class=" language-language-json"><code class="language-language-json">"clothes_data": 50</code></pre><p>在微调配置文件中配置自己的数据集</p><p>configs/finetune_datasets.json和configs/finetune_sample_weights.json</p><pre class=" language-language-json"><code class="language-language-json">[    "clothes_data"]</code></pre><p>重写HDF5VLADataset数据集类</p><p>parse_hdf5_file和parse_hdf5_file_state_only函数都要重写</p><pre class=" language-language-python"><code class="language-language-python"># 重写数据集路径和数据集名称HDF5_DIR = "data/datasets/clothes_data/rdt_data/put_the_coffee_cup"self.DATASET_NAME = "put_the_coffee_cup"</code></pre><pre class=" language-language-python"><code class="language-language-python"># 重写关节数据，夹爪不需要归一化qpos = qpos / np.array(     [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] )target_qpos = f['action'][step_id:step_id+self.CHUNK_SIZE] / np.array(     [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] )STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"] for i in range(7)STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(7)</code></pre><p>预先计算语言嵌入</p><pre class=" language-language-python"><code class="language-language-python"># Modify the TARGET_DIR to your dataset pathTARGET_DIR = "data/datasets/clothes_data/"</code></pre><p>下载预训练模型并设置路径</p><pre class=" language-language-bash"><code class="language-language-bash">--pretrained_model_name_or_path="robotics-diffusion-transformer/rdt-1b"</code></pre></li><li><p>RDT实机评估</p><p>启动参数暂不设置<code>use_actions_interpolation</code>，插值存在问题</p><ul><li><p>语言预嵌入</p><pre class=" language-language-bash"><code class="language-language-bash">python -m scripts.encode_lang</code></pre></li><li><p>基于Mobile Aloha的评估代码修改</p><pre class=" language-language-python"><code class="language-language-python">#!/home/lin/software/miniconda3/envs/aloha/bin/python# -- coding: UTF-8"""#!/usr/bin/python3"""import argparseimport sysimport threadingimport timeimport yamlfrom collections import dequeimport numpy as npimport rospyimport torchfrom cv_bridge import CvBridgefrom geometry_msgs.msg import Twistfrom nav_msgs.msg import Odometryfrom PIL import Image as PImagefrom sensor_msgs.msg import Image, JointStatefrom std_msgs.msg import Header, Float64, Float64MultiArrayimport cv2from scripts.agilex_model import create_model# sys.path.append("./")CAMERA_NAMES = ['cam_high', 'cam_right_wrist', 'cam_left_wrist']observation_window = Nonelang_embeddings = None# debugpreload_images = None# Initialize the modeldef make_policy(args):    with open(args.config_path, "r") as fp:        config = yaml.safe_load(fp)    args.config = config        # pretrained_text_encoder_name_or_path = "google/t5-v1_1-xxl"    pretrained_vision_encoder_name_or_path = "google/siglip-so400m-patch14-384"    model = create_model(        args=args.config,         dtype=torch.bfloat16,        pretrained=args.pretrained_model_name_or_path,        # pretrained_text_encoder_name_or_path=pretrained_text_encoder_name_or_path,        pretrained_vision_encoder_name_or_path=pretrained_vision_encoder_name_or_path,        control_frequency=args.ctrl_freq,    )    return modeldef set_seed(seed):    torch.manual_seed(seed)    np.random.seed(seed)# Interpolate the actions to make the robot move smoothlydef interpolate_action(args, prev_action, cur_action):    steps = np.concatenate((np.array(args.arm_steps_length), np.array(args.arm_steps_length)), axis=0)    diff = np.abs(cur_action - prev_action)    step = np.ceil(diff / steps).astype(int)    step = np.max(step)    if step <= 1:        return cur_action[np.newaxis, :]    new_actions = np.linspace(prev_action, cur_action, step + 1)    return new_actions[1:]def get_config(args):    config = {        'episode_len': args.max_publish_step,        'state_dim': 16,        'chunk_size': args.chunk_size,        'camera_names': CAMERA_NAMES,    }    return config# Get the observation from the ROS topicdef get_ros_observation(args,ros_operator):    rate = rospy.Rate(args.publish_rate)    print_flag = True    while True and not rospy.is_shutdown():        result = ros_operator.get_frame()        if not result:            if print_flag:                print("syn fail when get_ros_observation")                print_flag = False            rate.sleep()            continue        print_flag = True        (img_front, img_left, img_right, img_front_depth, img_left_depth, img_right_depth,         puppet_arm_left, puppet_arm_right, robot_base) = result        # print(f"sync success when get_ros_observation")        return (img_front, img_left, img_right,         puppet_arm_left, puppet_arm_right)# Update the observation window bufferdef update_observation_window(args, config, ros_operator):    # JPEG transformation    # Align with training    def jpeg_mapping(img):        img = cv2.imencode('.jpg', img)[1].tobytes()        img = cv2.imdecode(np.frombuffer(img, np.uint8), cv2.IMREAD_COLOR)        return img        global observation_window    if observation_window is None:        observation_window = deque(maxlen=2)            # Append the first dummy image        observation_window.append(            {                'qpos': None,                'images':                    {                        config["camera_names"][0]: None,                        config["camera_names"][1]: None,                        config["camera_names"][2]: None,                    },            }        )            img_front, img_left, img_right, puppet_arm_left, puppet_arm_right = get_ros_observation(args,ros_operator)    img_front = jpeg_mapping(img_front)    img_left = jpeg_mapping(img_left)    img_right = jpeg_mapping(img_right)    qpos = np.concatenate(            (np.array(puppet_arm_left), np.array(puppet_arm_right)), axis=0)    qpos = torch.from_numpy(qpos).float().cuda()    observation_window.append(        {            'qpos': qpos,            'images':                {                    config["camera_names"][0]: img_front,                    config["camera_names"][1]: img_right,                    config["camera_names"][2]: img_left,                },        }    )# RDT inferencedef inference_fn(args, config, policy, t):    global observation_window    global lang_embeddings        # print(f"Start inference_thread_fn: t={t}")    while True and not rospy.is_shutdown():        time1 = time.time()             # fetch images in sequence [front, right, left]        image_arrs = [            observation_window[-2]['images'][config['camera_names'][0]],            observation_window[-2]['images'][config['camera_names'][1]],            observation_window[-2]['images'][config['camera_names'][2]],                        observation_window[-1]['images'][config['camera_names'][0]],            observation_window[-1]['images'][config['camera_names'][1]],            observation_window[-1]['images'][config['camera_names'][2]]        ]                # fetch debug images in sequence [front, right, left]        # image_arrs = [        #     preload_images[config['camera_names'][0]][max(t - 1, 0)],        #     preload_images[config['camera_names'][2]][max(t - 1, 0)],        #     preload_images[config['camera_names'][1]][max(t - 1, 0)],        #     preload_images[config['camera_names'][0]][t],        #     preload_images[config['camera_names'][2]][t],        #     preload_images[config['camera_names'][1]][t]        # ]        # # encode the images        # for i in range(len(image_arrs)):        #     image_arrs[i] = cv2.imdecode(np.frombuffer(image_arrs[i], np.uint8), cv2.IMREAD_COLOR)        # proprio = torch.from_numpy(preload_images['qpos'][t]).float().cuda()                images = [PImage.fromarray(arr) if arr is not None else None                  for arr in image_arrs]                # for i, pos in enumerate(['f', 'r', 'l'] * 2):        #     images[i].save(f'{t}-{i}-{pos}.png')                # get last qpos in shape [14, ]        proprio = observation_window[-1]['qpos']        # unsqueeze to [1, 14]        proprio = proprio.unsqueeze(0)                # actions shaped as [1, 64, 14] in format [left, right]        actions = policy.step(            proprio=proprio,            images=images,            text_embeds=lang_embeddings         ).squeeze(0).cpu().numpy()        # print(f"inference_actions: {actions.squeeze()}")        print(f"Model inference time: {time.time() - time1} s")        # print(f"Finish inference_thread_fn: t={t}")        return actions# Main loop for the manipulation taskdef model_inference(args, config, ros_operator):    global lang_embeddings        # Load rdt model    policy = make_policy(args)        lang_dict = torch.load(args.lang_embeddings_path)    print(f"Running with instruction: \"{lang_dict['instruction']}\" from \"{lang_dict['name']}\"")    lang_embeddings = lang_dict["embeddings"]        max_publish_step = config['episode_len']    chunk_size = config['chunk_size']    # Initialize position of the puppet arm    # 机器人的初始位置通过VR设置    # left0 = [-0.00133514404296875, 0.00209808349609375, 0.01583099365234375, -0.032616615295410156, -0.00286102294921875, 0.00095367431640625, 3.557830810546875]    # right0 = [-0.00133514404296875, 0.00438690185546875, 0.034523963928222656, -0.053597450256347656, -0.00476837158203125, -0.00209808349609375, 3.557830810546875]    # left1 = [-0.00133514404296875, 0.00209808349609375, 0.01583099365234375, -0.032616615295410156, -0.00286102294921875, 0.00095367431640625, -0.3393220901489258]    # right1 = [-0.00133514404296875, 0.00247955322265625, 0.01583099365234375, -0.032616615295410156, -0.00286102294921875, 0.00095367431640625, -0.3397035598754883]    # ros_operator.puppet_arm_publish_continuous(left0, right0)    input("Press enter to continue")    # ros_operator.puppet_arm_publish_continuous(left1, right1)    # Initialize the previous action to be the initial robot state    # 16    pre_action = np.zeros(config['state_dim'])    # 这里的初始化也不需要了    # pre_action[:14] = np.array(    #     [-0.00133514404296875, 0.00209808349609375, 0.01583099365234375, -0.032616615295410156, -0.00286102294921875, 0.00095367431640625, -0.3393220901489258] +    #     [-0.00133514404296875, 0.00247955322265625, 0.01583099365234375, -0.032616615295410156, -0.00286102294921875, 0.00095367431640625, -0.3397035598754883]    # )    action = None    # Inference loop    with torch.inference_mode():        while True and not rospy.is_shutdown():            # The current time step            t = 0            rate = rospy.Rate(args.publish_rate)                action_buffer = np.zeros([chunk_size, config['state_dim']])                        while t < max_publish_step and not rospy.is_shutdown():                # Update observation window                update_observation_window(args, config, ros_operator)                                # When coming to the end of the action chunk                if t % chunk_size == 0:                    # Start inference                    action_buffer = inference_fn(args, config, policy, t).copy()                                raw_action = action_buffer[t % chunk_size]                action = raw_action                # Interpolate the original action sequence                if args.use_actions_interpolation:                    # print(f"Time {t}, pre {pre_action}, act {action}")                    interp_actions = interpolate_action(args, pre_action, action)                else:                    interp_actions = action[np.newaxis, :]                # Execute the interpolated actions one by one                for act in interp_actions:                    left_action = act[:8]                    right_action = act[8:16]                    if not args.disable_puppet_arm:                        ros_operator.puppet_arm_publish(left_action, right_action)  # puppet_arm_publish_continuous_thread                    # 这里是基座轮子，暂不使用                    if args.use_robot_base:                        vel_action = act[14:16]                        ros_operator.robot_base_publish(vel_action)                    rate.sleep()                    # print(f"doing action: {act}")                t += 1                                print("Published Step", t)                pre_action = action.copy()# ROS operator classclass RosOperator:    def __init__(self, args):        self.robot_base_deque = None        self.puppet_arm_right_deque = None        self.puppet_gripper_right_deque = None        self.puppet_arm_left_deque = None        self.puppet_gripper_left_deque = None        self.img_front_deque = None        self.img_right_deque = None        self.img_left_deque = None        self.img_front_depth_deque = None        self.img_right_depth_deque = None        self.img_left_depth_deque = None        self.bridge = None        self.puppet_arm_left_publisher = None        self.puppet_arm_right_publisher = None        self.robot_base_publisher = None        self.puppet_arm_publish_thread = None        self.puppet_arm_publish_lock = None        self.args = args        self.init()        self.init_ros()    def init(self):        self.bridge = CvBridge()        self.img_left_deque = deque()        self.img_right_deque = deque()        self.img_front_deque = deque()        self.img_left_depth_deque = deque()        self.img_right_depth_deque = deque()        self.img_front_depth_deque = deque()        self.puppet_arm_left_deque = deque()        self.puppet_gripper_left_deque = deque()        self.puppet_arm_right_deque = deque()        self.puppet_gripper_right_deque = deque()        self.robot_base_deque = deque()        self.puppet_arm_publish_lock = threading.Lock()        self.puppet_arm_publish_lock.acquire()    def puppet_arm_publish(self, left, right):        # 这里传过来的为左右臂关节和夹爪的值，这里要进行处理，通过两个ros话题进行发布        # joint_state_msg = JointState()        # joint_state_msg.header = Header()        # joint_state_msg.header.stamp = rospy.Time.now()  # Set timestep        # joint_state_msg.name = ['joint0', 'joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6']  # 设置关节名称        # joint_state_msg.position = left        action_left = Float64MultiArray()        gripper_left = Float64()        action_right = Float64MultiArray()        gripper_right = Float64()        action_left.data = left[:7]        self.puppet_arm_left_publisher.publish(action_left)        gripper_left.data = left[-1]        self.puppet_gripper_left_publisher.publish(gripper_left)        action_right.data = right[:7]        self.puppet_arm_right_publisher.publish(action_right)        gripper_right.data = right[-1]        self.puppet_gripper_right_publisher.publish(gripper_right)    def robot_base_publish(self, vel):        vel_msg = Twist()        vel_msg.linear.x = vel[0]        vel_msg.linear.y = 0        vel_msg.linear.z = 0        vel_msg.angular.x = 0        vel_msg.angular.y = 0        vel_msg.angular.z = vel[1]        self.robot_base_publisher.publish(vel_msg)    def puppet_arm_publish_continuous(self, left, right):        rate = rospy.Rate(self.args.publish_rate)        left_arm = None        right_arm = None        while True and not rospy.is_shutdown():            if len(self.puppet_arm_left_deque) != 0:                left_arm = list(self.puppet_arm_left_deque[-1].position)            if len(self.puppet_arm_right_deque) != 0:                right_arm = list(self.puppet_arm_right_deque[-1].position)            if left_arm is None or right_arm is None:                rate.sleep()                continue            else:                break        left_symbol = [1 if left[i] - left_arm[i] > 0 else -1 for i in range(len(left))]        right_symbol = [1 if right[i] - right_arm[i] > 0 else -1 for i in range(len(right))]        flag = True        step = 0        while flag and not rospy.is_shutdown():            if self.puppet_arm_publish_lock.acquire(False):                return            left_diff = [abs(left[i] - left_arm[i]) for i in range(len(left))]            right_diff = [abs(right[i] - right_arm[i]) for i in range(len(right))]            flag = False            for i in range(len(left)):                if left_diff[i] < self.args.arm_steps_length[i]:                    left_arm[i] = left[i]                else:                    left_arm[i] += left_symbol[i] * self.args.arm_steps_length[i]                    flag = True            for i in range(len(right)):                if right_diff[i] < self.args.arm_steps_length[i]:                    right_arm[i] = right[i]                else:                    right_arm[i] += right_symbol[i] * self.args.arm_steps_length[i]                    flag = True            joint_state_msg = JointState()            joint_state_msg.header = Header()            joint_state_msg.header.stamp = rospy.Time.now()  # Set the timestep            joint_state_msg.name = ['joint0', 'joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6']  # 设置关节名称            joint_state_msg.position = left_arm            self.puppet_arm_left_publisher.publish(joint_state_msg)            joint_state_msg.position = right_arm            self.puppet_arm_right_publisher.publish(joint_state_msg)            step += 1            print("puppet_arm_publish_continuous:", step)            rate.sleep()    def puppet_arm_publish_linear(self, left, right):        num_step = 100        rate = rospy.Rate(200)        left_arm = None        right_arm = None        while True and not rospy.is_shutdown():            if len(self.puppet_arm_left_deque) != 0:                left_arm = list(self.puppet_arm_left_deque[-1].position)            if len(self.puppet_arm_right_deque) != 0:                right_arm = list(self.puppet_arm_right_deque[-1].position)            if left_arm is None or right_arm is None:                rate.sleep()                continue            else:                break        traj_left_list = np.linspace(left_arm, left, num_step)        traj_right_list = np.linspace(right_arm, right, num_step)        for i in range(len(traj_left_list)):            traj_left = traj_left_list[i]            traj_right = traj_right_list[i]            traj_left[-1] = left[-1]            traj_right[-1] = right[-1]            joint_state_msg = JointState()            joint_state_msg.header = Header()            joint_state_msg.header.stamp = rospy.Time.now()  # 设置时间戳            joint_state_msg.name = ['joint0', 'joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6']  # 设置关节名称            joint_state_msg.position = traj_left            self.puppet_arm_left_publisher.publish(joint_state_msg)            joint_state_msg.position = traj_right            self.puppet_arm_right_publisher.publish(joint_state_msg)            rate.sleep()    def puppet_arm_publish_continuous_thread(self, left, right):        if self.puppet_arm_publish_thread is not None:            self.puppet_arm_publish_lock.release()            self.puppet_arm_publish_thread.join()            self.puppet_arm_publish_lock.acquire(False)            self.puppet_arm_publish_thread = None        self.puppet_arm_publish_thread = threading.Thread(target=self.puppet_arm_publish_continuous, args=(left, right))        self.puppet_arm_publish_thread.start()    def get_frame(self):        if len(self.img_left_deque) == 0 or len(self.img_right_deque) == 0 or len(self.img_front_deque) == 0 or \                (self.args.use_depth_image and (len(self.img_left_depth_deque) == 0 or len(self.img_right_depth_deque) == 0 or len(self.img_front_depth_deque) == 0)):            return False        if self.args.use_depth_image:            frame_time = min([self.img_left_deque[-1].header.stamp.to_sec(), self.img_right_deque[-1].header.stamp.to_sec(), self.img_front_deque[-1].header.stamp.to_sec(),                              self.img_left_depth_deque[-1].header.stamp.to_sec(), self.img_right_depth_deque[-1].header.stamp.to_sec(), self.img_front_depth_deque[-1].header.stamp.to_sec()])        else:            frame_time = min([self.img_left_deque[-1].header.stamp.to_sec(), self.img_right_deque[-1].header.stamp.to_sec(), self.img_front_deque[-1].header.stamp.to_sec()])        # if len(self.img_left_deque) == 0 or self.img_left_deque[-1].header.stamp.to_sec() < frame_time:        #     return False        # if len(self.img_right_deque) == 0 or self.img_right_deque[-1].header.stamp.to_sec() < frame_time:        #     return False        # if len(self.img_front_deque) == 0 or self.img_front_deque[-1].header.stamp.to_sec() < frame_time:        #     return False        # if len(self.puppet_arm_left_deque) == 0 or self.puppet_arm_left_deque[-1].header.stamp.to_sec() < frame_time:        #     return False        # if len(self.puppet_arm_right_deque) == 0 or self.puppet_arm_right_deque[-1].header.stamp.to_sec() < frame_time:        #     return False        # if self.args.use_depth_image and (len(self.img_left_depth_deque) == 0 or self.img_left_depth_deque[-1].header.stamp.to_sec() < frame_time):        #     return False        # if self.args.use_depth_image and (len(self.img_right_depth_deque) == 0 or self.img_right_depth_deque[-1].header.stamp.to_sec() < frame_time):        #     return False        # if self.args.use_depth_image and (len(self.img_front_depth_deque) == 0 or self.img_front_depth_deque[-1].header.stamp.to_sec() < frame_time):        #     return False        # if self.args.use_robot_base and (len(self.robot_base_deque) == 0 or self.robot_base_deque[-1].header.stamp.to_sec() < frame_time):        #     return False        while self.img_left_deque[0].header.stamp.to_sec() < frame_time:            self.img_left_deque.popleft()        img_left = self.bridge.imgmsg_to_cv2(self.img_left_deque.popleft(), 'passthrough')        while self.img_right_deque[0].header.stamp.to_sec() < frame_time:            self.img_right_deque.popleft()        img_right = self.bridge.imgmsg_to_cv2(self.img_right_deque.popleft(), 'passthrough')        while self.img_front_deque[0].header.stamp.to_sec() < frame_time:            self.img_front_deque.popleft()        img_front = self.bridge.imgmsg_to_cv2(self.img_front_deque.popleft(), 'passthrough')        # while self.puppet_arm_left_deque[0].header.stamp.to_sec() < frame_time:        #     self.puppet_arm_left_deque.popleft()        puppet_arm_left = self.puppet_arm_left_deque.popleft()        # while self.puppet_arm_right_deque[0].header.stamp.to_sec() < frame_time:        #     self.puppet_arm_right_deque.popleft()        puppet_arm_right = self.puppet_arm_right_deque.popleft()        # 这里需要对机械臂进行夹爪的参数拼接        puppet_gripper_left = self.puppet_gripper_left_deque.popleft()        puppet_gripper_right = self.puppet_gripper_right_deque.popleft()        puppet_arm_left = puppet_arm_left + (puppet_gripper_left, )        puppet_arm_right = puppet_arm_right + (puppet_gripper_right, )        img_left_depth = None        if self.args.use_depth_image:            while self.img_left_depth_deque[0].header.stamp.to_sec() < frame_time:                self.img_left_depth_deque.popleft()            img_left_depth = self.bridge.imgmsg_to_cv2(self.img_left_depth_deque.popleft(), 'passthrough')        img_right_depth = None        if self.args.use_depth_image:            while self.img_right_depth_deque[0].header.stamp.to_sec() < frame_time:                self.img_right_depth_deque.popleft()            img_right_depth = self.bridge.imgmsg_to_cv2(self.img_right_depth_deque.popleft(), 'passthrough')        img_front_depth = None        if self.args.use_depth_image:            while self.img_front_depth_deque[0].header.stamp.to_sec() < frame_time:                self.img_front_depth_deque.popleft()            img_front_depth = self.bridge.imgmsg_to_cv2(self.img_front_depth_deque.popleft(), 'passthrough')        robot_base = None        if self.args.use_robot_base:            while self.robot_base_deque[0].header.stamp.to_sec() < frame_time:                self.robot_base_deque.popleft()            robot_base = self.robot_base_deque.popleft()        return (img_front, img_left, img_right, img_front_depth, img_left_depth, img_right_depth,                puppet_arm_left, puppet_arm_right, robot_base)    def img_left_callback(self, msg):        if len(self.img_left_deque) >= 2000:            self.img_left_deque.popleft()        self.img_left_deque.append(msg)    def img_right_callback(self, msg):        if len(self.img_right_deque) >= 2000:            self.img_right_deque.popleft()        self.img_right_deque.append(msg)    def img_front_callback(self, msg):        if len(self.img_front_deque) >= 2000:            self.img_front_deque.popleft()        self.img_front_deque.append(msg)    def img_left_depth_callback(self, msg):        if len(self.img_left_depth_deque) >= 2000:            self.img_left_depth_deque.popleft()        self.img_left_depth_deque.append(msg)    def img_right_depth_callback(self, msg):        if len(self.img_right_depth_deque) >= 2000:            self.img_right_depth_deque.popleft()        self.img_right_depth_deque.append(msg)    def img_front_depth_callback(self, msg):        if len(self.img_front_depth_deque) >= 2000:            self.img_front_depth_deque.popleft()        self.img_front_depth_deque.append(msg)    def puppet_arm_callback(self, msg):        if len(self.puppet_arm_left_deque) >= 2000:            self.puppet_arm_left_deque.popleft()        self.puppet_arm_left_deque.append(msg.position[:7])        if len(self.puppet_arm_right_deque) >= 2000:            self.puppet_arm_right_deque.popleft()        self.puppet_arm_right_deque.append(msg.position[7:])    def puppet_gripper_left_callback(self, msg):        if len(self.puppet_gripper_left_deque) >= 2000:            self.puppet_gripper_left_deque.popleft()        self.puppet_gripper_left_deque.append(msg.data)    def puppet_gripper_right_callback(self, msg):        if len(self.puppet_gripper_right_deque) >= 2000:            self.puppet_gripper_right_deque.popleft()        self.puppet_gripper_right_deque.append(msg.data)    # def puppet_arm_left_callback(self, msg):    #     if len(self.puppet_arm_left_deque) >= 2000:    #         self.puppet_arm_left_deque.popleft()    #     self.puppet_arm_left_deque.append(msg)    #    # def puppet_arm_right_callback(self, msg):    #     if len(self.puppet_arm_right_deque) >= 2000:    #         self.puppet_arm_right_deque.popleft()    #     self.puppet_arm_right_deque.append(msg)    def robot_base_callback(self, msg):        if len(self.robot_base_deque) >= 2000:            self.robot_base_deque.popleft()        self.robot_base_deque.append(msg)    def init_ros(self):        rospy.init_node('joint_state_publisher', anonymous=True)        rospy.Subscriber(self.args.img_left_topic, Image, self.img_left_callback, queue_size=1000, tcp_nodelay=True)        rospy.Subscriber(self.args.img_right_topic, Image, self.img_right_callback, queue_size=1000, tcp_nodelay=True)        rospy.Subscriber(self.args.img_front_topic, Image, self.img_front_callback, queue_size=1000, tcp_nodelay=True)        if self.args.use_depth_image:            rospy.Subscriber(self.args.img_left_depth_topic, Image, self.img_left_depth_callback, queue_size=1000, tcp_nodelay=True)            rospy.Subscriber(self.args.img_right_depth_topic, Image, self.img_right_depth_callback, queue_size=1000, tcp_nodelay=True)            rospy.Subscriber(self.args.img_front_depth_topic, Image, self.img_front_depth_callback, queue_size=1000, tcp_nodelay=True)        # 由于我们的机器人是直接从joint_states话题中获取数据的，因此需要对这里进行修改        # 同时这里需要对puppet_arm_left_callback和puppet_arm_right_callback整合为一个函数        rospy.Subscriber(self.args.puppet_arm_topic, JointState, self.puppet_arm_callback, queue_size=1000, tcp_nodelay=True)        # 订阅两个夹爪        rospy.Subscriber('/inspire_gripper/leftget_copen_read_50hz', Float64, self.puppet_gripper_left_callback, queue_size=1000, tcp_nodelay=True)        rospy.Subscriber('/inspire_gripper/rightget_copen_read_50hz', Float64, self.puppet_gripper_right_callback, queue_size=1000, tcp_nodelay=True)        # rospy.Subscriber(self.args.puppet_arm_right_topic, JointState, self.puppet_arm_right_callback, queue_size=1000, tcp_nodelay=True)        # rospy.Subscriber(self.args.robot_base_topic, Odometry, self.robot_base_callback, queue_size=1000, tcp_nodelay=True)        # 发布话题配置        self.puppet_arm_left_publisher = rospy.Publisher(self.args.puppet_arm_left_cmd_topic, Float64MultiArray, queue_size=10)        self.puppet_arm_right_publisher = rospy.Publisher(self.args.puppet_arm_right_cmd_topic, Float64MultiArray, queue_size=10)        self.puppet_gripper_left_publisher = rospy.Publisher(self.args.puppet_gripper_left_cmd_topic, Float64, queue_size=10)        self.puppet_gripper_right_publisher = rospy.Publisher(self.args.puppet_gripper_right_cmd_topic, Float64, queue_size=10)        self.robot_base_publisher = rospy.Publisher(self.args.robot_base_cmd_topic, Twist, queue_size=10)def get_arguments():    # 这里配置我们机器人的各个话题订阅    parser = argparse.ArgumentParser()    parser.add_argument('--max_publish_step', action='store', type=int,                         help='Maximum number of action publishing steps', default=10000, required=False)    parser.add_argument('--seed', action='store', type=int,                         help='Random seed', default=None, required=False)    parser.add_argument('--img_front_topic', action='store', type=str, help='img_front_topic',                        default='/cam_high/color/image_raw', required=False)    parser.add_argument('--img_left_topic', action='store', type=str, help='img_left_topic',                        default='/cam_left_wrist/color/image_raw', required=False)    parser.add_argument('--img_right_topic', action='store', type=str, help='img_right_topic',                        default='/cam_right_wrist/color/image_raw', required=False)        parser.add_argument('--img_front_depth_topic', action='store', type=str, help='img_front_depth_topic',                        default='/cam_high/depth/image_raw', required=False)    parser.add_argument('--img_left_depth_topic', action='store', type=str, help='img_left_depth_topic',                        default='/cam_left_wrist/depth/image_raw', required=False)    parser.add_argument('--img_right_depth_topic', action='store', type=str, help='img_right_depth_topic',                        default='/cam_right_wrist/depth/image_raw', required=False)    # cmd为发布的话题    parser.add_argument('--puppet_arm_left_cmd_topic', action='store', type=str, help='puppet_arm_left_cmd_topic',                        default='left_arm_controller_position/command', required=False)    parser.add_argument('--puppet_arm_right_cmd_topic', action='store', type=str, help='puppet_arm_right_cmd_topic',                        default='right_arm_controller_position/command', required=False)    parser.add_argument('--puppet_gripper_left_cmd_topic', action='store', type=str, help='puppet_gripper_left_cmd_topic',                        default='inspire_gripper/leftset_copen_50hz', required=False)    parser.add_argument('--puppet_gripper_right_cmd_topic', action='store', type=str, help='puppet_gripper_right_cmd_topic',                        default='inspire_gripper/rightset_copen_50hz', required=False)    # 无cmd的为订阅的话题，其中左臂右臂的关节参数从joint_states中直接获取    # parser.add_argument('--puppet_arm_left_topic', action='store', type=str, help='puppet_arm_left_topic',    #                     default='left_arm_controller_position/command', required=False)    parser.add_argument('--puppet_arm_topic', action='store', type=str, help='puppet_arm_topic',                        default='/joint_states', required=False)    # 这里是夹爪的订阅话题    parser.add_argument('--puppet_gripper_left_topic', action='store', type=str, help='puppet_gripper_left_topic',                        default='/inspire_gripper/leftget_copen_read_50hz', required=False)    parser.add_argument('--puppet_gripper_right_topic', action='store', type=str, help='puppet_gripper_right_topic',                        default='/inspire_gripper/rightget_copen_read_50hz', required=False)        parser.add_argument('--robot_base_topic', action='store', type=str, help='robot_base_topic',                        default='/odom_raw', required=False)    parser.add_argument('--robot_base_cmd_topic', action='store', type=str, help='robot_base_topic',                        default='/cmd_vel', required=False)    parser.add_argument('--use_robot_base', action='store_true',                         help='Whether to use the robot base to move around',                        default=False, required=False)    parser.add_argument('--publish_rate', action='store', type=int,                         help='The rate at which to publish the actions',                        default=30, required=False)    parser.add_argument('--ctrl_freq', action='store', type=int,                         help='The control frequency of the robot',                        default=1, required=False)        parser.add_argument('--chunk_size', action='store', type=int,                         help='Action chunk size',                        default=64, required=False)    parser.add_argument('--arm_steps_length', action='store', type=float,                         help='The maximum change allowed for each joint per timestep',                        default=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.2], required=False)    parser.add_argument('--use_actions_interpolation', action='store_true',                        help='Whether to interpolate the actions if the difference is too large',                        default=False, required=False)    parser.add_argument('--use_depth_image', action='store_true',                         help='Whether to use depth images',                        default=False, required=False)        parser.add_argument('--disable_puppet_arm', action='store_true',                        help='Whether to disable the puppet arm. This is useful for safely debugging',default=False)        parser.add_argument('--config_path', type=str, default="configs/base.yaml",                         help='Path to the config file')    # parser.add_argument('--cfg_scale', type=float, default=2.0,    #                     help='the scaling factor used to modify the magnitude of the control features during denoising')    parser.add_argument('--pretrained_model_name_or_path', type=str, required=True, help='Name or path to the pretrained model')        parser.add_argument('--lang_embeddings_path', type=str, required=True,                         help='Path to the pre-encoded language instruction embeddings')        args = parser.parse_args()    return argsdef main():    args = get_arguments()    ros_operator = RosOperator(args)    if args.seed is not None:        set_seed(args.seed)    config = get_config(args)    model_inference(args, config, ros_operator)if __name__ == '__main__':    main()</code></pre></li><li><p>agilex_model的代码修改</p><p>AGILEX_STATE_INDICES维度设置</p><pre class=" language-language-python"><code class="language-language-python">AGILEX_STATE_INDICES = [    STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"] for i in range(7)] + [    STATE_VEC_IDX_MAPPING["left_gripper_open"]] + [    STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(7)] + [    STATE_VEC_IDX_MAPPING[f"right_gripper_open"]]</code></pre><p>joint维度设置(L177,L215)</p><pre class=" language-language-python"><code class="language-language-python">joints = joints / torch.tensor(   [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],   device=joints.device, dtype=joints.dtype)</code></pre><p>python -m scripts.encode_lang</p><p>python -m scripts.agilex_inference --pretrained_model_name_or_path= --lang_embeddings_path=outs/pusht.pt --ctrl_freq=50 --use_actions_interpolation</p></li></ul></li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具身智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aloha遥操作数据采集源代码</title>
      <link href="/2024/11/10/aloha-yao-cao-zuo-shu-ju-cai-ji/"/>
      <url>/2024/11/10/aloha-yao-cao-zuo-shu-ju-cai-ji/</url>
      
        <content type="html"><![CDATA[<h3 id="aloha遥操作数据采集源代码">aloha遥操作数据采集源代码</h3><p>1.ROS订阅话题的更改</p><p>2.无关参数的省略</p><p>3.回调函数参数赋值的方式</p><h4 id="record-episodes-py">record_episodes.py</h4><p>主动臂构建与位姿初始化</p><pre class=" language-language-python"><code class="language-language-python"># 构建机器人两条主动机械臂master_bot_left = InterbotixManipulatorXS(robot_model="wx250s", group_name="arm", gripper_name="gripper",                                              robot_name=f'master_left', init_node=True)    master_bot_right = InterbotixManipulatorXS(robot_model="wx250s", group_name="arm", gripper_name="gripper",                                               robot_name=f'master_right', init_node=False)# 创建机械臂环境env = make_real_env(init_node=False, setup_robots=False)# 将主动与从动臂设置为初始状态 需要修改为通过ROS控制传入四条机械臂以及夹爪的位姿将机器人设置为初始态# 遥操作采集数据设定夹爪开启后opening_ceremony(master_bot_left, master_bot_right, env.puppet_bot_left, env.puppet_bot_right)</code></pre><p>夹爪距离控制遥操作数据采集</p><pre class=" language-language-python"><code class="language-language-python">def opening_ceremony(master_bot_left, master_bot_right, puppet_bot_left, puppet_bot_right):    """ Move all 4 robots to a pose where it is easy to start demonstration """    # reboot gripper motors, and set operating modes for all motors    # 配置所有臂的电机模式，这里是否有对应的修改询问周博    # puppet_bot_left.dxl.robot_reboot_motors("single", "gripper", True)    # puppet_bot_left.dxl.robot_set_operating_modes("group", "arm", "position")    # puppet_bot_left.dxl.robot_set_operating_modes("single", "gripper", "current_based_position")    # master_bot_left.dxl.robot_set_operating_modes("group", "arm", "position")    # master_bot_left.dxl.robot_set_operating_modes("single", "gripper", "position")    # # puppet_bot_left.dxl.robot_set_motor_registers("single", "gripper", 'current_limit', 1000) # TODO(tonyzhaozh) figure out how to set this limit    #    # puppet_bot_right.dxl.robot_reboot_motors("single", "gripper", True)    # puppet_bot_right.dxl.robot_set_operating_modes("group", "arm", "position")    # puppet_bot_right.dxl.robot_set_operating_modes("single", "gripper", "current_based_position")    # master_bot_right.dxl.robot_set_operating_modes("group", "arm", "position")    # master_bot_right.dxl.robot_set_operating_modes("single", "gripper", "position")    # # puppet_bot_left.dxl.robot_set_motor_registers("single", "gripper", 'current_limit', 1000) # TODO(tonyzhaozh) figure out how to set this limit    #    # torque_on(puppet_bot_left)    # torque_on(master_bot_left)    # torque_on(puppet_bot_right)    # torque_on(master_bot_right)    # move arms to starting position    # 控制机械臂与夹爪的初始位姿，需要修改为自己的API    # start_arm_qpos = START_ARM_POSE[:6]    # move_arms([master_bot_left, puppet_bot_left, master_bot_right, puppet_bot_right], [start_arm_qpos] * 4, move_time=1.5)    # move grippers to starting position    # move_grippers([master_bot_left, puppet_bot_left, master_bot_right, puppet_bot_right], [MASTER_GRIPPER_JOINT_MID, PUPPET_GRIPPER_JOINT_CLOSE] * 2, move_time=0.5)    # press gripper to start data collection    # disable torque for only gripper joint of master robot to allow user movement    # master_bot_left.dxl.robot_torque_enable("single", "gripper", False)    # master_bot_right.dxl.robot_torque_enable("single", "gripper", False)    # 这里的逻辑为禁用主动臂的电机，让用户操作主动臂    # 以夹爪的姿态为判断，关闭夹爪开始进行数据采集    # 原代码机械臂使用的是夹爪距离数值，具体的情况具体修改    print(f'Close the gripper to start')    close_thresh = -1.4    pressed = False    while not pressed:        gripper_pos_left = get_arm_gripper_positions(master_bot_left)        gripper_pos_right = get_arm_gripper_positions(master_bot_right)        if (gripper_pos_left < close_thresh) and (gripper_pos_right < close_thresh):            pressed = True        time.sleep(DT/10)    # 关闭主动臂的电机    # torque_off(master_bot_left)    # torque_off(master_bot_right)    print(f'Started!')</code></pre><p>环境初始化</p><pre class=" language-language-python"><code class="language-language-python">    def __init__(self, init_node, setup_robots=True, setup_base=False):        #self.puppet_bot_left = InterbotixManipulatorXS(robot_model="vx300s", group_name="arm", gripper_name="gripper",        #                                               robot_name=f'puppet_left', init_node=init_node)        #self.puppet_bot_right = InterbotixManipulatorXS(robot_model="vx300s", group_name="arm", gripper_name="gripper",        #                                                 robot_name=f'puppet_right', init_node=False)        # if setup_robots:            # 启动从动臂及夹爪的电机,自己实现            # self.torque_on()            # self.setup_robots()                # if setup_base:            # 进行设备通信            # self.setup_base()                # self.setup_t265()        # 控制轮子，暂不需要        # self.setup_dxl()        # 配置两个从臂，需要自己修改话题        self.recorder_left = Recorder('left', init_node=False)        self.recorder_right = Recorder('right', init_node=False)        # 相机话题订阅        self.image_recorder = ImageRecorder(init_node=False)        # 单独命令夹爪关节，这里需要修改        self.gripper_command = None</code></pre><p>订阅话题，回调函数对参数赋值</p><pre class=" language-language-python"><code class="language-language-python">class Recorder:    # mc为删除标记    def __init__(self, side, init_node=True, is_debug=False):        from collections import deque        import rospy        from sensor_msgs.msg import JointState        from interbotix_xs_msgs.msg import JointGroupCommand, JointSingleCommand        self.secs = None        self.nsecs = None        self.qpos = None        self.qvel = None        self.effort = None        self.arm_command = None        self.gripper_command = None        self.is_debug = is_debug        if init_node:            rospy.init_node('recorder', anonymous=True)        # 更改为自己的订阅话题来订阅参数        # 关节消息，关节组及夹爪单个关节        rospy.Subscriber(f"/puppet_{side}/joint_states", JointState, self.puppet_state_cb)        # rospy.Subscriber(f"/puppet_{side}/commands/joint_group", JointGroupCommand, self.puppet_arm_commands_cb)        # 这里为夹爪的订阅        rospy.Subscriber(f"/puppet_{side}/commands/joint_single", JointSingleCommand, self.puppet_gripper_commands_cb)        if self.is_debug:            self.joint_timestamps = deque(maxlen=50)            self.arm_command_timestamps = deque(maxlen=50)            self.gripper_command_timestamps = deque(maxlen=50)        time.sleep(0.1)</code></pre><p>image使用realsense，ros订阅话题数据结构相似</p><pre class=" language-language-python"><code class="language-language-python">    def __init__(self, init_node=True, is_debug=False):        from collections import deque        import rospy        from cv_bridge import CvBridge        from sensor_msgs.msg import Image        self.is_debug = is_debug        self.bridge = CvBridge()        # 三个相机的名称        self.camera_names = ['cam_high', 'cam_left_wrist', 'cam_right_wrist'] #['cam_high', 'cam_low', 'cam_left_wrist', 'cam_right_wrist']        if init_node:            rospy.init_node('image_recorder', anonymous=True)        for cam_name in self.camera_names:            setattr(self, f'{cam_name}_image', None)            setattr(self, f'{cam_name}_secs', None)            setattr(self, f'{cam_name}_nsecs', None)            if cam_name == 'cam_high':                callback_func = self.image_cb_cam_high            elif cam_name == 'cam_low':                callback_func = self.image_cb_cam_low            elif cam_name == 'cam_left_wrist':                callback_func = self.image_cb_cam_left_wrist            elif cam_name == 'cam_right_wrist':                callback_func = self.image_cb_cam_right_wrist            else:                raise NotImplementedError            rospy.Subscriber(f"/usb_{cam_name}/color/image_raw", Image, callback_func)            if self.is_debug:                setattr(self, f'{cam_name}_timestamps', deque(maxlen=50))        time.sleep(0.5)</code></pre><p>遥操作数据采集过程</p><pre class=" language-language-python"><code class="language-language-python"># 环境重置ts = env.reset(fake=True)# 构建字典获取的参数obs = collections.OrderedDict()# qpos为从动臂及对应夹爪的位姿订阅obs['qpos'] = self.get_qpos()# qvel为从动臂及对应夹爪的速度参数obs['qvel'] = self.get_qvel()# effort为从动臂的力矩obs['effort'] = self.get_effort()# images为拍摄的图像obs['images'] = self.get_images()# obs['base_vel_t265'] = self.get_base_vel_t265()# 移动机器人基座的线速度和角速度obs['base_vel'] = self.get_base_vel()# 以上的所有参数获取和封装改为从ros中获取</code></pre><pre class=" language-language-python"><code class="language-language-python"># 这里获取主动臂及夹爪的位姿信息 需要替换为ros获取action = get_action(master_bot_left, master_bot_right)</code></pre><pre class=" language-language-python"><code class="language-language-python"># 这里将主动臂及夹爪的位姿赋给从动臂 再次获取参数字典ts = env.step(action)</code></pre><pre class=" language-language-python"><code class="language-language-python"># 打开主动臂扭矩和从动臂的夹爪# Torque on both master botstorque_on(master_bot_left)torque_on(master_bot_right)# Open puppet grippersenv.puppet_bot_left.dxl.robot_set_operating_modes("single", "gripper", "position")env.puppet_bot_right.dxl.robot_set_operating_modes("single", "gripper", "position")# 遥操作采集完成夹爪移动？什么作用？move_grippers([env.puppet_bot_left, env.puppet_bot_right], [PUPPET_GRIPPER_JOINT_OPEN] * 2, move_time=0.5)</code></pre><p>数据采集与图像压缩</p><pre class=" language-language-python"><code class="language-language-python">data_dict = {        '/observations/qpos': [],        '/observations/qvel': [],        '/observations/effort': [],        '/action': [],        '/base_action': [],        # '/base_action_t265': [],    }    for cam_name in camera_names:        data_dict[f'/observations/images/{cam_name}'] = []    # len(action): max_timesteps, len(time_steps): max_timesteps + 1    while actions:        action = actions.pop(0)        ts = timesteps.pop(0)        data_dict['/observations/qpos'].append(ts.observation['qpos'])        data_dict['/observations/qvel'].append(ts.observation['qvel'])        data_dict['/observations/effort'].append(ts.observation['effort'])        data_dict['/action'].append(action)        data_dict['/base_action'].append(ts.observation['base_vel'])        # data_dict['/base_action_t265'].append(ts.observation['base_vel_t265'])        for cam_name in camera_names:            data_dict[f'/observations/images/{cam_name}'].append(ts.observation['images'][cam_name])        # plot /base_action vs /base_action_t265    # import matplotlib.pyplot as plt    # plt.plot(np.array(data_dict['/base_action'])[:, 0], label='base_action_linear')    # plt.plot(np.array(data_dict['/base_action'])[:, 1], label='base_action_angular')    # plt.plot(np.array(data_dict['/base_action_t265'])[:, 0], '--', label='base_action_t265_linear')    # plt.plot(np.array(data_dict['/base_action_t265'])[:, 1], '--', label='base_action_t265_angular')    # plt.legend()    # plt.savefig('record_episodes_vel_debug.png', dpi=300)    COMPRESS = True    if COMPRESS:        # JPEG compression        t0 = time.time()        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50] # tried as low as 20, seems fine        compressed_len = []        for cam_name in camera_names:            image_list = data_dict[f'/observations/images/{cam_name}']            compressed_list = []            compressed_len.append([])            for image in image_list:                result, encoded_image = cv2.imencode('.jpg', image, encode_param) # 0.02 sec # cv2.imdecode(encoded_image, 1)                compressed_list.append(encoded_image)                compressed_len[-1].append(len(encoded_image))            data_dict[f'/observations/images/{cam_name}'] = compressed_list        print(f'compression: {time.time() - t0:.2f}s')        # pad so it has same length        t0 = time.time()        compressed_len = np.array(compressed_len)        padded_size = compressed_len.max()        for cam_name in camera_names:            compressed_image_list = data_dict[f'/observations/images/{cam_name}']            padded_compressed_image_list = []            for compressed_image in compressed_image_list:                padded_compressed_image = np.zeros(padded_size, dtype='uint8')                image_len = len(compressed_image)                padded_compressed_image[:image_len] = compressed_image                padded_compressed_image_list.append(padded_compressed_image)            data_dict[f'/observations/images/{cam_name}'] = padded_compressed_image_list        print(f'padding: {time.time() - t0:.2f}s')    # HDF5    t0 = time.time()    with h5py.File(dataset_path + '.hdf5', 'w', rdcc_nbytes=1024**2*2) as root:        root.attrs['sim'] = False        root.attrs['compress'] = COMPRESS        obs = root.create_group('observations')        image = obs.create_group('images')        for cam_name in camera_names:            if COMPRESS:                _ = image.create_dataset(cam_name, (max_timesteps, padded_size), dtype='uint8',                                         chunks=(1, padded_size), )            else:                _ = image.create_dataset(cam_name, (max_timesteps, 480, 640, 3), dtype='uint8',                                         chunks=(1, 480, 640, 3), )        _ = obs.create_dataset('qpos', (max_timesteps, 14))        _ = obs.create_dataset('qvel', (max_timesteps, 14))        _ = obs.create_dataset('effort', (max_timesteps, 14))        _ = root.create_dataset('action', (max_timesteps, 14))        _ = root.create_dataset('base_action', (max_timesteps, 2))        # _ = root.create_dataset('base_action_t265', (max_timesteps, 2))        for name, array in data_dict.items():            root[name][...] = array        if COMPRESS:            _ = root.create_dataset('compress_len', (len(camera_names), max_timesteps))            root['/compress_len'][...] = compressed_len    print(f'Saving: {time.time() - t0:.1f} secs')    return True</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 具身智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 遥操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GLM多模态大模型部署及调用</title>
      <link href="/2024/09/27/glm-duo-mo-tai-da-mo-xing-bu-shu-ji-diao-yong/"/>
      <url>/2024/09/27/glm-duo-mo-tai-da-mo-xing-bu-shu-ji-diao-yong/</url>
      
        <content type="html"><![CDATA[<h3 id="GLM大模态大模型部署">GLM大模态大模型部署</h3><p><strong>基本API调用函数来自官方</strong></p><p>服务器端</p><pre class=" language-language-python"><code class="language-language-python">from fastapi import FastAPIfrom pydantic import BaseModelimport uvicornimport requestsfrom io import BytesIOfrom modelscope import snapshot_downloadfrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchfrom PIL import Imageimport base64import cv2import numpy as npimport iomodel_dir = '/home/dgc/glm-4v-9b'device = "cuda:0"tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)glm4_vl = AutoModelForCausalLM.from_pretrained(model_dir, device_map=device, trust_remote_code=True,                                               torch_dtype=torch.float16).eval()# 创建FastAPI应用实例app = FastAPI()# 定义请求体模型，与OpenAI API兼容class ChatCompletionRequest(BaseModel):    model: str    messages: list    max_tokens: int = 50    temperature: float = 1.0# 文本生成函数def generate_text(model: str, messages: list, max_tokens: int, temperature: float):    query = messages[0]["content"][0]["text"]    image_base64 = messages[0]["content"][1]["image_base64"]["base64"]    image = Image.open(BytesIO(base64.b64decode(image_base64)))    inputs = tokenizer.apply_chat_template([{"role": "user", "image": image, "content": query}],                                           add_generation_prompt=True, tokenize=True, return_tensors="pt",                                           return_dict=True)    inputs = inputs.to(device)    gen_kwargs = {"max_length": 2500, "do_sample": True, "top_k": 1}    with torch.no_grad():        outputs = glm4_vl.generate(**inputs, **gen_kwargs)        outputs = outputs[:, inputs['input_ids'].shape[1]:]        print(tokenizer.decode(outputs[0]))    return tokenizer.decode(outputs[0])# 定义路由和处理函数，与OpenAI API兼容@app.post("/v1/chat/completions")async def create_chat_completion(request: ChatCompletionRequest):    # 调用自定义的文本生成函数    response = generate_text(request.model, request.messages, request.max_tokens, request.temperature)    return {"choices": [{"message": {"content": response}}], "model": request.model}# 启动FastAPI应用if __name__ == "__main__":    uvicorn.run(app, host="0.0.0.0", port=8000)</code></pre><p>客户端</p><pre class=" language-language-python"><code class="language-language-python">import requestsimport jsonimport cv2from PIL import Imageimport subprocessimport base64# 定义请求的URLurl = "http://ipaddress:8000/v1/chat/completions"# 本地将图片信息转换为base64编码上传image_url = './circle.png'f = open(image_url, 'rb')  # 二进制方式打开图文件image = base64.b64encode(f.read())# 定义请求体data = {    "model": "glm-4v",    "messages": [{"role": "user", "content": [{"type": "text", "text": "Now you are a robotic arm. The get_names_on_table() function is used to get the names of all objects on the table and returns a collection. The variable is defined as nams. The get_box_postion() function obtains the positions where all objects on the table need to move. The variable is defined as position, which is fixed. The get_location_by_name(name) function gets the current location of each object. The move_tool(xyz) function is used to move the robotic arm. The grasp() function is used to grasp objects. The ungrasp() function is used to drop objects. Analyze the picture and design the plan method to move the objects on the desktop to the same position on the other side according to the provided function. Please only provide the python code implementation of the plan method without outputting relevant comments."}, {"type": "image_base64",                                                                                      "image_base64": {                                                                                          "base64": image.decode('utf-8')}}]}],    "max_tokens": 1024,    "temperature": 0.5}# 将字典转换为JSON格式headers = {'Content-Type': 'application/json'}data_json = json.dumps(data)# 发送POST请求response = requests.post(url, data=data_json, headers=headers)llm_generated_func = None# 检查响应状态码if response.status_code == 200:    # 如果响应成功，打印响应内容    # print(response.json()['choices'][0]['message']['content'].replace('<|endoftext|>',''))    llm_generated_func = response.json()['choices'][0]['message']['content'].replace('<|endoftext|>','')else:    # 如果响应失败，打印错误信息    print(f"Error: {response.status_code}, {response.text}")</code></pre><p>同时贴一下初期用面阵相机和<a href="https://github.com/OpenRobotLab/EmbodiedScan" target="_blank" rel="noopener">EmbodiedScan</a>做仿真的代码，后来没写完就直接用机械臂了，留存一下吧。</p><pre class=" language-language-python"><code class="language-language-python">from pypylon import pylonimport cv2import numpy as npimport reimport subprocessimport base64import jsonimport requestsimport osimport warningsfrom argparse import ArgumentParserfrom copy import deepcopyfrom pathlib import Pathfrom typing import Optional, Unionimport numpy as npimport torchfrom mmengine.config import Configfrom mmengine.dataset import Compose, pseudo_collatefrom mmengine.registry import init_default_scopefrom mmengine.runner import load_checkpointfrom scipy.spatial.transform import Rotation as Rfrom embodiedscan.registry import DATASETS, MODELSfrom embodiedscan.structures import get_box_typedef init_model(config: Union[str, Path, Config],               checkpoint: Optional[str] = None,               device: str = 'cuda:0',               cfg_options: Optional[dict] = None):    """Initialize a model from config file, which could be a 3D detector or a    3D segmentor.    Args:        config (str, :obj:`Path`, or :obj:`mmengine.Config`): Config file path,            :obj:`Path`, or the config object.        checkpoint (str, optional): Checkpoint path. If left as None, the model            will not load any weights.        device (str): Device to use.        cfg_options (dict, optional): Options to override some settings in            the used config.    Returns:        nn.Module: The constructed detector.    """    if isinstance(config, (str, Path)):        config = Config.fromfile(config)    elif not isinstance(config, Config):        raise TypeError('config must be a filename or Config object, '                        f'but got {type(config)}')    if cfg_options is not None:        config.merge_from_dict(cfg_options)    config.model.train_cfg = None    init_default_scope(config.get('default_scope', 'mmdet3d'))    model = MODELS.build(config.model)    if checkpoint is not None:        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')        # save the dataset_meta in the model for convenience        model.dataset_meta = checkpoint['meta']['dataset_meta']        test_dataset_cfg = deepcopy(config.test_dataloader.dataset)        # lazy init. We only need the metainfo.        test_dataset_cfg['lazy_init'] = True        metainfo = DATASETS.build(test_dataset_cfg).metainfo        cfg_palette = metainfo.get('palette', None)        if cfg_palette is not None:            model.dataset_meta['palette'] = cfg_palette        else:            if 'palette' not in model.dataset_meta:                warnings.warn(                    'palette does not exist, random is used by default. '                    'You can also set the palette to customize.')                model.dataset_meta['palette'] = 'random'    model.cfg = config  # save the config in the model for convenience    if device != 'cpu':        torch.cuda.set_device(device)    else:        warnings.warn('Don\'t suggest using CPU device. '                      'Some functions are not supported for now.')    model.to(device)    model.eval()    return modeldef nms_filter(pred_results, iou_thr=0.15, score_thr=0.075, topk_per_class=10):    """Non-Maximum Suppression for 3D Euler boxes. Additionally, only the top-k    boxes will be kept for each category to avoid redundant boxes in the    visualization.    Args:        pred_results (mmengine.structures.instance_data.InstanceData):            Results predicted by the model        iou_thr (float): IoU thresholds for NMS. Defaults to 0.15.        score_thr (float): Score thresholds.            Instances with scores below thresholds will not be kept.            Defaults to 0.075.        topk_per_class (int): Number of instances kept per category.    Returns:        boxes (numpy.ndarray[float]): filtered instances, shape (N,9)        labels (numpy.ndarray[int]): filtered labels, shape (N,)    """    boxes = pred_results.bboxes_3d    boxes_tensor = boxes.tensor.cpu().numpy()    iou = boxes.overlaps(boxes, boxes, eps=1e-5)    score = pred_results.scores_3d.cpu().numpy()    label = pred_results.labels_3d.cpu().numpy()    selected_per_class = dict()    n = boxes_tensor.shape[0]    idx = list(range(n))    idx.sort(key=lambda x: score[x], reverse=True)    selected_idx = []    for i in idx:        if selected_per_class.get(label[i], 0) >= topk_per_class:            continue        if score[i] < score_thr:            continue        bo = False        for j in selected_idx:            if iou[i][j] > iou_thr:                bo = True                break        if not bo:            selected_idx.append(i)            if label[i] not in selected_per_class:                selected_per_class[label[i]] = 1            else:                selected_per_class[label[i]] += 1    return boxes_tensor[selected_idx], label[selected_idx]# 摄像头采集图像def get_camera_image(image_path):    nodeFile = 'acA2500-14gc_22225910.pfs'    tlf = pylon.TlFactory.GetInstance()    tl = tlf.CreateTl('BaslerGigE')    cam_info = tl.CreateDeviceInfo()    camera = pylon.InstantCamera(tlf.CreateDevice(cam_info))    camera.Open()    # Print the model name of the camera.    print("Using device ", camera.GetDeviceInfo().GetModelName())    # load pfs    pylon.FeaturePersistence.Load(nodeFile, camera.GetNodeMap(), True)    # print(camera.CameraContext())    camera.Width.SetValue(1800)  # (1920)    camera.Height.SetValue(1800)  # (1080)    # 设置中心点    camera.CenterX.SetValue(True)    camera.CenterY.SetValue(True)    camera.LightSourceSelector.SetValue('Daylight6500K')    a = camera.LightSourceSelector.GetValue()    camera.BalanceWhiteAuto.SetValue('Off')    # camera.BalanceWhiteAuto.SetValue('Once')    b = camera.BalanceWhiteAuto.GetValue()    camera.BalanceWhiteReset.Execute()    camera.BalanceRatioSelector.SetValue('Red')    camera.BalanceRatioRaw.SetValue(90)    c = camera.BalanceRatioRaw.GetValue()    camera.BalanceRatioSelector.SetValue('Green')    camera.BalanceRatioRaw.SetValue(59)    d = camera.BalanceRatioRaw.GetValue()    # print(a,b,c,d)    camera.StartGrabbing(pylon.GrabStrategy_LatestImageOnly)    converter = pylon.ImageFormatConverter()    # converting to opencv bgr format    converter.OutputPixelFormat = pylon.PixelType_BGR8packed    converter.OutputBitAlignment = pylon.OutputBitAlignment_MsbAligned    img_original = None    try:        # Create an instant camera object with the camera device found first.        camera.StopGrabbing()        aaaa = True        while aaaa:            # Wait for an image and then retrieve it. A timeout of 5000 ms is used.            # grabResult = camera.RetrieveResult(5000, pylon.TimeoutHandling_ThrowException)            grabResult = camera.GrabOne(1000)            if grabResult.GrabSucceeded():                # Access the image data                image = converter.Convert(grabResult)                img_original = image.GetArray()                cv2.imwrite(image_path, img_original)                aaaa = False            grabResult.Release()        camera.Close()    except Exception as e:        # Error handling.        print("An exception occurred.")# 获取相机标定的外参信息def parse_vectors_from_txt(file_path, rvecs_name="rvecs", tvecs_name="tvecs"):    rvecs = []    tvecs = []    current_list = None    with open(file_path, "r") as file:        lines = file.readlines()    # 解析数据    for line in lines:        line = line.strip()        if line.startswith(rvecs_name + ":"):            current_list = rvecs        elif line.startswith(tvecs_name + ":"):            current_list = tvecs        elif line and current_list is not None:            # 将当前行的数据解析为float并存储到current_list中            values = np.array([float(x) for x in re.split(r',\s*', line)]).reshape(-1, 1)            current_list.append(values)    return tuple(rvecs), tuple(tvecs)# 相机内参矫正def parse_data_from_txt(file_path, mtx_name="mtx", dist_name="dist"):    mtx = None    dist = None    with open(file_path, "r") as file:        data = file.read()    # 使用正则表达式匹配 'mtx' 和 'dist' 部分的内容    mtx_match = re.search(rf"{mtx_name}:\s*\[\[(.*?)\]\]", data, re.DOTALL)    dist_match = re.search(rf"{dist_name}:\s*\[\[(.*?)\]\]", data, re.DOTALL)    if mtx_match:        # 将匹配到的 'mtx' 数据字符串转换为 numpy 数组        mtx_str = mtx_match.group(1).replace("\n", "")        mtx = np.array([[float(num) for num in row.split()] for row in mtx_str.split('] [')])    if dist_match:        # 将匹配到的 'dist' 数据字符串转换为 numpy 数组        dist_str = dist_match.group(1).replace("\n", "")        dist = np.array([[float(num) for num in dist_str.split()]])    return mtx, dist# 机器人基座位姿获取参数def robot(pose_csv_path,rvecs, tvecs, img_point_length):    # 机器人末端在基座标系下的位姿    tool_pose = np.loadtxt(pose_csv_path, delimiter=',')    R_tool = []    t_tool = []    for i in range(int(img_point_length)):        R_tool.append(tool_pose[0:3, 4 * i:4 * i + 3])        t_tool.append(tool_pose[0:3, 4 * i + 3])    R, t = cv2.calibrateHandEye(R_tool, t_tool, rvecs, tvecs, cv2.CALIB_HAND_EYE_TSAI)    T_tool_camera = np.hstack((R, t))    T_tool_camera = np.vstack((T_tool_camera, np.array([0, 0, 0, 1])))    with open("t_tool_camera.txt", "w") as file:        # 写入参数到txt文件        file.write(f"T_tool_camera:{T_tool_camera}\n")    return T_tool_camera# 相机矫正，生成转换参数def distortion(image_path, robot_path, img_point_length):    # 获取相机标定内参数    mtx,dist = parse_data_from_txt('./distortion_correction.txt')    # 获取路径下的所有图片    image_files = [f for f in os.listdir(image_path) if os.path.isfile(os.path.join(image_path, f))]    # 遍历文件，读取并处理图片    for image_file in image_files:        # 获取文件的完整路径        image = os.path.join(image_path, image_file)        # 去畸变        img = cv2.imread(image)        h, w = img.shape[:2]        newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 0, (w, h))  # 自由比例参数        dst = cv2.undistort(img, mtx, dist, None, newcameramtx)        cv2.imwrite(image,dst)# 调用大模型进行场景理解def glm(server_url, image_path, prompt):    # 本地将图片信息转换为base64编码上传    f = open(image_path, 'rb')  # 二进制方式打开图文件    image = base64.b64encode(f.read())    # 定义请求体    data = {        "model": "glm-4v",        "messages": [{"role": "user", "content": [{"type": "text",                                                   "text": prompt},                                                  {"type": "image_base64",                                                   "image_base64": {                                                       "base64": image.decode('utf-8')}}]}],        "max_tokens": 1024,        "temperature": 0.5    }    # 将字典转换为JSON格式    headers = {'Content-Type': 'application/json'}    data_json = json.dumps(data)    # 发送POST请求    response = requests.post(server_url, data=data_json, headers=headers)    llm_generated_func = None    # 检查响应状态码    if response.status_code == 200:        # 如果响应成功，打印响应内容        # print(response.json()['choices'][0]['message']['content'].replace('<|endoftext|>',''))        llm_generated_func = response.json()['choices'][0]['message']['content'].replace('<|endoftext|>', '')    else:        # 如果响应失败，打印错误信息        print(f"Error: {response.status_code}, {response.text}")    return llm_generated_func.replace("```python", "").replace("```", "").strip()# 调用机器人执行def robot_controller(llm_generated_func):    execute = "/media/gongcheng/17dd821e-3aa1-471c-bb84-d9ba1098040a/workspace/EmbodiedScan/manipulate/tasks/exec_llm_code.py"    subprocess.run(['python', execute,llm_generated_func])# 获取3d算法检测的目标框信息def get_detection_box(args):    # 1.这里调用votenet算法进行点云检测并返回json文件结果    # 2.直接调用EmbodiedScan算法获取检测目标的坐标信息    # json_path = votenet(image_path)    # 解析3d点云的检测结果，分析并获取有价值的检测信息    # with open('000017.json', 'r') as fcc_file:    #     jess_dict = json.load(fcc_file)    # scores = jess_dict.get('scores_3d', [])    # labels = jess_dict.get('labels_3d', [])    # boxes = jess_dict.get('bboxes_3d', [])    # detections_box = []    # for index,score in enumerate(scores):    #     if score > 0.5:    #         detections_box.append([labels[index], score, boxes[index]])    # build the model    model = init_model(args.config, args.checkpoint, device=args.device)    cfg = model.cfg    # build the data pipeline    test_pipeline = deepcopy(cfg.test_dataloader.dataset.pipeline)    test_pipeline = Compose(test_pipeline)    # read demo data and construct model input    # 这里需要传入第一个参数，poses.txt为3d相机采集图像时存储的每一帧图片的位姿信息    with open(os.path.join(args.image_path, 'poses.txt'), 'r') as f:        poses = f.readlines()    # 第二个参数对齐矩阵，用于对齐相机采集到的所有图片中的坐标信息    # 相机固定是，对齐矩阵为单位阵    axis_align_matrix = np.loadtxt(        os.path.join(args.image_path, 'axis_align_matrix.txt'))    # 第三个参数，相机的内参    intrinsic = np.loadtxt(os.path.join(args.image_path, 'intrinsic.txt'))    intrinsic = intrinsic.astype(np.float32)    box_type = get_box_type('Euler-Depth')    info = dict(        axis_align_matrix=axis_align_matrix,        images=[],        img_path=[],        depth_img_path=[],        depth2img=dict(extrinsic=[],                       intrinsic=intrinsic,                       origin=np.array([.0, .0, .5]).astype(np.float32)),        depth_cam2img=intrinsic,        depth_shift=1000.0,        cam2img=intrinsic,        box_type_3d=box_type[0],        box_mode_3d=box_type[1],        ann_info=dict(  # empty annotation            gt_bboxes_3d=np.zeros((0, 9), dtype=np.float32),            gt_labels_3d=np.zeros((0,), dtype=np.int64),            visible_instance_masks=[[] for i in range(len(poses))],            gt_occupancy=np.zeros((0, 4), dtype=np.int64),            visible_occupancy_masks=[[] for i in range(len(poses))]))    n_frames = len(poses)    data = []    # 这里现有的设备D415无法采集每帧图片的位姿信息    # 将相机固定你进行标定，获取rvecs和tvecs，计算cam2global，这里也只检测单张图片    for i in range(1, n_frames):        # timestamp, x, y, z, qx, qy, qz, qw = poses[i].split()        # x, y, z, qx, qy, qz, qw = float(x), float(y), float(z), float(        #     qx), float(qy), float(qz), float(qw)        # # 四元数计算旋转矩阵        # rot_matrix = R.from_quat([qx, qy, qz, qw]).as_matrix()        # transform_matrix = np.identity(4)        # transform_matrix[:3, :3] = rot_matrix @ [[0, 0, 1], [-1, 0, 0],        #                                          [0, -1, 0]]        # # 每一帧图片，相机到全局坐标系的转换矩阵        # transform_matrix[:3, 3] = [x, y, z]  # CAM to NOT ALIGNED GLOBAL        # transform_matrix为固定值        timestamp = '0'        transform_matrix =  [[ 0.4378604, 0.79993325, 0.41034749, -1.24956107],                             [-0.84100497, 0.52577555, -0.12755664, -0.88529569],                             [-0.31778747, -0.28925228, 0.90296412, 1.21551931],                             [ 0., 0., 0., 1.]]        image_ann = dict(img_path=os.path.join(args.image_path, 'rgb',                                               timestamp + '.jpg'),                         depth_path=os.path.join(args.image_path, 'depth',                                                 timestamp + '.png'),                         cam2global=transform_matrix,                         cam2img=intrinsic)        info['images'].append(image_ann)        info['img_path'].append(            os.path.join(args.image_path, 'rgb', timestamp + '.jpg'))        info['depth_img_path'].append(            os.path.join(args.image_path, 'depth', timestamp + '.png'))        # 对齐矩阵到世界坐标系进行固定坐标        align_global2cam = np.linalg.inv(axis_align_matrix @ transform_matrix)        info['depth2img']['extrinsic'].append(            align_global2cam.astype(np.float32))    info_ = test_pipeline(info)    data.append(info_)    collate_data = pseudo_collate(data)    # forward the model    with torch.no_grad():        results = model.test_step(collate_data)    # remove model from GPU to free memory    del model    torch.cuda.empty_cache()    filtered_results = []    for i in range(len(results)):        boxes, labels = nms_filter(results[i].pred_instances_3d)        filtered_results.append((boxes, labels))    # 3D检测算法返回的检测框信息已经为世界坐标系坐标    # 获取相机的标定参数后使用机器人位姿参数获取转换参数    rvecs,tvecs = parse_vectors_from_txt(robot_path)    # 根据机器人的运动情况，将机器人位姿参数分为三种    # 1.机械臂固定，位姿参数确定，直接获取存为txt，减少运行时间    # 2.机器人移动，位姿实时变化    # 3.机器人移动，机器人的基坐标以某个固定坐标为基准，位姿不变    t_tool_camera = robot(robot_path, rvecs, tvecs, img_point_length)    # 返回机器人可以识别的坐标信息,这里返回的结果多了一个维度，及机器人可以识别的坐标    detections_box = get_object_position(filtered_results)    return detections_box# 转换为机器人可以识别的坐标def get_object_position(box_position):    outputBase = []    # 将3D检测算法的目标坐标信息通过机器人位姿进行转换为机器人可以理解的坐标    for position in box_position:        matrixBase2Camera = np.dot('RobotToolPose.csv', 't_tool_camera')        matrixCamera2Base = np.linalg.inv(matrixBase2Camera)        zc = position[0][2] # z        u = position[0][0]    # x        v = position[0][1]    # y        # 直接变换        outputBase2 = np.dot(np.linalg.inv(matrixCamera2Base[0:3, 0:3]), zc * np.dot(np.linalg.inv('mtx'),np.array([u, v, 1]).reshape(3,1)) - matrixCamera2Base[:3,3].reshape(3, 1))        position.append(outputBase2)        outputBase.append(position)    return outputBaseif __name__ == '__main__':    parser = ArgumentParser()    parser.add_argument('config', help='Config file')    parser.add_argument('checkpoint', help='Checkpoint file')    parser.add_argument('--device',                        default='cuda:0',                        help='Device used for inference')    parser.add_argument('image_path', help='images save path')    parser.add_argument('server_url', help='llm server url', default='http://ipaddress:8000/v1/chat/completions')    parser.add_argument('--robot_path',                        type=str,                        required=True,                        help='RobotToolPose.csv')    parser.add_argument('--prompt', type=str, required=True)    args = parser.parse_args()    # 调取摄像头采集图片并存储    image_path = "./circle.png"    robot_path = "./RobotToolPose.csv"    img_point_length = 12    server_url = "http://ipaddress:8000/v1/chat/completions"    prompt = "Now you are a robotic arm. The get_names_on_table() function is used to get the names of all objects on the table and returns a collection. The variable is defined as nams. The get_box_postion() function obtains the positions where all objects on the table need to move. The variable is defined as position, which is fixed. The get_location_by_name(name) function gets the current location of each object. The move_tool(xyz) function is used to move the robotic arm. The grasp() function is used to grasp objects. The ungrasp() function is used to drop objects. Analyze the picture and design the plan method to move the objects on the desktop to the same position on the other side according to the provided function. Please only provide the python code implementation of the plan method without outputting relevant comments."    # 这里参数传入图片的保存地址，该函数应保存3D相机采集的RGB图片，点云深度信息，相机的每帧图片的位姿信息    get_camera_image(args.image_path)    # 相机标定，内参保存为txt文件，该值固定不变，对路径下的所有图片进行图像矫正    distortion(args.image_path)    # 相机标定结束后使用2D/3D目标检测算法检测目标信息    box_position = get_detection_box(args)    # 向大模型中传入图像与prompt    llm_generated_func = glm(args.server_url, args.image_path, args.prompt)    # 得到大模型返回的运动控制指令调用机器人执行指令    # 不了解调用接口，可以将运动指令和检测到的目标坐标信息同步传入    robot_controller(llm_generated_func)</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大象机械臂Mycobot_280Pi眼在手上标定</title>
      <link href="/2024/09/24/da-xiang-ji-jie-bi-yan-zai-shou-shang-biao-ding/"/>
      <url>/2024/09/24/da-xiang-ji-jie-bi-yan-zai-shou-shang-biao-ding/</url>
      
        <content type="html"><![CDATA[<h3 id="手眼标定">手眼标定</h3><p>使用USB接口相机标定时需要先使用标定板标定获取相机内参等信息，realsense相机自带标定信息。相机标定完成后在机械臂末端固定相机后使用aruco和机械臂位姿进行手眼标定。</p><h4 id="相机标定">相机标定</h4><p>大象机械臂内置了ROS1和ROS2环境，usb_cam默认安装编译完成，如果不是自带的话，ROS环境的安装可以使用鱼香的一键安装。</p><pre class=" language-language-bash"><code class="language-language-bash">wget http://fishros.com/install -O fishros && . fishros</code></pre><p>usb_cam安装如下所示。</p><pre class=" language-language-bash"><code class="language-language-bash">cd ~/catkin_ws/srcgit clone https://github.com/ros-drivers/usb_cam.gitcd ..catkin_make</code></pre><p>或者直接安装ROS版本的（推荐）</p><pre class=" language-language-bash"><code class="language-language-bash">sudo apt-get install ros-ros版本名称-usb-cam</code></pre><p><strong>USB相机的分辨率和设备编号需要自己修改，默认的都为video0</strong></p><pre class=" language-language-bash"><code class="language-language-bash">roscd usb_camsudo vi usb_cam-test.launch</code></pre><pre class=" language-language-python"><code class="language-language-python">param name="video_device" value="/dev/video0"      param name="image_width" value="1280" param name="image_height" value="720" </code></pre><p>大象机械臂内置的系统没有交换内存，所以比较卡。</p><pre class=" language-language-bash"><code class="language-language-bash">roslaunch usb_cam usb_cam-test.launch</code></pre><p>上述的指令启动相机，相机没有问题的情况下可以关闭使用以下指令重新启动</p><pre class=" language-language-bash"><code class="language-language-bash">rosrun usb_cam usb_cam_node</code></pre><p>在相机标定时需要保持相机的话题分布状态，所以启动相机后不能关闭，该指令无额外窗口，不占用额外开销。</p><h5 id="ubuntu增加交换内存">ubuntu增加交换内存</h5><p>上述提到了大象机械臂内置系统内存不足，包括后续realsense进行库编译时会卡死，所以一定要增加交换空间。</p><pre class=" language-language-bash"><code class="language-language-bash">free -h</code></pre><p>我这里使用free可以看到系统是没有交换空间的。</p><pre class=" language-language-bash"><code class="language-language-bash">sudo fallocate -l 4G /swapfilesudo mkswap /swapfilesudo swapon /swapfile</code></pre><p>交换空间永久生效</p><pre class=" language-language-bash"><code class="language-language-bash">echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab</code></pre><p>重新查看交换空间大小如下所示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/free.png" alt=""></p><p>相机启动后，安装ROS标定程序进行标定</p><pre class=" language-language-bash"><code class="language-language-bash">sudo apt-get install ros-ros版本名称-camera-calibration</code></pre><pre class=" language-language-bash"><code class="language-language-bash">rosrun camera_calibration cameracalibrator.py --size 7x10 --square 0.09 image:=/usb_cam/image_raw camera:=/head_camera --no-serive-check</code></pre><p>启动后的标定换面如下所示</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/cali.png" alt=""></p><p>x不会打的话可以复制，size为你打印棋盘格的尺寸，这里需要实际尺寸减1，因为计算的是角点，square为格子的尺寸，单位为米，棋盘格的打印<a href="https://calib.io/pages/camera-calibration-pattern-generator" target="_blank" rel="noopener">地址</a>。</p><p>移动你的棋盘格，棋盘正向，斜向在相机视野内移动，距离相机的距离变动，倾斜棋盘格移动直到X，Y，Size，Skew变绿，CALIBRATE按钮亮起可以进行标定，后续COMMIT保存即可。</p><h4 id="RealSense相机标定">RealSense相机标定</h4><p>realsense相机提供了标定参数，这里主要讲一下realsense库的编译，因为内置的树莓派是ARM架构，所以编译时需要通过源码编译，通过<a href="https://dev.intelrealsense.com/docs/compiling-librealsense-for-linux-ubuntu-guide" target="_blank" rel="noopener">官方文档</a>编译即可。</p><p>上文已经增加了交换空间，这里根据官方文档编译的时候可以使用合适的 -j2 或 -j4 参数编译。</p><h4 id="手眼标定-2">手眼标定</h4><p>这里手眼标定使用的是<a href="https://chev.me/arucogen/" target="_blank" rel="noopener">Aruco标定板</a>，这里需要选择Original ArUco。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/aruco.png" alt=""></p><p>手眼标定程序使用的是<a href="https://gitee.com/ohhuo/handeye-calib" target="_blank" rel="noopener">鱼香的ROS标定程序</a>，使用的是在线标定。</p><p>这里根据作者的文档编译使用即可。</p><p>因为我这里使用的是大象机械臂，所以说一下大象机械臂的位姿订阅。</p><p>下载大象机械臂的ROS代码。</p><pre class=" language-language-bash"><code class="language-language-bash">https://github.com/elephantrobotics/mycobot_ros</code></pre><p>安装官方文档进行编译后</p><pre class=" language-language-bash"><code class="language-language-bash">roslaunch mycobot_280pi slider_control.launch</code></pre><p>运行上述命令后发现机械臂不随软件调试变化，这里修改一下slider_control代码即可，修改如下。</p><pre class=" language-language-python"><code class="language-language-python">node name="control_slider" pkg="mycobot_280pi" type="slider_control.py">param name="port" type="string" value="$(arg port)"param name="baud" type="int" value="$(arg baud)"node</code></pre><p>将上述代码取消注释，配置你自己的机械臂 port和baud参数即可。</p><p>启动后软件如下图所示</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/mycobot.png" alt=""></p><p>机械臂的位姿订阅话题为tf获取，配置你自己的机械臂节点名称即可，可以使用rostopic echo查看对应话题的相关信息。</p><p>运行handeye手眼标定程序标定即可。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 手眼标定 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 眼在手上 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>美剧爬虫</title>
      <link href="/2022/04/30/mei-ju-pa-chong/"/>
      <url>/2022/04/30/mei-ju-pa-chong/</url>
      
        <content type="html"><![CDATA[<h3 id="美剧爬虫">美剧爬虫</h3><p>因为隐私原因，隐去了cookie。文章末尾提供了exe文件。支持halo(光环)和moon-knight(月光骑士)。</p><pre class=" language-language-python"><code class="language-language-python">import requestsfrom bs4 import BeautifulSoup as bsimport reimport jsonimport urllib.requestimport gzipdef getData(src,total,number):    u = '' + src     if total > 1:        u = '' + src + '/' + str(total) + '/'    html = requests.get(u).content    soup = bs(html,'lxml')     introductions = soup.find_all("script", class_="wp-playlist-script")[0]    front = str(introductions).split('"tracks":[')[1].split(']')[0].replace('\/','')    front = re.sub(r"\n|\t|\r|\r\n|\n\r|\x08|\\", "", front)    result = json.loads('[' + front + ']')    info = result[number - 1]    url = info['src1']    github_url = '' + url + '&type=mix'    referer = '' + src + '?ep=' + str(number)    if total > 1:        referer = '' + src + '/' + str(total) + '/' '?ep=' + str(number)    result = getResponse(github_url,referer)    with open(src + '第' + str(total) + '季' + '第' +  str(number) + '集' + '.txt', 'w', encoding='utf-8') as fileobj:        fileobj.write(result)def getResponse(url,referer):    req_header = {        "user-agent": "",        "accept": "*/*",        "accept-language": "zh-CN,zh;q=0.9",        "accept-encoding": "gzip",        "cookie": "",        "Referer": referer        }    req = urllib.request.Request(url,headers=req_header)    resp = urllib.request.urlopen(req)    resp = gzip.decompress(resp.read())    return json.loads(resp.decode("utf-8"))['url']print('请输入剧名：')name = input()print('请输入季数：')total = int(input())print('请输入集数：')number = int(input())getData(name,total,number)</code></pre><h3 id="运行文件">运行文件</h3><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/dgc.exe" target="_blank" rel="noopener">exe文件</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSST多尺度论文阅读</title>
      <link href="/2022/04/07/dsst-duo-chi-du-lun-wen-yue-du/"/>
      <url>/2022/04/07/dsst-duo-chi-du-lun-wen-yue-du/</url>
      
        <content type="html"><![CDATA[<h3 id="DSST多尺度论文阅读">DSST多尺度论文阅读</h3><h4 id="算法整体思想">算法整体思想</h4><p>在kcf使用二维高斯构建函数找到最大值后，在新的响应值最大即目标位置处通过调整追踪框大小来获取不同尺寸的响应值，取最大响应值的尺寸作为最佳尺寸，从而实现跟踪框内小范围的尺度自适应。</p><h4 id="算法实现-matlab">算法实现(matlab)</h4><pre class=" language-language-matlab"><code class="language-language-matlab">% 这里就是尺寸比例系数 33个尺寸ss = 1:nScales;scaleFactors = scale_step.^(ceil(nScales/2) - ss);</code></pre><pre class=" language-language-matlab"><code class="language-language-matlab">for frame = 1:num_frames,    im = imread([video_path img_files{frame}]);    tic;    if frame > 1        xt = get_translation_sample(im, pos, sz, currentScaleFactor, cos_window);        response = real(ifft2(sum(hf_num .* xtf, 3) ./ (hf_den + lambda)));         % 这里先获取位置的最大响应        [row, col] = find(response == max(response(:)), 1);        pos = pos + round((-sz/2 + [row, col]) * currentScaleFactor);        xs = get_scale_sample(im, pos, base_target_sz, currentScaleFactor * scaleFactors, scale_window, scale_model_sz);        xsf = fft(xs,[],2);          scale_response = real(ifft(sum(sf_num .* xsf, 1) ./ (sf_den + lambda)));         % 这里得到的是尺寸最大响应        recovered_scale = find(scale_response == max(scale_response(:)), 1);        currentScaleFactor = currentScaleFactor * scaleFactors(recovered_scale);     %更新尺寸        if currentScaleFactor < min_scale_factor                                     %处理极值            currentScaleFactor = min_scale_factor;        elseif currentScaleFactor > max_scale_factor            currentScaleFactor = max_scale_factor;        end    end    xl = get_translation_sample(im, pos, sz, currentScaleFactor, cos_window);    xlf = fft2(xl);    new_hf_num = bsxfun(@times, yf, conj(xlf));    new_hf_den = sum(xlf .* conj(xlf), 3);    % 得到33个尺度特征    xs = get_scale_sample(im, pos, base_target_sz, currentScaleFactor * scaleFactors, scale_window, scale_model_sz);    xsf = fft(xs,[],2);    new_sf_num = bsxfun(@times, ysf, conj(xsf));    new_sf_den = sum(xsf .* conj(xsf), 1);    if frame == 1            hf_den = new_hf_den;        hf_num = new_hf_num;                sf_den = new_sf_den;        sf_num = new_sf_num;    else        hf_den = (1 - learning_rate) * hf_den + learning_rate * new_hf_den;        hf_num = (1 - learning_rate) * hf_num + learning_rate * new_hf_num;        sf_den = (1 - learning_rate) * sf_den + learning_rate * new_sf_den;        sf_num = (1 - learning_rate) * sf_num + learning_rate * new_sf_num;    end    target_sz = floor(base_target_sz * currentScaleFactor);    positions(frame,:) = [pos target_sz];        time = time + toc;    if visualization == 1        rect_position = [pos([2,1]) - target_sz([2,1])/2, target_sz([2,1])];        if frame == 1,              figure('Name',['Tracker - ' video_path]);            im_handle = imshow(uint8(im), 'Border','tight', 'InitialMag', 100 + 100 * (length(im) < 500));            rect_handle = rectangle('Position',rect_position, 'EdgeColor','g');            text_handle = text(10, 10, int2str(frame));            set(text_handle, 'color', [0 1 1]);        else            try                 set(im_handle, 'CData', im)                set(rect_handle, 'Position', rect_position)                set(text_handle, 'string', int2str(frame));            catch                return            end        end        drawnow    endend</code></pre><h4 id="算法实现-python-kcf">算法实现(python + kcf)</h4><p>kcf的实现过程中，作者在实现尺度自适应时，思想和dsst是相同的，但是实现过程中，在确定了位置最大响应后，调整的跟踪框比例是按照1.0 / self.scale_step和self.scale_step来进行调节的，并且将尺寸缩放到了96，这样实现了一个高帧率。缩放的越小，计算的越快，但是特征信息丢失的也多，明显的拿空间换时间。</p><pre class=" language-language-python"><code class="language-language-python">    def getFeatures(self,image,flag,current_scale_size):        # 获取横纵坐标        _roi = [0,0,0,0]        x = self.n_roi[0] + self.n_roi[2] / 2        y = self.n_roi[1] + self.n_roi[3] / 2        if flag:            # 首次构造汉宁窗            # 汉宁窗大小为检测框大小            # 使用padding填充            w = self.n_roi[2] * self.padding            h = self.n_roi[3] * self.padding            # 缩放以减少特征提取量，如何减少特征提取量的同时保证帧数            self._scale = np.maximum(w,h) / float(96)            w = int(w / self._scale)            h = int(h / self._scale)            # 使用fhog特征提取，确保汉宁窗大小满足加窗规则            w = int(w) // (2 * self.cell_size) * 2 * self.cell_size + 2 * self.cell_size            h = int(h) // (2 * self.cell_size) * 2 * self.cell_size + 2 * self.cell_size            self.window_size[0] = w            self.window_size[1] = h        # 检测框大小        _roi[2] = int(current_scale_size * self._scale * self.window_size[0])        _roi[3] = int(current_scale_size * self._scale * self.window_size[1])        _roi[0] = int(x - _roi[2] / 2)        _roi[1] = int(y - _roi[3] / 2)        # 提取检测区域的像素        detect = utils.getSubWindow(image,_roi,cv2.BORDER_REPLICATE)        if(detect.shape[1] != self.window_size[0] or detect.shape[0] != self.window_size[1]):            detect = cv2.resize(detect,tuple(self.window_size))        # fhog对检测区域进行特征提取        mapp = {'sizeX': 0, 'sizeY': 0, 'numFeatures': 0, 'map': 0}        mapp = fhog.getFeatureMaps(detect, self.cell_size, mapp)        mapp = fhog.normalizeAndTruncate(mapp, 0.2)        mapp = fhog.PCAFeatureMaps(mapp)        self.size_patch = list(map(int, [mapp['sizeY'], mapp['sizeX'], mapp['numFeatures']]))        featuresMap = mapp['map'].reshape((self.size_patch[0] * self.size_patch[1], self.size_patch[2])).T        if flag:            # 创建汉宁窗            self.createHann()        featuresMap = self.hann * featuresMap        return featuresMap</code></pre><pre class=" language-language-python"><code class="language-language-python"> # 在这里更新尺度因子min_loc, min_peak_value = self.detect(self.features, self.getFeatures(image, False, 1.0 / self.scale_step))max_loc, max_peak_value = self.detect(self.features, self.getFeatures(image, False, self.scale_step))</code></pre><p>小比例和大比例的最大响应值与原比例进行判断，系数为固定的1.05。</p><h4 id="算法实现-python-kcf-DSST">算法实现(python + kcf + DSST)</h4><pre class=" language-language-python"><code class="language-language-python">scale_factor = 1scale_factors = [math.pow(self.scale_step,(np.ceil(33 / 2) - x)) for x in range(1,33)]scale_factors = sorted(scale_factors)for x in scale_factors:   n_loc, n_peak_value = self.detect(self.features, self.getFeatures(image, False, x))   if (0.96 * n_peak_value > peak_value):       scale_factor = x       breakself._scale *= scale_factorself.n_roi[2] *= scale_factorself.n_roi[3] *= scale_factor</code></pre><p>帧数明显降低</p><h4 id="多尺度优化方向">多尺度优化方向</h4><p><strong>在实际的追踪中，跟踪框的抖动明显，不稳定</strong></p><p><strong>高帧率，多特征</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像追踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能</title>
      <link href="/2022/04/04/tu-xiang-zhui-zong/"/>
      <url>/2022/04/04/tu-xiang-zhui-zong/</url>
      
        <content type="html"><![CDATA[<h3 id="人工智能的定义">人工智能的定义</h3><h5 id="上世纪80年代以前">上世纪80年代以前</h5><p>徒有其名，计算能力和对应的硬件限制。</p><p>А.В.Брушлинский,世京.人工“智能”为什么不可能?[J].世界科学译刊,1979(07):79-80.</p><h5 id="2000年以前">2000年以前</h5><p>成本高，上层太高无法落地</p><p>陈庆霞.人工智能研究纲领的发展历程和前景[J].科技信息,2008(33):49+234.</p><h6 id="至今">至今</h6><p>大数据，算法，算力</p><p>张贺.大数据时代人工智能在计算机网络技术中的应用探究[J].科技资讯,2021,19(30):17-19.DOI:10.16661/j.cnki.1672-3791.2111-5042-1903.</p><h4 id="应用价值">应用价值</h4><p>高梦鸽,邢梦颖.人工智能在汽车制造工业中的应用价值与前景[J].无线互联科技,2021,18(21):85-86.</p><p>王笛,赵靖,金明超,刘婧,熊伟.人工智能在医疗领域的应用与思考[J].中国医院管理,2021,41(06):71-74.</p><h4 id="应用领域">应用领域</h4><h4 id="微观">微观</h4><h5 id="粒子追踪">粒子追踪</h5><p>王福斌,何江红,武晨.基于粒子滤波的烧结断面图像火焰区域跟踪[J].激光杂志,2021,42(12):94-101.DOI:10.14016/j.cnki.jgzz.2022.12.094.</p><p>王伟,肖建强,何小元. 基于图像跟踪的低维材料实时应变测量方法[J]. 中国激光,2010,37(2):526-530. DOI:10.3788/CJL20103702.0526.</p><h5 id="细胞">细胞</h5><p>李臣鸿. 囊泡转运的动态跟踪及其与脂筏关系的初步研究[D]. 湖北:华中科技大学,2005. DOI:10.7666/d.d010452.</p><h4 id="宏观">宏观</h4><h5 id="兵工">兵工</h5><p>周桢. 复杂场景中目标抗遮挡跟踪算法研究[J]. 航空兵器,2007(6):36-39. DOI:10.3969/j.issn.1673-5048.2007.06.009.</p><p>王华荣,刘霞. 飞行器红外图像识别与跟踪算法研究[J]. 激光与红外,2021,51(8):1097-1103. DOI:10.3969/j.issn.1001-5078.2021.08.020.</p><p>贾君君. 坦克目标自动识别与跟踪算法研究[D]. 陕西:西安工业大学,2006. DOI:10.7666/d.y934367.</p><h5 id="交通">交通</h5><p>韩艺. 基于视频图像的运动车辆检测与跟踪算法研究[D]. 黑龙江:哈尔滨工业大学,2015. DOI:10.7666/d.D753596.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像追踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KCF算法综合</title>
      <link href="/2022/03/26/kcf-suan-fa-zong-he/"/>
      <url>/2022/03/26/kcf-suan-fa-zong-he/</url>
      
        <content type="html"><![CDATA[<h3 id="KCF算法综合">KCF算法综合</h3><h4 id="实现效果">实现效果</h4><video id="video" controls="" preload="none">    <source id="mp4" src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/buhuo.mp4" type="video/mp4"></video><h4 id="优化方向">优化方向</h4><p>追踪目标消失与重捕获</p><h4 id="算法整体思想">算法整体思想</h4><p>将低维不可分问题使用核函数映射到高维变为可分问题。使用核技巧解决高维维度灾难问题。使用高斯函数和岭回归训练分类器。更新时使用上一帧和当前帧的特征获取当前帧的最大响应定位当前帧的跟踪位置并重新训练分类器参数。</p><h4 id="算法实现步骤">算法实现步骤</h4><p>获取视频第一帧的框选区域roi。</p><p>调整区域roi的大小，调整完的区域超出框选位置则对框选的图像边缘进行填充，符合调整后的区域大小。</p><p>对框选区域使用fhog进行特征提取。</p><p>为了防止泄露在特征提取完成之后对特征进行加窗。</p><p>创建高斯核函数将低维空间映射到高维，同时使用核技巧来解决维度灾难中的计算问题。</p><p>对于高斯核函数计算的结果使用岭回归训练参数得到分类器。</p><p>第一帧之后根据上一帧的特征值和当前帧的特征值计算最大响应位置。</p><p>根据最大响应位置定位检测框的位置，在根据当前的检测框进行训练更新分类器参数。</p><h4 id="KCF论文阅读">KCF论文阅读</h4><h5 id="摘要">摘要</h5><p>大多数现代跟踪器的核心组件是判别分类器，其任务是区分目标和目标周边的环境。为了应对自然图像的变化，通常使用经过平移和缩放的样本<code>patch</code>进行训练。但这样的样本充满冗余，任何重叠的像素都被视为相同。基于这个简单的观察，我们提出了数千个平移<code>patch</code>的数据集分析模型。结果显示数据是循环的，我们能够使用离散傅里叶变换对数据矩阵进行对角化，从而将存储和计算开销减少几个数量级。有趣的是对于线性回归我们的公式相当于一个相关滤波器，例如目前使用的那些最快的有竞争力的跟踪器。然而，对于核回归，我们推导了一个新的核化相关滤波器（KCF），不同于其他核算法，它与线性核相应部分有完全相同的复杂度。在此基础上，通过线性核，我们还提出了一种线性相关滤波器的快速多通道扩展，我们称之为双相关滤波器。KCF和DCF在50个视频基准（OTB-50）测试中的表现优于Struck或TLD等顶级跟踪器，虽然每秒运行数百帧，但是只需几行代码即可实现（算法1）。为了鼓励更进一步的发展，我们开源了跟踪框架。</p><p><a href="http://dingdm.website/2022/03/07/kcf-suan-fa-xiang-guan-zhi-shi/" target="_blank" rel="noopener">相关理论</a></p><h5 id="实现细节">实现细节</h5><p>输入<code>patch</code>（原始像素或从通道中提取的特征）由余弦窗口加权，可以平滑地消除由循环假设引起的图像边界的不连续性。 跟踪区域的大小是目标区域框的2.5倍，以提供一些背景和额外的负样本。</p><p>为什么选取高斯</p><p>训练样本由基样本移位组成，因为我们必须为y中每一个yi指定回归目标 。回归目标y简单遵循高斯函数，其目标中心值为1，根据空间带宽s 从中间向周边平滑衰减到0。高斯目标比二进制标签更加平滑，而且有利于减轻在傅里叶域中的振铃伪像问题。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/middle1.png" alt="图1"></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/middle2.png" alt="图2"></p><p>一个微妙的问题是确定y的哪个元素是居中样本的回归目标，我们将以高斯函数为中心。虽然直观地看起来它似乎是输出平面的中间（图 1），但事实证明正确的选择是左上角的元素（图 2)。 原因是，在计算傅立叶域中的两个图像之间的互相关并转换回空间域之后，结果的左上角元素对应于零的移位。 当然，由于我们总是处理循环信号，因此高斯函数的峰值必须从左上角到其他角落环绕，如图2所示。 将高斯峰放置在回归目标的中间在一些过滤器实现中是常见的，并且导致相关输出被不必要地移动半个窗口，必须在事后更正。</p><p><a href="http://dingdm.website/2022/03/18/kcf-yuan-ma-fu-xian/" target="_blank" rel="noopener">代码复现</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像追踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KCF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kcf源码复现</title>
      <link href="/2022/03/18/kcf-yuan-ma-fu-xian/"/>
      <url>/2022/03/18/kcf-yuan-ma-fu-xian/</url>
      
        <content type="html"><![CDATA[<h3 id="kcf源码复现">kcf源码复现</h3><pre class=" language-language-matlab"><code class="language-language-matlab"># author:dinggc# date:2022/3/23 下午4:04import cv2import fhogimport numpy as nppadding = 2.5cell_size = 4output_sigma_factor = 0.125sigma = 0.6lambd = 0.0001def tracker(roi,image):    roi = list(map(float,roi))    detetc_features_value,size_patch,window_size = getFeatures(roi,image,True,[0,0])    gaussian = createGaussian(size_patch)    alphaf = np.zeros((size_patch[0],size_patch[1],2),np.float32)    alphaf,features,gaussian = train(detetc_features_value,size_patch,gaussian,alphaf,1.0)    return roi,features,alphaf,gaussian,window_sizedef getFeatures(roi, image,flag,window_size):    global hann    _roi = [0,0,0,0]    x = roi[0] + roi[2] / 2    y = roi[1] + roi[3] / 2    if flag:        w = roi[2] * padding        h = roi[3] * padding        w = int(w) // (2 * cell_size) * 2 * cell_size + 2 * cell_size        h = int(h) // (2 * cell_size) * 2 * cell_size + 2 * cell_size        window_size[0] = w        window_size[1] = h    _roi[2] = int(window_size[0])    _roi[3] = int(window_size[1])    _roi[0] = int(x - _roi[2] / 2)    _roi[1] = int(y - _roi[3] / 2)    detect = getSubWindow(image,_roi,cv2.BORDER_REPLICATE)    if(detect.shape[1] != window_size[0] or detect.shape[0] != window_size[1]):        detect = cv2.resize(detect,tuple(window_size))    mapp = {'sizeX': 0, 'sizeY': 0, 'numFeatures': 0, 'map': 0}    mapp = fhog.getFeatureMaps(detect, cell_size, mapp)    mapp = fhog.normalizeAndTruncate(mapp, 0.2)    mapp = fhog.PCAFeatureMaps(mapp)    size_patch = list(map(int, [mapp['sizeY'], mapp['sizeX'], mapp['numFeatures']]))    featuresMap = mapp['map'].reshape((size_patch[0] * size_patch[1], size_patch[2])).T    if flag:        hann = createHann(size_patch)    featuresMap = hann * featuresMap    return featuresMap,size_patch,window_sizedef getSubWindow(image,roi,type):    newroi = transformValue(roi,[0,0,image.shape[1],image.shape[0]])    border = getBorder(roi,newroi)    new_image = image[newroi[1]:newroi[1] + newroi[3],newroi[0]:newroi[0] + newroi[2]]    return cv2.copyMakeBorder(new_image,border[1],border[3],border[0],border[2],type)def transformValue(roi,image_size):    if(roi[0] + roi[2] > image_size[0] + image_size[2]):        roi[2] = image_size[0] + image_size[2] - roi[0]    if(roi[1] + roi[3] > image_size[1] + image_size[3]):        roi[3] = image_size[1] + image_size[3] - roi[1]    if(roi[0] < image_size[0]):        roi[2] -= (image_size[0] - roi[0])        roi[0] = image_size[0]    if(roi[1] < image_size[1]):        roi[3] -= (image_size[1] - roi[1])        roi[1] = image_size[1]    if(roi[2] < 0):        roi[2] = 0    if(roi[3] < 0):        roi[3] = 0    return roidef getBorder(roi,newroi):    res = [0, 0, 0, 0]    res[0] = newroi[0] - roi[0]    res[1] = newroi[1] - roi[1]    res[2] = roi[0] + roi[2] - newroi[0] - newroi[2]    res[3] = roi[1] + roi[3] - newroi[1] - newroi[3]    return resdef createHann(size_patch):    hann2t, hann1t = np.ogrid[0:size_patch[0], 0:size_patch[1]]    hann1t = 0.5 * (1 - np.cos(2 * np.pi * hann1t / (size_patch[1] - 1)))    hann2t = 0.5 * (1 - np.cos(2 * np.pi * hann2t / (size_patch[0] - 1)))    hann2d = hann2t * hann1t    hann1d = hann2d.reshape(size_patch[0] * size_patch[1])    hann = np.zeros((size_patch[2], 1), np.float32) + hann1d    hann = hann.astype(np.float32)    return hanndef createGaussian(size_patch):    syh, sxh = size_patch[0] / 2, size_patch[1] / 2    output_sigma = np.sqrt(size_patch[0] * size_patch[1]) / padding * output_sigma_factor    mult = -0.5 / (output_sigma * output_sigma)    y, x = np.ogrid[0:size_patch[0], 0:size_patch[1]]    y, x = (y - syh) ** 2, (x - sxh) ** 2    res = np.exp(mult * (y + x))    return fftd(res)def fftd(img, backwards=False):    return cv2.dft(np.float32(img), flags=((cv2.DFT_INVERSE | cv2.DFT_SCALE) if backwards else cv2.DFT_COMPLEX_OUTPUT))def train(features,size_patch,gaussian,alphaf,interp_factor):    correlation = gaussianCorrelation(size_patch,features,features)    features = (1 - interp_factor) * features + interp_factor * features    alphaf = complexDivision(gaussian,fftd(correlation) + lambd)    alphaf = (1 - interp_factor) * alphaf + interp_factor * alphaf    return alphaf,features,gaussian    def gaussianCorrelation(size_patch,features,_features):    global d    c = np.zeros((size_patch[0], size_patch[1]), np.float32)    for i in range(size_patch[2]):        x1aux = features[i, :].reshape((size_patch[0], size_patch[1]))        x2aux = _features[i, :].reshape((size_patch[0], size_patch[1]))        caux = cv2.mulSpectrums(fftd(x1aux), fftd(x2aux), 0, conjB=True)        caux = fftd(caux, True)[:,:,0]        c += caux    c = rearrange(c)    if (features.ndim == 3 and _features.ndim == 3):        d = (np.sum(features[:, :, 0] * features[:, :, 0]) + np.sum(_features[:, :, 0] * _features[:, :, 0]) - 2.0 * c) / (                    size_patch[0] * size_patch[1] * size_patch[2])    elif (features.ndim == 2 and _features.ndim == 2):        d = (np.sum(features * features) + np.sum(_features * _features) - 2.0 * c) / (                    size_patch[0] * size_patch[1] * size_patch[2])    d = d * (d >= 0)    d = np.exp(-d / (sigma * sigma))    return ddef rearrange(image):    img_ = np.zeros(image.shape, image.dtype)    xh, yh = image.shape[1] // 2, image.shape[0] // 2    img_[0:yh, 0:xh], img_[yh:image.shape[0], xh:image.shape[1]] = image[yh:image.shape[0], xh:image.shape[1]], image[0:yh, 0:xh]    img_[0:yh, xh:image.shape[1]], img_[yh:image.shape[0], 0:xh] = image[yh:image.shape[0], 0:xh], image[0:yh, xh:image.shape[1]]    return img_def complexDivision(a, b):    res = np.zeros(a.shape, a.dtype)    divisor = 1. / (b[:, :, 0]**2 + b[:, :, 1]**2)    res[:, :, 0] = (a[:, :, 0] * b[:, :, 0] + a[:, :, 1] * b[:, :, 1]) * divisor    res[:, :, 1] = (a[:, :, 1] * b[:, :, 0] + a[:, :, 0] * b[:, :, 1]) * divisor    return resdef update(roi,features,image,alphaf,gaussian,window_size):    if (roi[0] + roi[2] <= 0):        roi[0] = -roi[2] + 1    if (roi[1] + roi[3] <= 0):        roi[1] = -roi[2] + 1    if (roi[0] >= image.shape[1] - 1):        roi[0] = image.shape[1] - 2    if (roi[1] >= image.shape[0] - 1):        roi[1] = image.shape[0] - 2    cx = roi[0] + roi[2] / 2.    cy = roi[1] + roi[3] / 2.    new_frame_featuresMap,size_patch,window_size = getFeatures(roi,image,False,window_size)    loc, peak_value = detect(features,new_frame_featuresMap,size_patch,alphaf)    roi[0] = cx - roi[2] / 2.0 + loc[0] * cell_size    roi[1] = cy - roi[3] / 2.0 + loc[1] * cell_size    if (roi[0] >= image.shape[1] - 1):        roi[0] = image.shape[1] - 1    if (roi[1] >= image.shape[0] - 1):        roi[1] = image.shape[0] - 1    if (roi[0] + roi[2] <= 0):        roi[0] = -roi[2] + 2    if (roi[1] + roi[3] <= 0):        roi[1] = -roi[3] + 2    new_frame_featuresMap,size_patch,window_size = getFeatures(roi,image,False,window_size)    alphaf,features,gaussian = train(new_frame_featuresMap,size_patch,gaussian,alphaf,0.012)    return roi,alphaf,features,gaussian,window_sizedef detect(features, new_frame_features,size_patch,alphaf):    k = gaussianCorrelation(size_patch,new_frame_features, features)    res = fftd(complexMultiplication(alphaf, fftd(k)), True)[:,:,0]    _, pv, _, pi = cv2.minMaxLoc(res)    p = [float(pi[0]), float(pi[1])]    if(pi[0] > 0 and pi[0] < res.shape[1] - 1):        p[0] += subPixelPeak(res[pi[1], pi[0] - 1], pv, res[pi[1], pi[0] + 1])    if(pi[1] > 0 and pi[1] < res.shape[0] - 1):        p[1] += subPixelPeak(res[pi[1] - 1, pi[0]], pv, res[pi[1] + 1, pi[0]])    p[0] -= res.shape[1] / 2.    p[1] -= res.shape[0] / 2.    return p,pvdef complexMultiplication(a, b):    res = np.zeros(a.shape, a.dtype)    res[:, :, 0] = a[:, :, 0] * b[:, :, 0] - a[:, :, 1] * b[:, :, 1]    res[:, :, 1] = a[:, :, 0] * b[:, :, 1] + a[:, :, 1] * b[:, :, 0]    return resdef subPixelPeak(left, center, right):    divisor = 2 * center - right - left  # float    return (0 if abs(divisor) < 1e-3 else 0.5 * (right - left) / divisor)</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像追踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KCF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>agx xavier开发板刷机、环境搭建与算法训练</title>
      <link href="/2022/03/16/agx-xavier-kai-fa-ban-shua-ji-huan-jing-da-jian-yu-suan-fa-xun-lian/"/>
      <url>/2022/03/16/agx-xavier-kai-fa-ban-shua-ji-huan-jing-da-jian-yu-suan-fa-xun-lian/</url>
      
        <content type="html"><![CDATA[<h3 id="agx-xavier开发板刷机、环境搭建与算法训练">agx xavier开发板刷机、环境搭建与算法训练</h3><h4 id="开发板刷机">开发板刷机</h4><p>主机需要Linux系统，安装sdkmanager。</p><p>这里由于我的电脑是mac，使用pd虚拟机镜像是arm的，没找到桌面版的，而且sdkmanager是deb，不太适配。所以我这里选择了使用windows系统，使用vm虚拟机，装了ubuntu20.04的desktop版。</p><p>这里提供一下sdkmanager和desktop的镜像地址，可以直接下。<strong>需要注意的是在创建虚拟机的时候内存建议设置8G及以上，硬盘空间至少60G。</strong></p><p><a href="https://developer.nvidia.com/nvidia-sdk-manager" target="_blank" rel="noopener">sdkmanager</a></p><p><a href="http://mirrors.aliyun.com/ubuntu-releases/18.04/ubuntu-18.04.6-desktop-amd64.iso" target="_blank" rel="noopener">ubuntu</a></p><h5 id="主机换源">主机换源</h5><p>主机源备份</p><pre class=" language-language-shell"><code class="language-language-shell">sudo mv /etc/apt/sources.list /etc/apt/sources.list.back</code></pre><p>换源</p><pre class=" language-language-shell"><code class="language-language-shell">sudo vi /etc/apt/sources.list</code></pre><p>粘贴以下内容</p><pre class=" language-language-shell"><code class="language-language-shell">##中科大源deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</code></pre><p>保存退出后执行以下命令</p><pre class=" language-language-shell"><code class="language-language-shell">sudo apt-get updatesudo apt-get upgrade</code></pre><h5 id="刷机">刷机</h5><p>下载好的sdkmanager安装包直接点击安装即可。</p><p>安装步骤</p><p>下面的图为tx2，来源于网络，实际配置的板子为agx xavier。</p><p><strong>step1</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step01.png" alt=""></p><p><strong>注意</strong></p><p>hardware configuration中的host machine是为自己当前的主机安装环境，这里可以根据自己的需要进行选择。后面的target hardware一定要连接到自己的开发板。使用的是厂家提供的原装线，并且type-c的口一定要连接靠近电源按钮的那个接口。</p><p>如果还是连接不到，在保证开发板通电且关机的情况下，按住开机按钮和开机旁边的按钮两秒钟，看到电源灯亮起的时候松开。这个时候电脑会弹出连接的选项，这里要将开发板连接到你的虚拟机而不是你的电脑。</p><p>jetpack版本的话我最初安装的是4.5，后来在配环境的时候看到4.5有bug，因此在线升级成了4.6。你可以在该步骤直接选择或者以后根据情况在板子的系统中自行升级。</p><p><strong>step2</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step02.png" alt=""></p><p><strong>注意</strong></p><p>该步骤的除了镜像是烧进系统之外，下面的开发环境配置是需要开发板和主机在一个局域网内的，你可以将电脑和局域网连接到一个路由器。我这里没有网线所以就没有安装，是在开发板系统装完之后在系统中安装的。安装步骤在本文的后面。推荐该步骤找根网线将环境安装完整。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step03.png" alt=""></p><p><strong>注意</strong></p><p>在系统镜像下载完成并安装完成之后，第三步会跳出该见面配置环境信息，同时连接开发板的显示屏亮屏进行开发板系统的安装，系统的安装按照步骤进行即可。环境的安装需要配置开发板的局域网ip和信息，之后进行安装即可。</p><h4 id="环境搭建">环境搭建</h4><p>由于开发板的空间较小，且我没有外接sdk的情况下，在环境配置的时候，虽然板子的arm系统有替换ancoda的软件，但是创建虚拟环境耗费的空间较大，因此我没有创建虚拟环境，而是直接在本地默认环境下进行python相关环境配置。</p><p>如果你的开发环境在刷机时未安装，则在刷机时未安装开发环境情况后添加</p><pre class=" language-language-shell"><code class="language-language-shell">sudo vi ~/.bashrc# 添加以下内容export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATHexport CUDA_ROOT=/usr/local/cuda</code></pre><pre class=" language-language-shell"><code class="language-language-shell">sudo vi ~/.bashrc# 最后一行添加export OPENBLAS_CORETYPE=ARMV8 </code></pre><pre class=" language-language-shell"><code class="language-language-shell">sudo vi /etc/profile# 最后一行添加export OPENBLAS_CORETYPE=ARMV8 </code></pre><pre class=" language-language-shell"><code class="language-language-shell">source ~/.bashrcsource /etc/profile</code></pre><p><strong>该步骤为了防止非法指令报错问题。</strong></p><p><strong>注意</strong></p><p>板子上我不推荐换源。</p><h5 id="更换默认python版本为3-6">更换默认python版本为3.6</h5><p>由于板子默认提供python2和python3，而直接使用python指向的是2版本，因此可以更改一下默认的指向。</p><pre class=" language-language-shell"><code class="language-language-shell">sudo rm -rf /usr/bin/pythonsudo ln -s /usr/bin/python3.6 /usr/bin/python #指向默认版本为3.6</code></pre><pre class=" language-language-shell"><code class="language-language-shell">sudo vi usr/local/bin/pip# 更改内容#!/usr/bin/python3 #指向默认版本为3.6</code></pre><h5 id="pip安装">pip安装</h5><pre class=" language-language-shell"><code class="language-language-shell">sudo apt-get install python3-pip</code></pre><h5 id="matplotlib安装">matplotlib安装</h5><p><strong>1.19.5的版本在你安装其他环境的时候可能存在bug</strong>，我这里推荐直接安装1.19.4版本的，如果存在了默认的版本，卸载即可。</p><h5 id="刷机时未安装开发环境情况">刷机时未安装开发环境情况</h5><pre class=" language-language-shell"><code class="language-language-shell">sudo apt updatesudo apt install nvidia-jetpack</code></pre><h5 id="升级jetpack-根据自己的需求">升级jetpack(根据自己的需求)</h5><pre class=" language-language-shell"><code class="language-language-shell">sudo apt install nvidia-jetpacksudo apt purge cuda-repo-l4t-10-0-local-10.0.326 libvisionworks-repo libvisionworks-sfm-repo libvisionworks-tracking-reposudo apt cleansudo apt updatesudo apt list --upgradablesudo apt upgrade</code></pre><p>更新完成后重启开发板。</p><pre class=" language-language-shell"><code class="language-language-shell">sudo vi /etc/apt/sources.list.d/nvidia-l4t-apt-source.list</code></pre><p>修改内容</p><pre class=" language-language-shell"><code class="language-language-shell">deb https://repo.download.nvidia.com/jetson/common r32.6 maindeb https://repo.download.nvidia.com/jetson/ t194 r32.6 main</code></pre><p>r32.x x即为你想升级的版本，保存即可</p><pre class=" language-language-shell"><code class="language-language-shell">sudo apt updatesudo apt dist-upgrade</code></pre><p>升级完成之后再次重新启动开发板</p><pre class=" language-language-shell"><code class="language-language-shell">sudo apt depends nvidia-jetpack | awk '{print $2}' | xargs -I {} sudo apt install -y {}</code></pre><h5 id="pytorch安装">pytorch安装</h5><p><strong>注意</strong></p><p>这里的torch和torchvision版本一定要对应，且版本要满足你的算法运行环境。我这里安装的版本为1.8.0的torch和0.9.0的torchvision。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/torch-torchvision.png" alt=""></p><p>1.9.0以上的torch对torchvision对应的时候在训练yolov5的时候有bug，所以使用了1.8.0。</p><p>而且只有1.8.0才能使用torch.fx，所以我这里也不能使用1.7.0，shit！所以一定要注意算法要求。</p><p><strong>安装</strong></p><p>torch与jetpack版本的对应关系</p><p><a href="https://elinux.org/Jetson_Zoo#PyTorch_.28Caffe2.29" target="_blank" rel="noopener">https://elinux.org/Jetson_Zoo#PyTorch_.28Caffe2.29</a></p><p>由于开发板只能使用官网提供的whl安装，但是去官网下载需要科学上网，因此我提供下载地址。</p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/torch-1.6.0-cp36-cp36m-linux_aarch64.whl" target="_blank" rel="noopener">torch-1.6.0-cp36-cp36m-linux_aarch64.whl</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/torch-1.7.0-cp36-cp36m-linux_aarch64.whl" target="_blank" rel="noopener">torch-1.7.0-cp36-cp36m-linux_aarch64.whl</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/torch-1.8.0-cp36-cp36m-linux_aarch64.whl" target="_blank" rel="noopener">torch-1.8.0-cp36-cp36m-linux_aarch64.whl</a></p><p>下载完成后</p><pre class=" language-language-shell"><code class="language-language-shell">sudo pip install torch-1.8.0-cp36-cp36m-linux_aarch64.whl</code></pre><h5 id="torchvision安装">torchvision安装</h5><pre class=" language-language-shell"><code class="language-language-shell">sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev</code></pre><p>由于pip无法下载所有版本，因此torch对应的相关版本需要自行下载github地址进行编译，我这里提供部分版本。</p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/vision-0.8.0.tar.gz" target="_blank" rel="noopener">0.8.0</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/detect.tar.gz" target="_blank" rel="noopener">0.9.0</a></p><p>如果报错fatal error: libavcodec/avcodec.h(建议编译前修改以下内容)</p><p>更改torchvision的setup.py源码，找到<strong>if has ffmpeg</strong> 改为<strong>if False</strong>即可</p><pre class=" language-language-shell"><code class="language-language-shell">cd visionsudo python setup.py install</code></pre><p>如果我没有提供你需要的版本，且pip中也无版本</p><pre class=" language-language-shell"><code class="language-language-shell">git clone --branch v0.9.0 https://github.com/pytorch/vision torchvision cd torchvisionsudo python setup.py install</code></pre><p>v0.9.0更换为你的版本号即可。torchvision编译时间较长。</p><h5 id="其他环境">其他环境</h5><p>这里根据你的需求或者requirements.txt文件进行安装即可。</p><h4 id="算法训练">算法训练</h4><h5 id="训练过程">训练过程</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/trainyolov5.png" alt=""></p><h5 id="检测效果">检测效果</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/masktestyolov5.jpeg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 开发板 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nvdia agx Xavier </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KCF源码解析</title>
      <link href="/2022/03/09/kcf-yuan-ma-jie-xi/"/>
      <url>/2022/03/09/kcf-yuan-ma-jie-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="KCF源码解析">KCF源码解析</h3><h4 id="双线性插值">双线性插值</h4><p>例：将一个feature map由3*3放大为4*4时，如何进行外围的填充？填充的方式即为插值。</p><p>普通的插值方式</p><p>src为原图的像素值，dst为填充后的目标图的像素值，计算公式为srcX=dstX* (srcWidth/dstWidth) , srcY = dstY * (srcHeight/dstHeight)</p><p>但是计算目标图的像素值时会出现小数的情况，使用四舍五入或者之间去掉小数位会导致插值后的目标图出现马赛克或者失真现象。</p><p>双线性插值</p><p>双线性插值使用源图像像素点周围真实存在的四个像素值来决定目标图的像素值。</p><p>对于一个目的像素，设置坐标通过反向变换得到的浮点坐标为(i+u,j+v) (其中i、j均为浮点坐标的整数部分，u、v为浮点坐标的小数部分，是取值[0,1)区间的浮点数)，则这个像素得值 f(i+u,j+v) 可由原图像中坐标为 (i,j)、(i+1,j)、(i,j+1)、(i+1,j+1)所对应的周围四个像素的值决定，即：f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1)<br>其中f(i,j)表示源图像(i,j)处的的像素值</p><p>假如目标图的象素坐标为（1，1），那么反推得到的对应于源图的坐标是（0.75 , 0.75）, 这其实只是一个概念上的虚拟象素,实际在源图中并不存在这样一个象素,那么目标图的象素（1，1）的取值不能够由这个虚拟象素来决定，而只能由源图的这四个象素共同决定：（0，0）（0，1）（1，0）（1，1），而由于（0.75,0.75）离（1，1）要更近一些，那么（1,1）所起的决定作用更大一些，这从公式1中的系数uv=0.75×0.75就可以体现出来，而（0.75,0.75）离（0，0）最远，所以（0，0）所起的决定作用就要小一些</p><p>首先，在X方向上进行两次线性插值计算，然后在Y方向上进行一次插值计算。<br>在图像处理的时候，我们先根据srcX=dstX* (srcWidth/dstWidth) ,srcY = dstY * (srcHeight/dstHeight)来计算目标像素在源图像中的位置，这里计算的srcX和srcY一般都是浮点数，比如f（1.2, 3.4）这个像素点是虚拟存在的，先找到与它临近的四个实际存在的像素点<br>　　（1，3） （2，3）<br>　　（1，4） （2，4）<br>写成f(i+u,j+v)的形式，则u=0.2,v=0.4, i=1, j=3，f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1) 。</p><p><strong>线性插值空间因子的引入</strong></p><p>源图像与目标图像几何中心的对齐</p><p>srcX=dstX* (srcWidth/dstWidth)+0.5*(srcWidth/dstWidth-1)</p><p>我们在原始的浮点坐标上加上了0.5*<em>(srcWidth/dstWidth-1)这样一个控制因子，这项的符号可正可负，与srcWidth/dstWidth的比值也就是当前插值是扩大还是缩小图像有关，假设源图像是3</em>3，中心点坐标（1，1）目标图像是9**9，中心点坐标（4，4），我们在进行插值映射的时候，尽可能希望均匀的用到源图像的像素信息，最直观的就是（4,4）映射到（1,1）现在直接计算srcX=4**3/9=1.3333！=1，也就是我们在插值的时候所利用的像素集中在图像的右下方，而不是均匀分布整个图像。</p><h4 id="高斯核函数">高斯核函数</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220309-111917.png" alt=""></p><p>σ控制高斯核函数的局部作用范围。当 x 和x′的欧式距离处于某一个区间范围内的时候，假设固定 x′， k(x,x′) 随x的变化而变化的相当显著。</p><p>σ=1</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220309-112058.png" alt=""></p><p>σ=5</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220309-112108.png" alt=""></p><h4 id="hog特征提取">hog特征提取</h4><p>在HOG特征描述符中，梯度方向的分布，也就是梯度方向的直方图被视作特征。图像的梯度(x和y导数)非常有用，因为边缘和拐角(强度突变的区域)周围的梯度幅度很大，并且边缘和拐角比平坦区域包含更多关于物体形状的信息。</p><p>在每个像素处，梯度有一个大小和一个方向。x方向梯度图会强化垂直边缘特征，y方向梯度图会强化水平边缘特征。这就使得有用的特征（轮廓）得到保留，无关不重要的信息被去除。</p><p>把整个图像划分为若干个8x8的小单元，称为cell，并计算每个cell的梯度直方图。</p><p>计算cell中像素的梯度直方图，先将角度范围分成9份，也就是9 bins，每20°为一个单元，也就是这些像素可以根据角度分为9组。将每一份中所有像素对应的梯度值进行累加，可以得到9个数值。直方图就是由这9个数值组成的数组，对应于角度0、20、40、60… 160。如果某个像素的梯度角度大于160度，也就是在160度到180度之间，那么把这个像素对应的梯度值按比例分给0度和160度对应的bin。</p><p>HOG将8×8的一个区域作为一个cell，再以2×2个cell作为一组，称为block。由于每个cell有9个值，2×2个cell则有36个值，HOG是通过滑动窗口的方式来得到block的</p><pre class=" language-language-matlab"><code class="language-language-matlab">feature.hog(image, orientations=9, pixels_per_cell=(16, 16),                    cells_per_block=(2, 2), visualize=True)</code></pre><ul><li><code>image</code>：可以是灰度图或者彩色图；</li><li>orientations：就是把180度分成几份，也就是bin的数量；</li><li><code>pixels_per_cell</code>：一个cell里包含的像素个数；</li><li><code>cells_per_block</code>：一个block包含的cell个数；</li><li><code>visualize</code>：是否返回一个hog图像用于显示</li></ul><h4 id="参数设置">参数设置</h4><pre class=" language-language-matlab"><code class="language-language-matlab">% 高斯核函数kernel.type = 'gaussian';% 额外区域，每帧检测的范围padding = 1.5;% 控制过拟合的岭回归参数，即惩罚因子lambda = 1e-4;% 空间带宽output_sigma_factor = 0.1;% 线性插值的控制因子interp_factor = 0.02;% 高斯核函数的带宽，控制高斯核函数的局部作用范围kernel.sigma = 0.5;kernel.poly_a = 1;kernel.poly_b = 9;% 使用hog特征提取features.hog = true;% bin的数量features.hog_orientations = 9;% cell的大小cell_size = 4;</code></pre><h4 id="tracker">tracker</h4><pre class=" language-language-matlab"><code class="language-language-matlab">function [positions, time] = tracker(video_path, img_files, pos, target_sz, ...padding, kernel, lambda, output_sigma_factor, interp_factor, cell_size, ...features, show_visualization)% 判断框选区域大小，区域较大减小分辨率，并非提取全部细节resize_image = (sqrt(prod(target_sz)) >= 100);if resize_image,pos = floor(pos / 2);target_sz = floor(target_sz / 2);end% 设置检测窗口，大小为框选区域的 1+padding倍window_sz = floor(target_sz * (1 + padding));% 计算带宽output_sigma = sqrt(prod(target_sz)) * output_sigma_factor / cell_size;% 傅立叶变换到频域进行处理，gaussian_shaped_labels进行移位采样yf = fft2(gaussian_shaped_labels(output_sigma, floor(window_sz / cell_size)));  % 为了防止傅立叶变换之后频域信号泄露，进行加窗，窗函数参考之前kcf算法相关知识cos_window = hann(size(yf,1)) * hann(size(yf,2))';  % 图像追踪可视化if show_visualization,update_visualization = show_video(img_files, video_path, resize_image);end  % FPS计算time = 0;positions = zeros(numel(img_files), 2);  % 逐帧跟踪，即数据集中的图片for frame = 1:numel(img_files),% 图片读取，并转换为灰度图im = imread([video_path img_files{frame}]);if size(im,3) > 1,im = rgb2gray(im);endif resize_image,im = imresize(im, 0.5);endtic()    % 第一帧之后的图像追踪处理，可以先看代码下方对第一帧的分类器训练逻辑if frame > 1,% 在接下来的图像帧中根据设定的检测框和上一帧的追踪位置提取检测区域patch = get_subwindow(im, pos, window_sz);% 对特征区域进行傅立叶变换zf = fft2(get_features(patch, features, cell_size, cos_window));switch kernel.typecase 'gaussian',  % 计算每个采样的高斯响应，也就是低维映射到高维之后高斯核函数的快速计算结果kzf = gaussian_correlation(zf, model_xf, kernel.sigma);case 'polynomial',kzf = polynomial_correlation(zf, model_xf, kernel.poly_a, kernel.poly_b);case 'linear',kzf = linear_correlation(zf, model_xf);end% 归一化逆傅立叶变换后的时域图response = real(ifft2(model_alphaf .* kzf));% 获取最大响应位置[vert_delta, horiz_delta] = find(response == max(response(:)), 1);% 处理追踪时目标不移动而导致的峰值不在中心的情形if vert_delta > size(zf,1) / 2, vert_delta = vert_delta - size(zf,1);endif horiz_delta > size(zf,2) / 2,horiz_delta = horiz_delta - size(zf,2);endpos = pos + cell_size * [vert_delta - 1, horiz_delta - 1];end% 若是第一帧，获取第一帧检测的区域，否则是第一帧之后的检测位置信息patch = get_subwindow(im, pos, window_sz);% 获取第一帧检测区域的特征矩阵并进行傅立叶变换到频域或后面帧的特征矩阵xf = fft2(get_features(patch, features, cell_size, cos_window));% 根据不同的核函数调用不同的处理方法switch kernel.typecase 'gaussian',  % 获取高斯响应，也就是低维映射到高维之后高斯核函数的快速计算结果kf = gaussian_correlation(xf, xf, kernel.sigma);case 'polynomial',kf = polynomial_correlation(xf, xf, kernel.poly_a, kernel.poly_b);case 'linear',kf = linear_correlation(xf, xf);end% 快速计算 lambda为岭回归的惩罚因子 yf为检测框的高斯标签，kf为hog特征提取后特征矩阵的高斯响应alphaf = yf ./ (kf + lambda);    % 第一帧的分类器训练参数，为上面计算的alphaf分类器参数和检测区域特征矩阵变换后的频域信息if frame == 1, model_alphaf = alphaf;model_xf = xf;else% interp_factor作为更新系数更新分类器参数，理解为插值运算保留上一次区域检测的信息？model_alphaf = (1 - interp_factor) * model_alphaf + interp_factor * alphaf;model_xf = (1 - interp_factor) * model_xf + interp_factor * xf;endpositions(frame,:) = pos;time = time + toc();% 可视化if show_visualization,box = [pos([2,1]) - target_sz([2,1])/2, target_sz([2,1])];stop = update_visualization(frame, box);if stop, break, enddrawnowendendif resize_image,positions = positions * 2;endend</code></pre><h4 id="高斯回归">高斯回归</h4><pre class=" language-language-matlab"><code class="language-language-matlab">function labels = gaussian_shaped_labels(sigma, sz)% 前面是高斯σ，后面是一维向量，是检测框以cell为单位后计算的x，y所包含的4*4cell的个数  % ndgrid的第一个参数为 1:x轴的cell个数(为啥减去x的一半啊) 第二个同理为y轴方向  % ndgrid函数的作用主要是用于低维到高维的映射  % rs的列数等于cs的列数，而cs的行数等于rs的行数，即rs复制了cs的列，cs复制了rs的行。rs垂直于cs，cs有16个坐标点，所以rs有16列，而cs垂直于rs，rs有38个坐标点，所以cs有18行。这样低维到高维就映射完成了[rs, cs] = ndgrid((1:sz(1)) - floor(sz(1)/2), (1:sz(2)) - floor(sz(2)/2));  % rs是一个矩阵 1到x轴次数减次数的一半 1到最后一个值每个以行为基准构成矩阵 cs是y轴 以列为基准构成矩阵  % 在高维的图像上计算高斯函数 获取检测框内的特征信息labels = exp(-0.5 / sigma^2 * (rs.^2 + cs.^2));% 检测区域内的循环移位操作，列移位和行移位labels = circshift(labels, -floor(sz(1:2) / 2) + 1);  % 为什么要确定移位到左上角呢assert(labels(1,1) == 1)end</code></pre><h4 id="高斯响应">高斯响应</h4><pre class=" language-language-matlab"><code class="language-language-matlab">% 输入两个频率域上的尺寸完全相同的两个三维矩阵xf(当前帧特征矩阵的傅立叶变换)，yf(上一帧特征矩阵的傅立叶变换)以及主函数中选择的高斯带宽gaussian.sigam% 这里也就是使用高斯核将低维映射到高维之后的低维计算方式function kf = gaussian_correlation(xf, yf, sigma)N = size(xf,1) * size(xf,2);% xx和yy等于xf和yf中各元素的模的平方和再除以N，都是实数xx = xf(:)' * xf(:) / N; yy = yf(:)' * yf(:) / N; % xyf等于xf与yf的共轭矩阵点乘 xyf = xf .* conj(yf);% xyf经过傅里叶反变换回到时域，取实部并将第三维度上的值求和，最后得到尺寸与输入xf和yf相同的二维矩阵xy xy = sum(real(ifft2(xyf)), 3); % 用xx+yy-2xy再除以xf中的元素数，然后乘以高斯核函数，最后经过傅里叶变换得到所有位置的高斯响应kf = fft2(exp(-1 / sigma^2 * max(0, (xx + yy - 2 * xy) / numel(xf))));end</code></pre><h4 id="HOG特征提取">HOG特征提取</h4><pre class=" language-language-matlab"><code class="language-language-matlab">function x = get_features(im, features, cell_size, cos_window)if features.hog,% hog特征提取x = double(fhog(single(im) / 255, cell_size, features.hog_orientations));x(:,:,end) = [];endif features.gray,x = double(im) / 255;x = x - mean(x(:));endif ~isempty(cos_window),  % 加窗函数的处理x = bsxfun(@times, x, cos_window);endend</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像追踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KCF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KCF算法相关知识</title>
      <link href="/2022/03/07/kcf-suan-fa-xiang-guan-zhi-shi/"/>
      <url>/2022/03/07/kcf-suan-fa-xiang-guan-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="KCF算法相关知识">KCF算法相关知识</h3><h4 id="相关滤波">相关滤波</h4><p>互相关与子相关</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220307-201016.png" alt=""></p><p>图像追踪</p><p>输入第一帧，选定追踪区域提取特征，训练得到相关滤波器。对与之后的每一帧，裁剪之前的预测区域，进行特征提取，加窗函数进行FFT变换，然后与相关滤波器相乘，结果做IFFT，最大响应点所在的区域即为要追踪目标的位置，使用新位置进行新的相关滤波器的训练在进行预测。</p><h4 id="岭回归">岭回归</h4><p><a href="https://blog.csdn.net/weixin_44225602/article/details/112912067" target="_blank" rel="noopener">可参考内容</a></p><p>自己的理解</p><h5 id="优化原理">优化原理</h5><p>最小二乘法存在的问题：当自变量存在线性关系时，求的的回归系数波动较大。</p><p>且存在线性关系时，矩阵的奇异值接近0。</p><p>岭回归在最小二乘法的基础上添加k值，k值用于改变单位矩阵的对角线值。</p><p>引入k为了解决在自变量具有线性关系时导致计算最小二乘法时的不满秩的情况。</p><h5 id="K值的选取">K值的选取</h5><p>由于引入k之后属于有偏估计，因此k的值不应过大或过小。</p><p>选取k值时应考虑现实意义，防止出现自变量的值差距过大时且具有线性关系时导致的无现实意义的问题(符号)</p><p>k值的选取应在自变量稳定下的前提下进行选取。</p><h5 id="岭回归自变量的选取">岭回归自变量的选取</h5><p>自变量在稳定后对因变量影响较低的不予考虑。</p><p>自变量以波动形式稳定的不予考虑。</p><p>去除自变量后再次进行岭回归较好的不予考虑。</p><h4 id="傅立叶变换与逆傅立叶变换">傅立叶变换与逆傅立叶变换</h4><p>在图像处理中，将空间域转换为频域。</p><p>在频域中对原图像的相关细节对应到高频和低频进行图像处理。</p><p>使用逆傅立叶变换将处理完成的频域图转换为图像。</p><h4 id="循环矩阵">循环矩阵</h4><p>取数域a1-an，则循环矩阵为</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220308-150152.png" alt=""></p><p>即除第一行元素之外，其他行都是由第一行元素进行循环移位得到的。</p><h4 id="循环移位">循环移位</h4><p>对于输入图像来说，其实际是二维的，但在公式中对于输入 x 的定义往往是一个一维列向量。应理解为将图像逐元素由二维拉为一维，得到循环矩阵的第一行，之后再进行一维的循环移位得到循环矩阵。需要注意的是，<strong>实际中图像的循环移位并不是真正存在的，只是一种假设。有了这个假设，才能利用循环矩阵的性质，将相关操作转换为频域的卷积操作</strong>，以达到减少计算量，提升跟踪速度的效果。</p><h4 id="核函数">核函数</h4><p><strong>核函数是二元函数，输入是映射之前的两个向量，其输出等价于两个向量映射之后的内积。</strong></p><p>即将低维映射高维，然而高维的复杂计算可以通过低维的简单计算来获取。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220308-151343.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/WX20220308-151419.png" alt=""></p><h4 id="标量积">标量积</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/1046925-20200204093255306-1956874066.png" alt=""></p><p>向量 𝑟 对 𝑠 作投影，得到 𝑟𝑐𝑜𝑠𝛽，再将投影的大小 |𝑟|𝑐𝑜𝑠𝛽 和 𝑠 向量的大小 |𝑠| 相乘（可以交换，不管哪个向量对另一个向量作投影结果都是相同的）。</p><h4 id="核技巧">核技巧</h4><p>在当前唯独下的线性不可分的数据通过映射到高维度的空间时可以会变得线性可分。然后低维转到高维之后可能会面临维度灾难问题(即计算量)和每个点的转换问题。</p><p>核技巧就是当算法的输入向量x只以标量积的形式出现，可以用其他的核来替换标量积。</p><p>线性模型的正则化平方误差函数(真实值与预测值之间的误差)使用对偶公式可以用核函数进行表示，使用核函数在映射到高维空间时，其计算量也可以减少。</p><h4 id="窗函数">窗函数</h4><h5 id="信号泄露">信号泄露</h5><p>对于周期性信号而言，1帧的时间长度为周期性信号的整数倍，截断方式为周期性截断，截断后的信号也为周期性信号。在进行FFT变换之后，周期性信号的频域幅值是与原信号的幅值相同的。</p><p>对于图像追踪来说，信号的截断为非周期性截断，导致截取信号结束时刻与开始时刻的幅值是不想等的，同理IFFT时也是不想等的，这种情形就是频域放生了拖尾，也就是泄漏现象。</p><p>为了减少泄漏，加入了窗函数，满足FFT处理时的周期性需求，减少处理时的失真现象。</p><h5 id="窗函数-2">窗函数</h5><p><a href="https://zhuanlan.zhihu.com/p/24318554" target="_blank" rel="noopener">参考链接</a></p><p><strong>窗函数的时频域特征</strong></p><p>加窗实质是用一个所谓的窗函数与原始的时域信号作乘积的过程（当然加窗也可以在频域进行，但时域更为普遍），使得相乘后的信号似乎更好地满足傅立叶变换的周期性要求。如下图所示，原始的信号是不满足FFT变换的周期性要求的，变换后存在泄漏，如果施加一个窗函数，会在一定程度上减少泄漏。为了减少泄漏，用一个窗函数与原始周期信号相乘，得到加窗后的信号为周期信号，从而满足FFT变换的周期性要求。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-e795a8994ccab653d0ef26fcdfcf97da_720w.jpeg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-78d2244634e77047feeffacc7cb7aa80_720w.jpeg" alt=""></p><p>原始周期信号×窗函数</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-c7b8d090f913fa85f4c1903fa1ad839c_720w.jpeg" alt=""></p><p>各种窗函数频谱特征的主要差别在于：主瓣宽度（也称为有效噪声带宽，ENBW）、幅值失真度、最高旁瓣高度和旁瓣衰减速率等参数。加窗的主要想法是用比较光滑的窗函数代替截取信号样本的矩形窗函数，也就是对截断后的时域信号进行特定的不等计权，使被截断后的时域波形两端突变变得平滑些，以此压低谱窗的旁瓣。因为旁瓣泄露量最大，旁瓣小了泄露也相应减少了。不同的窗函数具有不同的频谱特征，下表列出了一些常用窗函数的特征。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-2e03a289e32c75ef80696fbb50e62a69_720w.jpeg" alt=""></p><p>主瓣宽度主要影响信号能量分布和频率分辨能力。频率的实际分辨能力为有效噪声带宽乘以频率分辨率，因此，主瓣越宽，有效噪声带宽越宽，在频率分辨率相同的情况下，频率的分辨能力越差。如下图所示，红色为平顶窗（3.77∆f），黑色为汉宁窗（1.5∆f），蓝色为信号频率，可以明显地看出，主瓣越窄，频率分辨越准确。对于窗函数宽的主瓣而言，如果有邻近的小峰值频率，则越难辨别出来。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-242a1e689b0eba71ea56f24d330a9651_720w.jpeg" alt=""></p><p>旁瓣高低及其衰减率影响能量泄漏程度（频谱拖尾效应）。旁瓣越高，说明能量泄漏越严重，衰减越慢，频谱拖尾越严重。对50.5Hz（频率分辨率为1Hz）的信号分别施加矩形窗（红色）、汉宁窗（绿色）和平顶窗（蓝色），用对数显示幅值，加窗后的结果如下图所示。从图中可以看出，矩形窗的频谱拖尾更严重。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-cd978cb28377a6ad101b1d5d19db0209_720w.jpeg" alt=""></p><p>相对而言，如果旁瓣能量较小，高度趋于零，使得信号能量相对集中于主瓣，则较为接近真实的频谱。不同的窗函数对信号频谱的影响是不一样的，这主要是因为不同的窗函数，产生泄漏的大小不一样，频率分辨能力也不一样。</p><p>加窗函数时，应使窗函数频谱的主瓣宽度应尽量窄，以获得高的频率分辨能力；旁瓣衰减应尽量大，以减少频谱拖尾，但通常都不能同时满足这两个要求。各种窗的差别主要在于集中于主瓣的能量和分散在所有旁瓣的能量之比。</p><p>窗的选择取决于分析的目标和被分析信号的类型。一般说，有效噪声频带越宽，频率分辨能力越差，越难于分清有相同幅值的邻近频率。选择性（即分辨出强分量频率邻近的弱分量的能力）的提高与旁瓣的衰减率有关。通常，有效噪声带宽窄的窗，其旁瓣的衰减率较低，因此窗的选择是在二者中取折衷。</p><p>因而，<strong>窗函数的选择一般原则如下</strong>：</p><ol><li><p>如果截断的信号仍为周期信号，则不存在泄漏，无须加窗，相当于加矩形窗。</p></li><li><p>如果信号是随机信号或者未知信号，或者有多个频率分量，测试关注的是频率点而非能量大小，建议选择汉宁窗，像LMS Test.Lab中默认加的就是汉宁窗。</p></li><li><p>对于校准目的，则要求幅值精确，平顶窗是个不错的选择。</p></li><li><p>如果同时要求幅值精度和频率精度，可选择凯塞窗。</p></li><li><p>如果检测两个频率相近、幅值不同的信号，建议用布莱克曼窗。</p></li><li><p>锤击法试验力信号加力窗，响应可加指数窗。</p></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像追踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KCF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu20.04安装Hadoop和Hive(PD虚拟机 MAC M1)</title>
      <link href="/2021/12/01/ubuntu20.04-an-zhuang-hadoop-he-hive-pd-xu-ni-ji-mac-m1/"/>
      <url>/2021/12/01/ubuntu20.04-an-zhuang-hadoop-he-hive-pd-xu-ni-ji-mac-m1/</url>
      
        <content type="html"><![CDATA[<h3 id="Ubuntu20-04安装Hadoop和Hive-PD虚拟机-MAC-M1">Ubuntu20.04安装Hadoop和Hive(PD虚拟机 MAC M1)</h3><p>对于本文中的markdown中的代码标签解析错误问题，可以通过下载该笔记的pdf文件进行直接复制即可，只需修改所在的路径。</p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/Ubuntu20.04%E5%AE%89%E8%A3%85Hadoop%E5%92%8CHive%28PD%E8%99%9A%E6%8B%9F%E6%9C%BA%20MAC%20M1%29.pdf" target="_blank" rel="noopener">pdf文件下载链接</a></p><h3 id="Ubuntu20-04安装mysql">Ubuntu20.04安装mysql</h3><h4 id="安装mysql">安装mysql</h4><pre class=" language-language-shell"><code class="language-language-shell">#更新源sudo apt-get update#安装mysql服务sudo apt-get install mysql-server</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/mysql1.png" alt=""></p><h4 id="配置mysql">配置mysql</h4><pre class=" language-language-shell"><code class="language-language-shell">sudo mysql_secure_installation</code></pre><pre class=" language-language-shell"><code class="language-language-shell">VALIDATE PASSWORD PLUGIN can be used to test passwords...Press y|Y for Yes, any other key for No: N (选择N ,不会进行密码的强校验)Please set the password for root here...New password: (输入密码)Re-enter new password: (重复输入)By default, a MySQL installation has an anonymous user,allowing anyone to log into MySQL without having to havea user account created for them...Remove anonymous users? (Press y|Y for Yes, any other key for No) : N (选择N，不删除匿名用户)Normally, root should only be allowed to connect from'localhost'. This ensures that someone cannot guess atthe root password from the network...Disallow root login remotely? (Press y|Y for Yes, any other key for No) : N (选择N，允许root远程连接)By default, MySQL comes with a database named 'test' thatanyone can access...Remove test database and access to it? (Press y|Y for Yes, any other key for No) : N (选择N，不删除test数据库)Reloading the privilege tables will ensure that all changesmade so far will take effect immediately.Reload privilege tables now? (Press y|Y for Yes, any other key for No) : Y (选择Y，修改权限立即生效)</code></pre><h4 id="配置远程访问">配置远程访问</h4><pre class=" language-language-shell"><code class="language-language-shell">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf #找到bind-address 修改值为 0.0.0.0sudo /etc/init.d/mysql restart #重启mysql</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/mysql2.png" alt=""></p><p><strong>刷新权限</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/mysql3.png" alt=""></p><h4 id="远程连接-navicat">远程连接(navicat)</h4><p>ip地址为10.211.55.4</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/mysql4.png" alt=""></p><h3 id="ubuntu20-04安装Hadoop">ubuntu20.04安装Hadoop</h3><h4 id="安装jdk1-8">安装jdk1.8</h4><p><a href="https://www.oracle.com/java/technologies/javase-downloads.html" target="_blank" rel="noopener">官方链接</a></p><pre class=" language-language-shell"><code class="language-language-shell">mkdir dingdm                                        #创建dingdm文件夹</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/jdk1.png" alt=""></p><p>上传jdk文件到pd虚拟机</p><pre class=" language-language-shell"><code class="language-language-shell">sudo tar zxvf jdk-8u311-linux-aarch64.tar.gz  -C /dingdm  #/解压到/dingdm目录下mv  jdk1.8.0_311 java                          #重命名为javavi ~/.bashrc                                              #给JDK配置环境变量</code></pre><p>在 ~/.bashrc 文件中添加如下代码</p><pre class=" language-language-shell"><code class="language-language-shell">export JAVA_HOME=/dingdm/javaexport JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH</code></pre><pre class=" language-language-shell"><code class="language-language-shell">source ~/.bashrc                       #使新配置的环境变量生效</code></pre><h4 id="安装hadoop">安装hadoop</h4><p><a href="https://archive.apache.org/dist/hadoop/common/" target="_blank" rel="noopener">官方链接</a></p><p>上传hadoop文件到pd虚拟机</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hadoop1.png" alt=""></p><h5 id="配置环境变量">配置环境变量</h5><pre class=" language-language-shell"><code class="language-language-shell">sudo tar -zxvf  hadoop-3.2.0.tar.gz                   #解压sudo mv  hadoop-3.2.0    hadoop                      #重命名为hadoop，可改可不改，如果修改下边的名字也要对应</code></pre><pre class=" language-language-shell"><code class="language-language-shell">export HADOOP_HOME=/dingdm/hadoopexport CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATHexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></pre><pre class=" language-language-shell"><code class="language-language-shell">source ~/.bashrc</code></pre><h4 id="伪分布式配置">伪分布式配置</h4><pre class=" language-language-shell"><code class="language-language-shell">cd /dingdm/hadoop/etc/hadoop/ </code></pre><p>hadoop-env.sh中配置java环境</p><pre class=" language-language-sh"><code class="language-language-sh">export JAVA_HOME=/dingdm/java</code></pre><p>修改core-site.xml与hdfs-site.xml文件，在configuration下添加property标签配置</p><pre class=" language-language-xml"><code class="language-language-xml">\*<property>    <name>hadoop.tmp.dir</name>    <value>file:/dingdm/hadoop/tmp</value>    <description>Abase for other temporary directories.</description></property><property>    <name>fs.defaultFS</name>    <value>hdfs://localhost:9000</value></property></code></pre><pre class=" language-language-xml"><code class="language-language-xml"> <property>     <name>dfs.replication</name>     <value>1</value></property><property>     <name>dfs.namenode.name.dir</name>     <value>file:/dingdm/hadoop/tmp/dfs/name</value></property><property>     <name>dfs.datanode.data.dir</name>     <value>file:/dingdm/hadoop/tmp/dfs/data</value></property><property>     <name>dfs.http.address</name>     <value>0.0.0.0:50070</value></property></code></pre><p>执行 NameNode 的格式化</p><pre class=" language-language-shell"><code class="language-language-shell">cd /dingdm/hadoop/binhdfs namenode -format</code></pre><p>启动hadoop</p><pre class=" language-language-shell"><code class="language-language-shell">cd /dingdm/hadoop/sbinstart-dfs.shjps</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hadoop2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hadoop3.png" alt=""></p><h3 id="ubuntu20-04安装Hive">ubuntu20.04安装Hive</h3><h4 id="文件上传">文件上传</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hive1.png" alt=""></p><h4 id="配置环境变量-2">配置环境变量</h4><pre class=" language-language-shell"><code class="language-language-shell">export HIVE_HOME=/dingdm/hiveexport PATH=$PATH:$HIVE_HOME/bin</code></pre><pre class=" language-language-shell"><code class="language-language-shell">source ~/.bashrc</code></pre><h4 id="Hive配置文件">Hive配置文件</h4><p>指定 Hive 数据仓库的数据存储在 HDFS 上的目录</p><pre class=" language-language-shell"><code class="language-language-shell">sudo mkdir -p /user/hive/warehousesudo mkdir -p /tmp/hivesudo chmod -R 777 /user/hive/warehousesudo chmod -R 777 /tmp/hive</code></pre><p>在/dingdm/hive/conf目录下，创建hive-site.xml，并添加以下代码</p><pre class=" language-language-xml"><code class="language-language-xml"><configuration><property>    <name>hive.metastore.schema.verification</name>    <value>false</value>     <description>          Enforce metastore schema version consistency.          True: Verify that version information stored in metastore matches with one from Hive jars.  Also disable automatic          schema migration attempt. Users are required to manully migrate schema after Hive upgrade which ensures          proper metastore schema migration. (Default)          False: Warn if the version information stored in metastore doesn't match with one from in Hive jars.      </description>    </property>    <!-- 存储在hdfs上的数据路径 -->    <property>    <name>hive.metastore.warehouse.dir</name>    <value>/user/hive/warehouse</value>    <description>location of default database for the warehouse</description>  </property>  <property>    <name>hive.exec.scratchdir</name>    <value>/tmp/hive</value>    <description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.</description>  </property>    <!-- 本地mysql -->  <property>                <name>javax.jdo.option.ConnectionURL</name>                <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>                <description>JDBC connect string for a JDBC metastore</description>                <!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost -->        </property>        <property>                <name>javax.jdo.option.ConnectionDriverName</name>                <value>com.mysql.cj.jdbc.Driver</value>                <description>Driver class name for a JDBC metastore</description>        </property>        <property>                <name>javax.jdo.option.ConnectionUserName</name>                <value>root</value>                <description>username to use against metastore database</description>        </property>        <property>                <name>javax.jdo.option.ConnectionPassword</name>                <value>你的数据库密码</value>        <description>password to use against metastore database</description>        </property></configuration></code></pre><p>在/dingdm/hive/conf下执行</p><pre class=" language-language-shell"><code class="language-language-shell">cp hive-env.sh.template hive-env.sh</code></pre><p>添加以下内容</p><pre class=" language-language-shell"><code class="language-language-shell">export HADOOP_HOME=/dingdm/hadoopexport HIVE_CONF_DIR=/dingdm/hive/conf</code></pre><p>拷贝jdbc的包到/dingdm/hive/lib目录下</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hive2.png" alt=""></p><p>在dingdm/hive/bin目录下启动hive</p><pre class=" language-language-shell"><code class="language-language-shell">schematool -dbType mysql -initSchemaschematool -dbType mysql -infohive</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hive3.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>L0梯度最小化去雨论文代码</title>
      <link href="/2021/11/29/l0-ti-du-zui-xiao-hua-qu-yu-lun-wen-dai-ma/"/>
      <url>/2021/11/29/l0-ti-du-zui-xiao-hua-qu-yu-lun-wen-dai-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="Rain-Removal-From-Still-Images-Using-L0-Gradient-Minimization-Technique论文代码">Rain Removal From Still Images Using L0 Gradient Minimization Technique论文代码</h3><h4 id="原图像">原图像</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/f1.png" alt=""></p><h4 id="find-rain">find_rain</h4><pre class=" language-language-matlab"><code class="language-language-matlab">function [No_Of_RainPixels OnlyRainImage] = find_rain(I)% 三通道彩色图转换为灰度二值图if ndims(I) == 3   I2=rgb2gray(I);else   I2 = I;end% 对二值图进行形态学操作% 创建一个任意形状的结构元素，5*5的矩阵，用于指定形状可以用se=strel(NHOOD)简化 进行图像腐蚀%开运算数学上是先腐蚀后膨胀的结果%开运算的物理结果为完全删除了不能包含结构元素的对象区域，平滑了对象的轮廓，断开了狭窄的连接，去掉了细小的突出部分SE = strel('arbitrary',eye(5));BW2 = imerode(I2,SE);% 图像膨胀BW3 = imdilate(BW2,SE);% 对图像实现闭运算%闭运算在数学上是先膨胀再腐蚀的结果%闭运算的物理结果也是会平滑对象的轮廓，但是与开运算不同的是，闭运算一般会将狭窄的缺口连接起来形成细长的弯口，并填充比结构元素小的洞closeBW = imclose(BW3,SE);% 图像膨胀Iobrd = imdilate(BW2, SE );% 重构是涉及两幅图和一个结构元素的形态学变换，imcomplement(Iobrd)成为标记，作为变换的开始点，imcomplement(closeBW)掩模，用来约束变换过程。% imcomplement(closeBW) 中符合 imcomplement(Iobrd) 的特征% 结构元素用于定义连结性。% 重构不是从一副缺失的图像中重新构建完整图像，而是提取原始图像中的含有某些特征的连通区域来构建新的图像。% 对图像掩膜下的图像标记进行形态重建，并返回Iobrcbr中的重建Iobrcbr = imreconstruct(imcomplement(Iobrd), imcomplement(closeBW));% 对图像数据进行取反运算（实现底片效果）。反转图像的像素强度，使图像中的前景变为背景，背景变为前景。Iobrcbr = imcomplement(Iobrcbr);% 返回识别Iobrcbr中的区域最大值的二值图像BW 区域最大值是具有恒定强度值t的像素的连通分量，其外部边界像素都具有小于t的值。在BW中，设置为1的像素标识区域最% % 大值；所有其他像素都设置为0。OnlyRainImage = imregionalmax(Iobrcbr);% 找出二值图像中所有的连通区域。所需的连通性为8 cc = bwconncomp(OnlyRainImage,8); No_Of_RainPixels = cc.NumObjects;</code></pre><h5 id="图片腐蚀">图片腐蚀</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/kai.png" alt=""></p><h5 id="图片膨胀">图片膨胀</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/peng.png" alt=""></p><h5 id="图像闭运算">图像闭运算</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/bi.png" alt=""></p><h5 id="区域最大值的二值图像即雨滴图像">区域最大值的二值图像即雨滴图像</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/iyd.png" alt=""></p><h4 id="train">train</h4><h5 id="Lab色彩空间">Lab色彩空间</h5><p>Lab是一种不常用的色彩空间。它是在1931年国际照明委员会（CIE）制定的颜色度量国际标准的基础上建立起来的。1976年，经修改后被正式命名为CIELab。它是一种设备无关的颜色系统，也是一种基于生理特征的颜色系统。这也就意味着，它是用数字化的方法来描述人的视觉感应。Lab颜色空间中的L分量用于表示像素的亮度，取值范围是[0,100],表示从纯黑到纯白；a表示从红色到绿色的范围，取值范围是[127,-128]；b表示从黄色到蓝色的范围，取值范围是[127,-128]。</p><h5 id="峰值信噪比">峰值信噪比</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fengzhi.png" alt=""></p><p>MSE表示当前图像X和参考图像Y的均方误差（Mean Square Error），H、W分别为图像的高度和宽度；n为每像素的比特数，一般取8，即像素灰阶数为256. PSNR的单位是dB，数值越大表示失真越小。</p><pre class=" language-language-matlab"><code class="language-language-matlab">I = imread(['/Users/dinggongcheng/Documents/MATLAB/2/','02.png']);I_Large = imresize(I,[650,950]);imshow(I_Large);title('Rainy Image');% 返回输入图像中没有被雨滴覆盖的像素数量和纯雨滴的图像[No_Of_RainPixels1 OnlyRainImage1] = find_rain(I);Rain = imresize(OnlyRainImage1,[350,650]);figure,imshow(Rain);title('OnlyRainImage1');Img = I;% 调节灰度图像的亮度或彩色图像的颜色矩阵% stretchlim自适应找到一个分割阈值向量来改变一幅图像的对比度Img = imadjust(Img,stretchlim(Img));% L0范数平滑S = L0Smoothing(Img);% 对比度增强S = imadjust(S,stretchlim(S));% 定义指定的颜色空间转换的类型。执行转换,将颜色转换结构作为参数传递给applycform功能。srgb2lab = makecform('srgb2lab'); % RGB转lab公式lab2srgb = makecform('lab2srgb'); % lab转RGB公式% 将rgb颜色空间转换为Lab颜色空间shadow_lab = applycform(S, srgb2lab);% 缩放亮度值到[0,1] 第一个亮度为度缩小100倍max_luminosity = 100;L = shadow_lab(:,:,1)/max_luminosity;shadow_imadjust = shadow_lab;% 将灰度图像L(亮度缩放)中的灰度值映射成输出图像shadow_imadjust中的亮度维度新值，使得灰度图像L在低灰度值和高灰度值上1%的数据是饱和的。这增强了输出图像% shadow_imadjust的对比度。% imadjust(L)*max_luminosity shadow_imadjust(:,:,1) = imadjust(L) * max_luminosity;% 将Lab颜色空间转换为rgb颜色空间shadow_imadjust = applycform(shadow_imadjust, lab2srgb);% 返回输入图像中没有被雨滴覆盖的像素个数和纯雨滴的图像[No_Of_RainPixels2 OnlyRainImage2] = find_rain(shadow_imadjust);shadow = imresize(shadow_imadjust,[650,950]);Rain2 = imresize(OnlyRainImage2,[350,650]);figure,imshow(Rain2);title('OnlyRainImage2');figure,imshow(shadow);title(' Rain Removed Image ');% 计算峰值信噪比im1 = double(Img);im2 = double(shadow_imadjust);mse = sum((im1(:)-im2(:)).^2)/prod(size(im1));psnr1 = 10*log10(255*255/mse);im3 = double(S);mse2 = sum((im1(:)-im3(:)).^2)/prod(size(im1));psnr2 = 10*log10(255*255/mse2);psnr = [psnr2,psnr1];% 直方图显示figure,bar(psnr,0.25);title('PSNR values before & after Rain Removal');A1 = No_Of_RainPixels1;A2 = No_Of_RainPixels2;kk = bwconncomp(OnlyRainImage2,8);NoOfRainPixels2 = kk.NumObjects;Original = A1Removed = NoOfRainPixels2Remove = NoOfRainPixels2/A1;Removed = 1-Remove;R= Removed*100;</code></pre><h5 id="对比度增强图片">对比度增强图片</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/duibidu.png" alt=""></p><h5 id="L0范数平滑后的对比度图像">L0范数平滑后的对比度图像</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/L.png" alt=""></p><h5 id="缩放100比例亮度并在Lab色彩空间更新亮度维度后找到区域最大值的二值图像">缩放100比例亮度并在Lab色彩空间更新亮度维度后找到区域最大值的二值图像</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/f3.png" alt=""></p><h5 id="去雨图像">去雨图像</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/f4.png" alt=""></p><h5 id="PSNR">PSNR</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/psnr.png" alt=""></p><h4 id="其他测试图">其他测试图</h4><h5 id="1-原图">1 原图</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/01y.png" alt=""></p><h5 id="1-去雨">1 去雨</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/01q.png" alt=""></p><h5 id="1-PSNR">1 PSNR</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/01p.png" alt=""></p><h5 id="2-原图">2 原图</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/03y.png" alt=""></p><h5 id="2-去雨">2 去雨</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/03q.png" alt=""></p><h5 id="2-PSNR">2 PSNR</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/03p.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode刷题(11.24)</title>
      <link href="/2021/11/24/leetcode-shua-ti-11.24/"/>
      <url>/2021/11/24/leetcode-shua-ti-11.24/</url>
      
        <content type="html"><![CDATA[<h3 id="leetcode刷题-11-24">leetcode刷题(11.24)</h3><h4 id="题目">题目</h4><p>给你一个链表的头节点 head 和一个特定值 x ，请你对链表进行分隔，使得所有 小于 x 的节点都出现在 大于或等于 x 的节点之前。</p><p>你应当 保留 两个分区中每个节点的初始相对位置。</p><p><strong>示例 1：</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/partition.jpeg" alt=""></p><pre><code>输入：head = [1,4,3,2,5,2], x = 3输出：[1,2,2,4,3,5]</code></pre><p><strong>示例 2：</strong></p><pre><code>输入：head = [2,1], x = 2输出：[1,2]</code></pre><h4 id="题解">题解</h4><pre class=" language-language-java"><code class="language-language-java">/** * Definition for singly-linked list. * public class ListNode { *     int val; *     ListNode next; *     ListNode() {} *     ListNode(int val) { this.val = val; } *     ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */class Solution {    public ListNode partition(ListNode head, int x) {        ListNode small = new ListNode(0);        // 头节点        ListNode smallHead = small;        // 头节点        ListNode big = new ListNode(0);        ListNode bigHead = big;        while(head != null){            if(head.val < x){                small.next = head;                small = small.next;            }else{                big.next = head;                big = big.next;            }            head = head.next;        }        big.next = null;        small.next = bigHead.next;        return smallHead.next;    }}</code></pre><h4 id="学习点">学习点</h4><p>1.分割思想</p><p>2.链表指针思想</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FPN网络理论</title>
      <link href="/2021/11/11/fpn-wang-luo-li-lun/"/>
      <url>/2021/11/11/fpn-wang-luo-li-lun/</url>
      
        <content type="html"><![CDATA[<h3 id="FPN网络理论">FPN网络理论</h3><h4 id="FPN论文">FPN论文</h4><h5 id="摘要">摘要</h5><p>特征金字塔是识别系统中用于检测不同尺度对象的基本组件。但是最近的深度学习对象检测器避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络固有的多尺度金字塔层次结构来构建没有额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于构建所有尺度的高级语义特征图。这种称为特征金字塔网络 (FPN) 的架构在多个应用程序中作为通用特征提取器拥有显著的改进。在基本的 Faster R-CNN 系统中使用 FPN，我们的方法在COCO检测基准上实现了最先进的单模型结果，超越了所有现有的单模型，包括来自COCO 2016的挑战胜利者。此外，我们的方法可以在 GPU 上以 6 FPS 的速度运行，因此是多尺度目标检测的实用且准确的解决方案。</p><p>我们的目标是利用卷积网络的金字塔具有从低级到高级的语义特征层次结构，并构建一个始终具有高级语义的特征金字塔。 由此产生的特征金字塔网络是通用的，在本文中，我们专注于滑动窗口区域候选器和基于区域的检测器（Fast R-CNN）。我们还将 FPN 推广到实例分割中。</p><p>我们的方法将任意大小的单尺度图像作为输入，并以完全卷积的方式在多个级别输出按比例大小的特征图。 该过程独立于主干卷积架构，在本文中，我们使用 ResNets展示了结果。 我们金字塔的构建涉及自下而上的路径、自上而下的路径和横向连接，如下所述。</p><h5 id="自下而上的路径">自下而上的路径</h5><p>自下而上的路径是主干卷积网络的前馈计算，它计算由多个尺度的特征图组成的特征层次结构，缩放步长为 2。通常有许多层产生相同大小的输出图，我们说这些层在同一个网络阶段。 对于我们的特征金字塔，我们为每个阶段定义一个金字塔级别。 我们选择每个阶段最后一层的输出作为我们的特征图参考集，我们将对其进行丰富以创建我们的金字塔。 这种选择很自然，因为每个阶段的最深层应该具有最强的特征。</p><p>具体来说，对于 ResNets，我们使用每个阶段的最后一个残差块输出的特征激活。 对于 conv2、conv3、conv4 和 conv5 输出，我们将这些最后残差块的输出表示为 {C2、C3、C4、C5}，并注意到它们的步长为 {4, 8, 16, 32} 个像素输入图像。 由于其大内存占用，我们没有将 conv1 包含在金字塔中。</p><h5 id="自上而下的路径和横向连接">自上而下的路径和横向连接</h5><p>自上而下的路径通过对来自更高金字塔级别的空间上更粗糙但语义上更强大的特征图进行上采样来产生更高分辨率的特征。 这些特征通过横向连接从自下而上的路径中得到增强。 每个横向连接从自下而上的路径和自上而下的路径中合并相同空间大小的特征图。 自下而上的特征图具有较低级别的语义，但因为它被子采样的次数更少其激活更准确。</p><p>图 3 显示了构建自顶向下特征图的构建块。 对于较粗糙分辨率的特征图，我们将空间分辨率上采样 2 倍（为简单起见，使用最近邻上采样）。 然后通过逐维度加法将上采样映射与相应的自底向上映射（经过 1×1 卷积层以减少通道尺寸）合并。 这个过程会不断迭代，直到生成最精细的分辨率特征图。 开始迭代之前，我们简单地在 C5 上附加一个 1×1 的卷积层来生成最粗糙的分辨率图。 最后，我们在每个合并图上附加一个 3 × 3 的卷积来生成最终的特征图，这是为了减少上采样的混叠效应。 这最后一组特征图称为{P2，P3，P4，P5}，对应于分别具有相同空间大小的{C2，C3，C4，C5}。</p><p>简单性是我们设计的核心，我们发现我们的模型对许多设计选择都是稳健的。 我们已经对更复杂的块进行了实验（例如，使用多层残差块作为连接）并观察到稍微好一点的结果。 设计更好的连接模块不是本文的重点，因此我们选择了上述的简单设计。</p><h4 id="相关理论">相关理论</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fpn3.png" alt=""></p><p>FPN是为了自然地利用CNN层级特征的金字塔形式，<strong>同时生成在所有尺度上都具有强语义信息的特征金字塔</strong>。所以FPN的结构设计了top-down结构和横向连接，以此融合具有高分辨率的浅层layer和具有丰富语义信息的深层layer。这样就实现了从单尺度的单张输入图像，快速构建在所有尺度上都具有强语义信息的特征金字塔，同时不产生明显的代价。</p><p>因为深度卷积神经网络具有内在的多尺度性质（池化带来的不同size的feature map），网络由浅即深，分辨率越来越粗糙，但是语义信息越来越丰富。相似的思想利用在FCN的skip layer。要识别多尺度的目标，也需要利用不同level的feature map。作者利用了这种内在多尺度性质，设计出FPN。</p><p>FPN中的缩放比例为2的整数倍，合并的项为横向连接的1*1卷积核和自上而下的2倍的上采样。</p><p>使用最近邻值插值法，可以在上采样的过程中最大程度地保留特征图的语义信息(有利于分类)，从而与bottom-up 过程中相应的具有丰富的空间信息(高分辨率，有利于定位)的特征图进行融合，从而得到既有良好的空间信息又有较强烈的语义信息的特征图。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fpn1.png" alt=""></p><p>代码明天根据这张模型图慢慢写。这个模型是用于faster rcnn的。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ResNet网络理论</title>
      <link href="/2021/11/11/resnet-wang-luo-jie-gou-li-lun/"/>
      <url>/2021/11/11/resnet-wang-luo-jie-gou-li-lun/</url>
      
        <content type="html"><![CDATA[<h3 id="ResNet网络理论">ResNet网络理论</h3><h4 id="ResNet论文">ResNet论文</h4><h5 id="摘要">摘要</h5><p>更深的神经网络更难训练。我们提出了一个残差学习框架，以简化比普通网络更深的网络的训练。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从显着增加的深度中获得准确性。在 ImageNet 数据集上，我们评估了深度高达 152 层的残差网络——比 VGG 网络深8倍，但仍然具有较低的复杂性。这些残差网络的集合在 ImageNet 测试集上实现了 3.57% 的错误率。该结果在 ILSVRC 2015 分类任务中获得第一名。我们还对具有 100 层和 1000 层的 CIFAR-10 进行了分析。</p><h5 id="残差学习">残差学习</h5><p>让我们将 H(x) 视为适合几个堆叠层（不一定是整个网络）的底层映射，其中 x 表示这些层中第一层的输入，如果假设多个非线性层可以渐近逼近复杂函数，那么假设它们可以渐近逼近残差函数，即 H(x) − x（假设输入和输出是尺寸相同）。 因此，与其期望堆叠层近似 H(x)，我们明确地让这些层近似一个残差函数 F(x) = H(x) − x。原来的函数因此变成了 F(x)+x。 尽管这两种形式都应该能够渐近逼近所需的函数（如假设的那样），但学习的难易程度可能不同。</p><p>这种重新表述的动机是关于退化问题的违反直觉的现象。 正如我们在介绍中所讨论的，如果添加的层可以构建为恒等映射，那么更深的模型的训练误差应该不大于其更浅的对应部分。 退化问题表明求解器可能难以通过多个非线性层逼近恒等映射。 通过残差学习重新表述，如果恒等映射是最优的，求解器可以简单地将多个非线性层的权重推向零以接近恒等映射。</p><p>在实际情况下，恒等映射不太可能是最佳的，但我们的重新表述可能有助于先决问题。 如果最优函数更接近恒等映射而不是零映射，则求解器应该更容易参考恒等映射找到扰动，而不是将函数作为新函数学习。 我们通过实验表明，学习到的残差函数通常具有很小的响应，这表明恒等映射提供了合理的预处理。</p><h5 id="通过捷径进行恒等映射">通过捷径进行恒等映射</h5><p>我们对每几个堆叠层采用残差学习。 一个构成如图所示。正式地，在本文中，我们考虑一个公式定义为</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet5.png" alt=""></p><p>这里 x 和 y 是所考虑层的输入和输出向量。 函数 F (x, {Wi }) 表示要学习的残差映射。 对于图 中具有两层的示例，F = W2σ(W1x) 其中 σ 表示 ReLU激活函数，并且为了简化符号省略了偏差。 操作 F + x 是通过快捷连接和逐元素加法来执行的。</p><p>等式中的快捷连接既不引入额外参数，也不引入计算复杂度。 这不仅在实践中很有吸引力，而且在我们比较普通网络和残差网络时也很重要。 我们可以公平地比较同时具有相同数量参数、深度、宽度和计算成本的普通/残差网络。</p><p>x 和 F 的维度在等式中必须相等。 如果不是这种情况，我们可以通过快捷连接进行线性投影 Ws 以匹配维度</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet6.png" alt=""></p><p>我们也可以在方程中使用方Ws。 但是我们将通过实验证明恒等映射足以解决退化问题并且是便宜的，因此Ws仅在匹配维度时使用。</p><p>残差函数 F 的形式是灵活的。 本文中的实验涉及具有两层或三层的函数 F，但更多层也是可能的。 但如果F只有单层，对此我们没有观察到优势。</p><p>我们还注意到，虽然上述关于全连接层的符号是简化的，但它们适用于卷积层。 函数 F(x,{Wi}) 可以表示多个卷积层，逐个通道地在两个特征图上执行维度添加。</p><h5 id="网络架构">网络架构</h5><h6 id="残差网络">残差网络</h6><p>基于上述普通网络，我们插入快捷连接，将网络转换为其对应的残差版本。 当输入和输出的维度相同时，可以直接使用恒等快捷连接。 当维度增加时，我们考虑两个选项：（A）快捷连接仍然执行恒等映射，填充额外的零条目以增加维度。 这个选项没有引入额外的参数； (B) 使用投影快捷方式匹配维度（由 1×1 卷积完成）。 对于这两个选项，当快捷连接连接两种尺寸的特征图时，它们的步长为 2。</p><h4 id="实现">实现</h4><p>我们对 ImageNet 的实现遵循前人的做法。图像被调整大小，其较短的边在 [256, 480] 中随机采样以进行缩放 。一个 224×224 的裁剪是从图像或其水平翻转中随机采样的，减去每个像素的平均值 。使用了 [21] 中的标准颜色增强。我们在每次卷积之后和激活之前采用批量归一化（BN），遵循 [16]。我们按照 [13] 中的方法初始化权重，并从头开始训练所有普通/残差网络。我们使用小批量大小为 256 的 SGD优化函数。学习率从 0.1 开始，并在误差平稳时除以 10，并且模型最多训练 60 × 104 次迭代。我们使用 0.0001 的权重衰减和 0.9 的动量。我们不使用 dropout [14]，遵循 [16] 中的做法。<br>在测试中，对于消融实验，我们采用标准的 10 裁剪测试 [21]。为了获得最佳结果，我们采用 [41, 13] 中的全卷积形式，并在多个尺度上取平均得分（调整图像大小，使短边在 {224, 256, 384, 480, 640} 中）。</p><p>梯度消失或梯度爆炸<br>退化问题</p><p>残差网络的相加是维度的相加</p><h4 id="残差块">残差块</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet1.png" alt=""></p><p>使用1*1的卷积核可以有效的减少卷积过程中的参数个数</p><h4 id="ResNet卷积网络">ResNet卷积网络</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet7.png" alt=""></p><p>在使用残差块时，对输入和输出特征进行相加时要保证矩阵的shape相同，因此对于输入和输出矩阵shape不相同的情况，在进行相加前，需要对输入特征矩阵进行1*1的卷积操作来改变特征矩阵的通道数。</p><h4 id="BN">BN</h4><p>对于一个具有n层深度的输入特征矩阵，即一个拥有n维的输入x，bn标准化处理就是对这n个通道分别进行标准化处理使得每个通道的特征矩阵都满足某个分布规律。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet4.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resnet3.png" alt=""></p><p>根据特征矩阵求的均值和方差，均值和方差都为n维向量，再通过bn的公式进行标准化处理。</p><p>1.训练时要将training参数设置为True，在验证时将training参数设置为False。使用历史统计的均值和方差。<br>2.batch size尽可能设置大点，设置小后表现可能很差，设置的越大求的均值和方差越接近整个训练集的均值和方差。<br>3.建议将bn层放在卷积层和激活层之间，且卷积层不要使用偏置bias。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolo相关理论补充</title>
      <link href="/2021/11/09/yolo-xiang-guan-li-lun/"/>
      <url>/2021/11/09/yolo-xiang-guan-li-lun/</url>
      
        <content type="html"><![CDATA[<h3 id="yolo相关理论补充">yolo相关理论补充</h3><p>TP:iou大于阈值检测框数量<br>FP:iou小于等于阈值的检测框数量<br>FN:没有检测到的GT的数量</p><p>precision:查准率，TP/(TP+FP)，预测正确的比例<br>recall:查全率，TP/(TP+FN)，真实目标中，模型预测正确的目标比例</p><p>AP:P-R曲线下面积<br>P-R曲线:precision-recall曲线</p><p>mAP:各类AP的平均值</p><h4 id="yolo-v1">yolo v1</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v.png" alt=""></p><p>1.将一幅图像分成S*S个网格，如果某个object的中心落在这个网格中，则这个网络就负责预测这个object。</p><p>2.每个网格要预测B个bounding box，每个bounding box除了要预测位置之外，还要附带预测一个confidence值。每个网格还要预测C个类别的分数。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v1%20art.png" alt=""></p><p>在yolo v1的架构中，每一个网格进行预测时要预测两个bounding box，即两对位置参数和两对confidence，以及数据集设定的分类数量。即一共30个参数。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolov1%20in.png" alt=""></p><p>yolo v1的网络架构中，前面的部分都是正常的卷积，需要注意的是由于是端到端的训练，在最后的全连接层输出4096维的向量之后，需要进行reshape最终输出7*7*30的tensor，而其中对应的某一列的结构在上图中已经说明。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v1%20loss.png" alt=""></p><p>在yolo的边界损失函数中，对于高和宽两个属性值，对于相同的偏移，小物体的检测框与大物体的检测框的误差是不一样的，对于小物体来说，iou更小，因此应该对这两个参数进行开方，使用幂函数进行loss损失计算。其他的损失都是使用误差平方和进行计算。</p><h4 id="yolo-v3">yolo v3</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v3%20ar.png" alt=""></p><p>残差模块和主分支上的输出进行相加。每个框起来的就是一个残差。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/anchor.png" alt=""></p><p>对于yolo v3的tensor的参数为85个，四个预测的位置参数，一个confidence和COCO数据集的80个分类。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v3%20xia.png" alt=""></p><p>在yolov3中进行输出预测时，对于特征图的拼接和FPN的维度拼接不同，yolov3中的拼接是深度的拼接。预测输出时使用的为1*1卷积，而不是卷积归一和激活。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolov3%20mu.png" alt=""></p><p>yolov3在进行目标边界框检测时使用了sigmoid函数来固定预测坐标的位置在单元格之内。</p><p>置信度损失</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v3%20zhi.png" alt=""></p><p>置信度损失使用的是二元交叉熵损失，oi为预测目标边界框与真实目标边界框的IOU，c为预测值，Ci hat为c通过sigmod函数得到的预测置信度，N为正负样本的总和。</p><p>类别损失</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v3%20lei.png" alt=""></p><p>其中Oij为预测目标边界框i中是否存在第j类目标，Cij为预测值，Cij hat为Cij通过Sigmod函数得到的目标概率，Npos为正样本个数。</p><p>定位损失</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolo%20v3%20ding.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolov3%20can.png" alt=""></p><p>定位损失中的g hat值由真实标签的中心点位置减去上图边界框检测图中的单元格的左上角左边求的。对应的gw和gh hat由预测的公式反向计算得到。</p><h4 id="SPP">SPP</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolov3%20spp.png" alt=""></p><p>DIOU</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/diou.png" alt=""></p><p>上述损失函数中，<code>b，bgt</code>分别代表了anchor框和目标框的中心点，且<code>p</code>代表的是计算两个中心点间的欧式距离。<code>c</code>代表的是能够同时覆盖anchor和目标框的最小矩形的对角线距离。因此DIoU中对anchor框和目标框之间的归一化距离进行了建模。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/dioupp.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AUNet论文阅读</title>
      <link href="/2021/10/07/aunet-lun-wen-yue-du/"/>
      <url>/2021/10/07/aunet-lun-wen-yue-du/</url>
      
        <content type="html"><![CDATA[<h3 id="AUNet论文阅读">AUNet论文阅读</h3><h5 id="论文理解">论文理解</h5><p>作者采用FPN金字塔作为主干网络，对实例和语义分割进行融合，使用了RoIUpsample和PAM，MAM。</p><p>核心点就是通过前景的语境信息帮助背景进行事物分割。</p><p>PAM：通过RPN的粗糙前景指导背景分割的分支。</p><p>MAM：将Mask进行上采样恢复到feature map大小，之后采用PAM的操作。</p><p>代码实现后面在写。不管是什么，我小机灵先放一张模型图在这就完事了。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu2" alt=""></p><h5 id="相关知识">相关知识</h5><h6 id="注意力机制">注意力机制</h6><p>attention机制可以它认为是一种资源分配的机制，可以理解为对于原本平均分配的资源根据attention对象的重要程度重新分配资源，重要的单位就多分一点，不重要或者不好的单位就少分一点，在深度神经网络的结构设计中，attention所要分配的资源基本上就是权重了</p><p>视觉注意力分为几种，核心思想是基于原有的数据找到其之间的关联性，然后突出其某些重要特征，有通道注意力，像素注意力，多阶注意力等，也有把NLP中的自注意力引入。</p><p>注意力机制的各种类型介绍https://zhuanlan.zhihu.com/p/146130215</p><p>相关论文以后阅读，这里只是进行初步的了解。</p><h6 id="RPN">RPN</h6><p>Region Proposal Network，直接翻译是“<strong>区域生成网络</strong>”，通俗讲是“筛选出可能会有目标的框”。其本质是基于滑窗的无类别object检测器，输入是任意尺度的图像，输出是一系列矩形候选区域。</p><p><a href="https://zhuanlan.zhihu.com/p/138515680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/138515680</a></p><h6 id="归一化">归一化</h6><p>神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。<br>在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。</p><p><a href="https://blog.csdn.net/qq_41853758/article/details/82930944" target="_blank" rel="noopener">https://blog.csdn.net/qq_41853758/article/details/82930944</a></p><h5 id="摘要">摘要</h5><p>本篇论文研究了最近提出的前景由实例分割，背景由语义分割的全景分割任务。现在的方法大多数都是分开处理这两个问题的，但是在论文中，我们揭示了它们之间存在的潜在关系，实际上，前景物体为帮助背景的理解提供了补充的线索。我们的方法就是AUNet，一种同时包括前景和背景分割两个分支的统一框架。我们在背景分支中添加了RPN和前景分割掩膜来分别提供物体级别和像素级别的注意力。我们的网络可以泛化在各种的骨架模型中，并且拥有着在前景和背景分割中稳定的正确率。我们的网络在COCO和Cityscapes数据集上有着优异的表现。</p><h5 id="引言">引言</h5><p>场景理解是计算领域挑战任务的基础选择，同时在其他诸如自动驾驶和机器人领域也有着非常重要的影响。杰出的应用场景理解的领域包括物体检测，实例分割和语义分割。本篇论文介绍了全景分割，旨在通过实例分割来找到所有的前景物体和通过语义分割解析背景语义信息。基础的算法和COCO挑战的胜者都是通过直接将前景实例分割模型和背景场景解析算法直接结合来解决的，然而这样就忽视了两者之间潜在的关系和没有借鉴前景信息和背景信息之间丰富的语境关系。</p><p>在论文中，我们提出了一种概念上简单的用于全景分割的统一的框架。为了简化前景事物和背景事物之间的信息流，我们结合了卷积化的实例分割和语义分割网络，从而产生一种拥有两个分支的统一网络。这种策略为分割的准确性带来了显著的提供并且提供了运算的效率。这意味着全景分割可以从前景物体和背景语义中的补充信息中受益，同时这也是我们方法的基本原理。</p><p>接着往下，我们探索了结合更高级别视觉线索来提高分割准确性的可能。通过两种分别作用于物体级别和像素级别的注意力机制模型我们实现了该功能。对于第一个模型，我们参考局部生成，每一个局部都表示可能的前景事物，并且调整了其对应区域被认为是前景和背景事物的概率。对于第二个模型，我们使用了前景分割掩膜，并重新定义前景和背景事物的边界。在我们的网络中，这两种模型被称为PAM和MAM，是通过在前景和背景分支之间添加额外的连接实现的。通过MAM，一个叫做RoIUpsample的新层设计用于在固定形状的前景掩膜和对应的特征图中的像素间定义准确的映射函数。事实上，根据观察结果中前景分割的准确率是高的来看，所有的额外连接在由前景分支到背景分支中被丢弃了。更进一步，背景的事物在被前景事物精炼的同时，背景事物也可以通过梯度进行反馈，因此。前景和背景的分割准确率都得到了提高。</p><p>以上就是我们的AUNet，可以很方便的应用于各种网络架构，并且通过端到端的方式进行优化。我们通过两种基础流行的数据集来评估我们的网络，COCO和Citysapce，并就PQ而言，我们的网络取得了最优异的效果。除此之外，通过优化连接也带来了受益且通过消融学习，两种注意力机制模型也证实了其有效性。</p><p>我们研究的最大贡献就是提出了一种简单统一同时包含前景分割和背景分割的框架，并且在COCO和Cityspaces数据集上取得了优异的表现。同时，我们的工作也调查了在前景物体和背景语义信息之间传递的互补信息。全景分割是作为学习这种问题的自然选择，他应用于广泛的视觉任务。我们在这个领域只是进行了初步的探索，我们会沿着这个方向更加努力。</p><p>论文中剩下的部分如下，第二部分中我们简洁的说明了我们的相关工作，第三部分详细阐述了我们的AUNet网络架构，在第四部分的实验之后，我们在第五部分总结了实验结果。</p><h5 id="相关工作">相关工作</h5><p>基于场景理解的传统的深度学习研究经常关注与前景或者背景目标。随着物体检测的高速发展，实例分割使物体定为和分割发展成为了可能。与此同时，全景分割在场景解析上的表现也得到了发展。尽管它们很有效，但是分离的任务引起了实例分割中语境信息的丢失以及独立的语义分割带来了信息的混乱。为了解决这一问题，提出了全景分割，可以同时进行前景和背景的分割任务。</p><h6 id="全景分割">全景分割</h6><p>在前人的论文中，作者给出了一种结合实例和语义分割模型的一种基础算法。后来，在初始化分割结果之上又提出了一种弱监督的方法，并且通过端到端的方式来结合前景和背景线索。然后，这种方法的表现远不如基础算法。和他们不同，我们的端到端的AUNet框架得到了最优异的表现。更进一步，我们在基于生成的实例和基于FCN的语义分割之间建立了联系。</p><h6 id="实例分割">实例分割</h6><p>实例分割旨在判别同种物体的不同实例。可以通过基生成和基于分割的方法来解决。通过区域生成准确的帮助，基于生成的方法取得了较高的表现，包括MNC，FCIS，MASK R-CNN和PANet。</p><h6 id="语义分割">语义分割</h6><p>伴随类似FCN这种编码解码网络的发展，语义分割也取得了飞速的发展。分割中，捕获语境信息发挥了重要的作用。</p><h6 id="注意力模型">注意力模型</h6><p>注意力模型已经被广泛的应用于视觉任务，包括图像处理，视频理解和物体追踪。事实上，SENet通过attention-and-gating机制阐述了通道关系，non-loacl网络将机器翻译的自注意力桥接到使用non-local过滤器的视频分类。在场景理解范围内，增加全局语境信息和通过通道注意操作的特征信息。自注意力和通道注意力被用于处理空间和通道维度中长距离的语境信息。在我们的工作中，我们在全景分割中通过一系列粗糙到精细的注意力块在前景事物和背景事物之间建立了联系。</p><h5 id="AUNet">AUNet</h5><h6 id="问题和参照物">问题和参照物</h6><p>全景分割任务旨在理解单视野内的所有可见事物，这意味着要为图像的每个像素分配一个分割标签和实例ID。为了解决这个问题，当前最好的算法直接结合从分离模型中得到的实例和语义分割的结果，例如Mask R-CNN和PSPNet。</p><p>我们将全景分割问题表述为识别和分割所有前景事物并理解所有背景事物。我们从我们统一网络中的前景分支和背景分支两个方面来解决这个问题。我们给定输入图像X，我们的目标是产生前景事物分割结果Yth和背景事物分割结果Yst，因此，全景分割结果Ypa可以直接使用融合方法从Yth和Yst中产生。全景分割的结果表现通过PQ来评估。我们详细阐述了我们网络中的关键部分即PAM和MAM，我们在最后给出了我们的实现细节。</p><p>在工作中，我们将从分离模型中产生的前景和背景事物方法作为我们的参照。具体来说，基础的方法分别给出了来自分离模型MTh和MSt的事物YTh和事物YSt的结果。前景模型MTh和背景模型MSt为以下统一框架提供了相似的主干网络。</p><h6 id="统一的框架">统一的框架</h6><p>为了桥接前景事物和背景事物，我们提出了AUNet。和基础方法相比，AUNet通过分享相同的主干网络来融合两种模型产生Yth和Yst。如图2所说明，AUNet是概念简单的，FPN被用来作为主干网络来从不同的尺度和分支中提取特征判别。</p><p>和传统的直接融合Mth和Mst结果的方法不同，AUNet通过连接损失函数来进行优化，并简化了网络中的任务。我们采用了一种基于生成的实例切割模型来生成前景分支较好的掩膜M，而对于背景分支，重要的是从分享的多尺度特征中聚合场景信息。通过这种方式，主干网络同时被前景和背景事物监督，推动了两个分支在空间特征上的联系。为了更加明确的在前景事物和背景语义之间建立关系，我们使用了两种源注意力模型。我们考虑i个尺度背景feature map与相应的RPN feature map之间的粗注意力操作，分别用Si和Pi表示。注意力模块可以表示为Si ⊗ Pi ，其中“⊗”表示注意力操作，如图 2 所示。此外，通过处理后的feature map Spam和生成的前景分割掩膜之间的注意力建立更精细的关系，可以表述为Spam ⊗ Proi。 详细信息将在以下部分进行研究。</p><h6 id="注意力引导模型">注意力引导模型</h6><p><strong>PAM</strong></p><p>在传统的两阶段的检测框架中，RPN被用来进行二元标签分类和边界坐标的预测。这意味着RPN features包括只从背景分支的事物注释中获得的丰富背景信息。因此，我们提出了一种新的方法在前景元素和背景语义之间建立互补关系，被称为PAM。如图3所示，我们利用从RPN分支中获得的语境信息来进行注意力操作。我们给出了该过程的细节。给定一个从第i个RPN分支尺度输入的feature map Pi，在sigmoid激活前前景的权重map Mi如下表示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNet1" alt=""></p><p>f函数表示卷积操作，参数表示ReLU激活函数，Mi表示生成的前景权重map，Wi,1和Wi,2表示卷积参数。</p><p>为了加强背景的语义信息，我们将注意力权重map表示为1-sigmoid(Mi)。然后，第i个尺度的激活的feature map S’i表示如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNet2" alt=""></p><p>⊗和⊕分别表示元素累乘和累加，Si,j表示第j层的分割feature map Si</p><p>受同行启发，设计一种简单的背景重新加权方法用来在注意力操作后对无用的背景的层进行权重减少。重新加权feature mapS’'i如下表示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNet3" alt=""></p><p>G和GN表示全局平均池化和群体范数。Ni表示重新加权操作，Wi,3表示卷积参数，S’i,k表示S’i中的第k个像素通道。</p><p>基于 PAM 的上述公式，我们通过注意力操作和背景重加权函数突出共享feature map中的背景区域。它还通过在反向传播期间增强激活的前景区域的权重来方便前景事物的学习。</p><p><strong>MAM</strong></p><p>随着PAM引入语境信息，背景分支被鼓励更多地关注事物的区域。然而，从RPN分支中预测的粗糙的区域缺少足够的线索来对背景进行精确的陈述。和RPN feature不同，从前景分支生成的m×m固定形状掩码可以编码更精细的前景布局。因此我们提出了MAM来进一步模拟这种关系，我们提出了一种新的RoIUpsample层。RoIUpsample层设计的和RoIAlign的过程反转比较相似，如图四所示。在RoIUpsample层中，m×m的掩膜第一次被重塑为和RoIs一样的形状。然后我们利用设计的逆双线性插值计算每个掩码箱中四个规则采样位置的输出特征值，然后将最终结果累加生成掩码feature map。为了满足双线性插值的要求，其中接近点被赋予更多的权重，对于逆双线性插值表示如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNet4" alt=""></p><p>R(pj,k)表示在逆双线性插值之后点Pj,k的结果，R(pg)输入掩码对应值的四分之一，并且规范化的权重valuex和valuey定义如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNet5" alt=""></p><p>其中xp和yp分别表示两个轴上网格点pg和生成的p1,1之间的距离，如图4b所示，注意到公式5和6，也可以使用前向双线性插值从生成的 W’×H’ feature map还原m×m掩码。</p><p>然后，根据RoIs的尺寸将生成的feature map分配到四个不同的尺度，这与FPN中的相似。生成的前景feature map是通过以下操作实现的。</p><p><strong>注意力操作</strong></p><p>和传统的实例分割任务不同，预测的前景掩膜可以利用来为背景分支在像素级别上提供更多的语境导向。我们第一次使用RoIUpsample将它们聚合到Cm*W’*H‘ feature map中。然后，就可以产生1*W’*H’的精细的背景激活区域，这与PAM中的操作相似。随着注意力的引入，前景掩膜也被分割损失函数监督，这意味着在场景理解的能力可以进一步提高。一种相似的背景重新加权函数被采用来聚合有用的背景feature map。我们使用PAM和MAM为前景事物和背景事物提供互补信息。</p><h5 id="实现细节">实现细节</h5><p>作者在本节给出了在训练和推理阶段的相关实现细节。</p><h6 id="训练">训练</h6><p>像在3.2中已经详细阐述的一样，所以提出的方法在统一的框架中都进行了训练。整个网络在训练阶段通过连接的loss函数进行优化，损失函数如下所示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNet6" alt=""></p><p>其中的LRPN，LRCNN，LMASK和LSeg分别代表了RPN，RCNN，实例分割和语义分割的损失函数。具体来说，超参数被用来平衡训练过程，λ1到λ4在COCO数据集上被设为1，1，1，0.3，在Cityspaces数据集上被设为1，0.75，1，1。</p><p>在细节上，我们采用了ResNet-FPN作为我们的主干网络。前景分支的超参数来自Mask R-CNN。主干网络在ImageNet数据集上进行了预训练，剩下的参数来自于参考的另一篇论文。作为标准训练，使用8个GPU来训练所有模型。对于基于ResNet-50和ResNet-101的网络，每个小批量处理的GPU有2个图像，其他每个GPU有1个图像。网络通过使用4e-5的权重衰减和0.9的动量进行迭代的优化。主干网络中的批量归一化是固定的，并且在我们的最终结果中将组归一化添加到所有分支中。对于COCO数据集，在前十三次迭代的学习率为0.02并在第十五次和第十八次的时候将学习率除10。输入的图像在训练时进行水平翻转并重塑短边为600个像素。多尺度测试在最后的结果中被采用。对于Cityspaces数据集来说，初始学习率设置为0.01并在第68次和第88次迭代后分别除10。在将每个图像随机翻转和缩放0.5到2.0倍之后，我们从16个随机512×1024图像裁剪构建mini-batch用于训练。</p><h6 id="推理">推理</h6><p>在参考的论文中，推理阶段的全景分割结果通过融合前景事物和背景事物产生。在该阶段，前景事物的重叠部分通过NMS极大值抑制进行处理，可以通过较高的置信度来预测分割任务。在程序中，分类之间的关系也被考虑在内。无重叠的实例分割和首次指配实例标签的背景事物进行结合来支持前景事物。</p><h5 id="实验">实验</h5><p>作者在COCO和Cityspaces数据集上进行实验并和同行算法相比。简言之就是吹。所以略。</p><h5 id="附加文件">附加文件</h5><h6 id="图1">图1</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu1" alt=""></p><h6 id="图2">图2</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu2" alt=""></p><h6 id="图3">图3</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu3" alt=""></p><h6 id="图4">图4</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu4" alt=""></p><h6 id="图5">图5</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu5" alt=""></p><h6 id="图6">图6</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu6" alt=""></p><h6 id="图7">图7</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNettu7" alt=""></p><h6 id="表1">表1</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNetbiao1" alt=""></p><h6 id="表2">表2</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNetbiao2" alt=""></p><h6 id="表3">表3</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/AUNetbiao3" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UNet编码</title>
      <link href="/2021/10/06/unet-bian-ma/"/>
      <url>/2021/10/06/unet-bian-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="UNet编码">UNet编码</h3><h5 id="UNet模型图">UNet模型图</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unettu1" alt=""></p><h5 id="3-3卷积">3*3卷积</h5><pre class=" language-language-python"><code class="language-language-python">class DoubleConvWithThreeKernnel(torch.nn.Module):    def __init__(self,in_channel,out_channel):        super(DoubleConvWithThreeKernnel,self).__init__()        # 通过序列前后的方式进行两次卷积和激活        self.doubleConv = torch.nn.Sequential(            torch.nn.Conv2d(in_channel,out_channel,padding=1,kernel_size=3),            torch.nn.BatchNorm2d(out_channel),            torch.nn.ReLU(),            torch.nn.Conv2d(out_channel,out_channel,padding=1,kernel_size=3),            torch.nn.BatchNorm2d(out_channel),            torch.nn.ReLU()        )    def forward(self,x):        return self.doubleConv(x)</code></pre><h5 id="最大池化">最大池化</h5><pre class=" language-language-python"><code class="language-language-python">class MaxPooling(torch.nn.Module):    def __init__(self,in_channel,out_channel):        super(MaxPooling,self).__init__()        self.maxPooling = torch.nn.Sequential(            torch.nn.MaxPool2d(kernel_size=2),            # 从第一次池化开始，池化完成后进行两次卷积操作            DoubleConvWithThreeKernnel(in_channel,out_channel)        )    def forward(self,x):        return self.maxPooling(x)</code></pre><h5 id="反卷积">反卷积</h5><pre class=" language-language-python"><code class="language-language-python">class UpConv(torch.nn.Module):    def __init__(self,in_channel,out_channel):        super(UpConv,self).__init__()        # 反卷积        self.up = torch.nn.ConvTranspose2d(in_channel,in_channel//2,kernel_size=2,stride=2)        # 反卷积后两次卷积        self.conv = DoubleConvWithThreeKernnel(in_channel,out_channel)    def forward(self,x2,x1):        # 反卷积后通过收缩层和扩张层的feature map尺寸对浅层的feature map进行pad操作，在与扩展层feature map进行拼接        x1 = self.up(x1)        diffY = x2.size()[2] - x1.size()[2]        diffX = x2.size()[3] - x1.size()[3]        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,                        diffY // 2, diffY - diffY // 2])        x = torch.cat([x2, x1], dim=1)        return self.conv(x)</code></pre><h5 id="1-1卷积输出">1*1卷积输出</h5><pre class=" language-language-python"><code class="language-language-python">class OneMultiOne(torch.nn.Module):    def __init__(self,in_channel,out_channel):        super(OneMultiOne,self).__init__()        self.conv = torch.nn.Conv2d(in_channel,out_channel,padding=0,kernel_size=1)    def forward(self,x):        return self.conv(x)</code></pre><h5 id="模型搭建">模型搭建</h5><pre class=" language-language-python"><code class="language-language-python">class UNet(torch.nn.Module):    def __init__(self,in_channels,out_classes):        super(UNet,self).__init__()        self.in_channels = in_channels        self.out_classes = out_classes        self.conv = DoubleConvWithThreeKernnel(in_channels,64)        self.maxPooling1 = MaxPooling(64, 128)        self.maxPooling2 = MaxPooling(128, 256)        self.maxPooling3 = MaxPooling(256, 512)        self.maxPooling4 = MaxPooling(512, 1024)        self.upConv1 = UpConv(1024, 512)        self.upConv2 = UpConv(512, 256)        self.upConv3 = UpConv(256, 128)        self.upConv4 = UpConv(128, 64)        self.out = OneMultiOne(64,out_classes)    def forward(self,x):        # 首次卷积        x = self.conv(x)        # 四次池化        x1 = self.maxPooling1(x)        x2 = self.maxPooling2(x1)        x3 = self.maxPooling3(x2)        x4 = self.maxPooling4(x3)        # 四次反卷积        x5 = self.upConv1(x3,x4)        x6 = self.upConv2(x2,x5)        x7 = self.upConv3(x1,x6)        x8 = self.upConv4(x,x7)        # 最后输出        return self.out(x8)</code></pre><h5 id="数据增强">数据增强</h5><pre class=" language-language-python"><code class="language-language-python">class ISBI_Loader(Dataset):    def __init__(self, data_path):        # 初始化函数，读取所有data_path下的图片        self.data_path = data_path        self.imgs_path = glob.glob(os.path.join(data_path, 'image/*.png'))    def augment(self, image, flipCode):        # 使用cv2.flip进行数据增强，filpCode为1水平翻转，0垂直翻转，-1水平+垂直翻转        flip = cv2.flip(image, flipCode)        return flip    def __getitem__(self, index):        # 根据index读取图片        image_path = self.imgs_path[index]        # 根据image_path生成label_path        label_path = image_path.replace('image', 'label')        # 读取训练图片和标签图片        image = cv2.imread(image_path)        label = cv2.imread(label_path)        # 将数据转为单通道的图片        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)        label = cv2.cvtColor(label, cv2.COLOR_BGR2GRAY)        image = image.reshape(1, image.shape[0], image.shape[1])        label = label.reshape(1, label.shape[0], label.shape[1])        # 处理标签，将像素值为255的改为1        if label.max() > 1:            label = label / 255        # 随机进行数据增强，为2时不做处理        flipCode = random.choice([-1, 0, 1, 2])        if flipCode != 2:            image = self.augment(image, flipCode)            label = self.augment(label, flipCode)        return image, label    def __len__(self):        # 返回训练集大小        return len(self.imgs_path)</code></pre><h5 id="训练函数">训练函数</h5><pre class=" language-language-python"><code class="language-language-python">def train_net(model,data,device,batch_size,lr,epochs):    # 加载训练集    isbi_dataset = ISBI_Loader(data)    train_loader = Data.DataLoader(dataset=isbi_dataset,batch_size=batch_size,shuffle=True)    # 优化器    optimizer = optim.RMSprop(model.parameters(),weight_decay=1e-8,lr=lr,momentum=0.9)    # loss函数    criterion = torch.nn.BCEWithLogitsLoss()    best_loss = float('inf')    for epoch in range(epochs):        model.train()        for image,label in train_loader:            # 梯度归0            optimizer.zero_grad()            image = image.to(device=device, dtype=torch.float32)            label = label.to(device=device,dtype=torch.float32)            yhat = model(image)            # 计算损失            loss = criterion(yhat,label)            print('loss',loss)            if loss < best_loss:                best_loss = loss                torch.save(model.state_dict(), 'best_model.pth')            # 反向传播            loss.backward()            optimizer.step()</code></pre><h5 id="数据集">数据集</h5><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/data.zip" target="_blank" rel="noopener">https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/data.zip</a></p><h5 id="训练过程及最优模型">训练过程及最优模型</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/rununet.png" alt=""></p><h5 id="测试">测试</h5><pre class=" language-language-python"><code class="language-language-python">    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model = UNet(1,1)    model.to(device=device)    # 加载模型    model.load_state_dict(torch.load('./best.pt', map_location=device))    model.eval()    # 读取所有图片路径    tests = glob.glob('./data/test/3.png')    for test in tests:        # 保存结果地址        save_res_path = test.split('.')[0] + '_res.png'        img = cv2.imread(test)        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)        # 转为batch为1，通道为1，大小为512*512的数组        img = img.reshape(1, 1, img.shape[0], img.shape[1])        img_tensor = torch.from_numpy(img)        img_tensor = img_tensor.to(device=device, dtype=torch.float32)        pred = model(img_tensor)        # 提取结果        pred = np.array(pred.data.cpu()[0])[0]        # 处理结果        pred[pred >= 0.5] = 255        pred[pred < 0.5] = 0        cv2.imwrite(save_res_path, pred)</code></pre><h5 id="测试结果">测试结果</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/trainresult.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>U-Net论文阅读</title>
      <link href="/2021/10/05/u-net-lun-wen-yue-du/"/>
      <url>/2021/10/05/u-net-lun-wen-yue-du/</url>
      
        <content type="html"><![CDATA[<h3 id="U-Net论文阅读">U-Net论文阅读</h3><h5 id="相关知识">相关知识</h5><h6 id="局部感知">局部感知</h6><p>网络部分连通，每个神经元只与上一层的部分神经元相连，只感知局部，而不是整幅图像。</p><h5 id="摘要">摘要</h5><p>默认的成功的深层网络的训练需要上千个被注释的训练样本。在论文中，我们展示了一种网络和一种依赖于数据增强的训练策略来更加有效的使用可获得的注释样本。我们的架构由可以捕获上下文的收缩路径和能够精确定位的扩展路径构成。对于分割电子显微镜下神经元追踪的ISBI挑战，我们展示了我们端到端训练模型的优越性。将同样的模型应用于光学显微镜图像，我们的网络架构仍具有良好的优越性。除此之外，我们的网络的训练速度较快。在最近的GPU上，分割一张512*512的图片的时间不到一秒。</p><h5 id="引言">引言</h5><p>在过去的两年，深度卷积网络在很多视觉识别领域拥有优异的表现。尽管卷积网络已经应用了很久，但是可用训练集的尺寸和网络的尺寸限制了它的性能。Krizhevsky等人使用在拥有1百万训练图像的ImageNet数据集上通过8层的网络和百万的参数进行监督学习从而取得了突破。之后，更多更大和更深的模型出现了。</p><p>传统的卷积网络主要用于分类任务，输出为单一的类型标签。然而，在许多视觉任务中，特别是生物医学的图像处理，人们想要的输出应包括局部感知和每个像素对应的标签分类。而且对于生物医学任务，获取大量的训练图像是不现实的。因此，Ciresan等人通过提供输入像素周围的局部区域并设置到滑动窗口来进行每个像素标签分类的预测。第一，这种网络具有局部感知能力。第二，训练的数据块的数量远大于训练训练图像的数量。这种网络在ISBI 2012上赢得了EM分割挑战。</p><p>很明显的，Ciresan等人的这种策略有两个缺点。第一，在每个块中都要进行训练会消耗大量的时间，且在重叠的快上训练会带来更多的冗余。第二，存在局部标记准确率和上下文使用之间平衡的问题。大的图像块需要更多的最大池化层，而较多的池化层会减少局部标记的准确率，然而很小的图像块只能看到很少的上下文。最近的很多方法提出了一种考虑多种层的特征的输出分类器，可以保证良好的局部标记和上下文感知。</p><p>在论文中，在FCN的架构上，我们构建了一种更优秀的架构。我们修改并扩展了架构来使它使用较少的训练图像并产生更加精确的分割结果。主要的思想是通过连续层来补充平常的收缩网络。我们通过上采样操作来代替池化操作，因此提高了输出层的解析度。为了局部化，将收缩网络中产生的高解析度特征和上采样的输出相结合。连续的卷积层就可以基于此学习聚合更加精确的输出。</p><p>在我们架构中一个比较重要的修改是在上采样部分我们也有大量的特征通道，这意味着网络可以将语义信息传播到更高的解析层。作为结论，扩展层就或多或少的与收缩层相对称，产生了一种U型的架构。这种网络不再使用全连接层，并且只使用卷积中的有效部分。分割的map只包含像素，对于输入的图像可以获得全部的语义信息。通过overlap-tile策略可以对任意大小的图片进行无缝分割。为了预测图像区域边缘的像素，丢失的语义信息可以通过镜像法进行推理。这种tiling策略对于较大的图片是非常重要的，由于GPU显存的原因会导致解析度的限制。</p><p>对于我们的任务只有很少一部分训练数据是可使用的，我们通过对可使用的训练图像进行弹性形变来进行额外的数据增强。这意味着网络可以不通过注释图片资料的转变来学习到形变的不变性。这在生物医学分割领域是非常重要的，形变在此类问题中是非常普遍的转变方式且弹性形变可以有效的模仿。在Dosovitskiy等人的论文中，在非监督学习那块，数据增强用于学习不变性的价值已经被说明了。</p><p>许多细胞分割任务的另一个挑战是对相同细胞的分离，如图3所示。我们提出了一种加权损失的方法，在接触的细胞中分离背景白浅来获得损失函数中较大的权重。</p><p>我们的网络可以应用于各种各样的生物医学的分割问题。在论文中，我们展示了在EM比赛中我们对于神经元结构分割的结果，比Ciresan等人的网络具有更好的表现效果。同时，我们也展示了用于光学显微镜图像的分割结果。</p><h5 id="网络架构">网络架构</h5><p>网络架构如图1所示，它包含了左边的收缩网络和右边的扩展网络。收缩网络使用的是传统的卷积网络的架构。拥有含有两个3*3的卷积操作，ReLu的激活和用于下采样步幅为2的2*2最大池化操作的重复块。在每次下采样中，我们加倍特征通道的数量，扩展网络中每一层都包含2*2卷积的上采样来减半特征通道的数量，紧接着一个和收缩网络对应的feature maps裁剪和3*3卷积以及relu激活函数。由于每层卷积边界像素的丢失，因此裁剪操作是非常重要的。最后的1*1卷积用来将64个元素的特征数组输出为想要的分类数量。整个网络一共含有32个卷积层。</p><p>为了使输出的分割map可以无缝tiling，输入的tile尺寸非常重要，可以使所有的最大池化操作应用于偶数x和y。</p><h5 id="训练">训练</h5><p>通过Caffe框架并使用SGD来训练网络，使用输入图像和对应的输出分割map。由于是为扩展的卷积，因此输出的图像的固定边界宽度比输入的小。为了最小化系统消耗和最大化GPU显存的使用，我们为大的batch选择大的输入tile因此减少了单张图片的batch。我们使用了0.99的高动量，所以先前训练的大量的样本决定了当前优化步骤的更新。</p><p>能量函数是通过最后的feature map使用sofxmax函数计算得到的，以及使用交叉熵计算feature map。softmax函数定义如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unet1" alt=""></p><p>ak(x)表示像素x位置对应的特征通道k的激活，K表示分类的数量以及pk(x)是k对x的分类结果，pk(x)约等于1表示最大激活ak(x)，约等于0表示其他情况。交叉熵对每个位置的pl(x)(X)进行惩罚，公式为</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unet2" alt=""></p><p>l属于1,k是每个像素的真实标签，w是用来让在训练中的一些元素更加重要的权重map。</p><p>我们为每一个真实标签分割预计算权重map来补偿训练集中某个类的像素频率，并强迫网络学习我们在细胞接触之间引入的小分离边界。</p><p>小分离边界是通过形态学操作进行计算的，然后通过以下公式计算权重图。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unet4" alt=""></p><p>wc是平衡类型频率的权重map，d1表示到边界距离最近的细胞，d2表示第边界巨鹿第二近的细胞。在我们的实验中，我们将参数设为10和约5个像素。</p><p>在深度网络中拥有很多卷积层和通过网络的不同路径，好的权重初始化特别重要。否则，即使网络中的某部分没有发挥作用，但仍进行了多余的激活。理想情况下，初始权重应该被调整为使网络中的每个feature map都有近似单元方差。我们架构中的网络通过以固定偏差根号下2/N的高斯分布中移动初始权重来实现的，N表示一个神经元输入点的数量。对于一个3*3卷积和64个feature channels来说，前一层的N为9*64 = 576。</p><h6 id="3-1-数据增强">3.1 数据增强</h6><p>当只有很少的训练样本可用时，对于想要教会网络学会不变性和鲁棒性来说数据增强是基本的。在显微镜图像中，我们最初需要移位和旋转不变性以及形变和灰度值变化的鲁棒性。尤其是训练样本随机的弹性形变对于只有很少的注释图像来训练的分割网络来说是比较关键的。我们在3*3粗网格上使用随机位移向量来生成平滑形变。这些位置从具有10个像素偏差的高斯分布中采样。然后使用双三次插值计算像素位移。收缩路径末端的丢弃层来进一步隐式数据增强。</p><h5 id="实验">实验</h5><p>我们演示了 u-net 在三种不同的分割任务中的应用。第一项任务是在电子显微镜记录中分割神经元结构。数据集和我们获得的分割示例如图 2 所示。我们提供完整结果作为补充材料。该数据集由EM分割挑战提供，该挑战始于 2012 年 ISBI，并且仍然对新的参与者开放。训练数据是一组来自果蝇一龄幼虫腹神经索 (VNC) 的串行切片透射电子显微镜的 30 张图像。每个图像都带有相应的完全注释的细胞（白色）和膜（黑色）的真实标签分割图。测试集是公开可用的，但其分割图是保密的。可以通过将预测的膜概率图发送给组织者来获得评估。评估是通过在 10 个不同级别对分割图进行阈值处理并计算“扭曲误差”、“兰德误差”和“像素误差”来完成的。</p><p>u-net在没有任何进一步预处理或后处理的情况下实现了0.0003529和0.0382的随机误差。</p><p>这明显优于 Ciresan 等人的滑动窗口卷积网络结果。其最佳提交的扭曲误差为0.000420，兰特误差为0.0504。就兰特误差而言，这是最好的表现算法通过使用数据集特定的后处理方法来应用于 Ciresan 等人的概率图。</p><p>我们还将 u-net 应用于光显微图像中的细胞分割任务。 该分割任务是 2014 年和 2015 年 ISBI 单元追踪挑战的一部分。第一个数据集“PhC-U373”2 包含通过相差显微镜记录的聚丙烯酰亚胺基质上的胶质母细胞瘤-星形细胞瘤 U373 细胞。它包含 35 个部分注释的训练图像。在这里，我们实现了 92% 的平均 IOU，这明显优于第二好的算法 83%。 第二个数据集“DIC-HeLa”3 是通过微分干涉对比显微镜记录的平板玻璃上的 HeLa 细胞。它包含 20 个部分注释的训练图像。在这里，我们实现了 77.5% 的平均 IOU，这明显优于第二好的算法 46%。</p><h5 id="结论">结论</h5><p>u-net架构在不同的生物医学分割应用中实现了非常好的性能。 由于可以进行弹性形变的数据增强，它只需要很少的注释图像，并且在NVidia Titan GPU上的训练时间只需要10个小时。 我们提供完整的基于Caffe 实现和训练好的网络。 我们相信，u-net架构可以轻松应用于更多的任务。</p><h5 id="附加文件">附加文件</h5><h6 id="图1">图1</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unettu1" alt=""></p><h6 id="图2">图2</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unettu2" alt=""></p><h6 id="图3">图3</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unettu3" alt=""></p><h6 id="图四">图四</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unettu4" alt=""></p><h6 id="表1">表1</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unetbiao1" alt=""></p><h6 id="表2">表2</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/unetbiao2" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读第二部分</title>
      <link href="/2021/10/01/lun-wen-yue-du-di-er-bu-fen/"/>
      <url>/2021/10/01/lun-wen-yue-du-di-er-bu-fen/</url>
      
        <content type="html"><![CDATA[<h3 id="论文阅读第二部分">论文阅读第二部分</h3><h4 id="相关知识">相关知识</h4><h5 id="残差学习">残差学习</h5><p>对于传统的CNN网络，简单的增加网络的深度，容易<strong>导致梯度消失和爆炸</strong>。针对梯度消失和爆炸的解决方法一般是<strong>正则初始化(<strong>normalized initialization</strong>)<strong>和</strong>中间的正则化层(<strong>intermediate normalization layers</strong>)，<strong>但是这会导致另一个问题，<strong>退化问题</strong>，随着网络层数的增加，在</strong>训练集上的准确率却饱和甚至下降</strong>了。这个和过拟合不一样，因为过拟合在训练集上的表现会更加出色。</p><p>按照常理更深层的网络结构的解空间是包括浅层的网络结构的解空间的，也就是说深层的网络结构能够得到更优的解，性能会比浅层网络更佳。但是实际上并非如此，深层网络无论从训练误差或是测试误差来看，都有可能比浅层误差更差，这也证明了并非是由于过拟合的原因。导致这个原因可能是因为<strong>随机梯度下降的策略</strong>，往往解到的并不是全局最优解，而是局部最优解，<strong>由于深层网络的结构更加复杂，所以梯度下降算法得到局部最优解的可能性就会更大</strong>。</p><p>这里提供了一种想法：既然深层网络相比于浅层网络具有退化问题，那么是否可以保留深层网络的深度，又可以有浅层网络的优势去避免退化问题呢？如果将深层网络的后面若干层学习成恒等映射h(x)=x那么模型就退化成浅层网络。但是直接去学习这个恒等映射是很困难的，那么就换一种方式，把网络设计成：</p><p>H(x) = F(x) + x =&gt; F(x) = H(x) - x</p><p>只要F(x) = 0就构成了一个恒等映射H(x) = x，这里F(x)为残差。</p><p><a href="https://zhuanlan.zhihu.com/p/72679537" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72679537</a></p><h6 id="4-3-网络架构的改善">4.3 网络架构的改善</h6><p>网络的潜力还没有完全被挖掘。交叉分解的工作中已经证实feature maps常常折叠为一个小的子集。从这个看法来说，不是每一个feature map都有必要进行下一层的卷积。根据这种现象，我们设计了一种改良型的网络架构来解决这个问题。</p><p><strong>网络架构</strong></p><p>作者提出了一种如图3所示的由块组成的完全开发网络即FE-Net。在该块中，只有feature maps的子集被用来计算，其余的直接传播到下一层卷积来确保信息流动。如下公式：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yiwei8" alt=""></p><p>I和O分别表示输入和输出的feature maps，箭头表示通道拆分，或符合表示通道的合并。在训练中，I1参加计算并且f(I1)的输出被重写回I1的初始内存位置的计算模式可以非常高效的实现。剩下的I2不需要任何的操作。随着卷积层的深度增加，我们可以在计算中混合更多的feature maps。通过这种方式，每一个输入的feature map在最后都可以参与优化，并且可以获得用于预测的多尺度feature maps。我们在实验的5.4中证明了它的有效性。</p><p><strong>基础计算单元</strong></p><p>在论文中，如图四所示。我们采用了倒瓶颈的方式作为基础计算单元来构建我们高效的网络。没有具体的说明，它们的扩充速率默认设置为6，意味着总是使用第一个1*1卷积核将输入通道扩展为6倍。为了结合残差学习的优势，对于图三中每一个计算块，除了最后一个我们主要使用了图四b来作为基础运算单元。对于每个计算块的最后一个计算单元，我们使用图四a来为下一个计算块改变通道数量。图四c用于空间下采样。</p><h5 id="实验">实验</h5><p>在实验中，我们第一次在CIFA10和CIFAR100采用一些消融实验来证实SSL的有效性。在这些实验中，我们证实了只使用一些较少的移位操作就足够提供空间信息交流和构建紧凑的卷积神经网络。然后我们在ILSSVRC-2012数据集上进行实验来证明对于大尺度数据集的泛用性。</p><h6 id="5-1-基准和训练设置">5.1 基准和训练设置</h6><p>CIFAR10和CIFAR100分别是包含10种分类和100种图像分类。都包含了32*32分辨率的5万张训练图像和1万张测试图像。</p><p>在CIFAR的实验中，我们使用了由CSC模型构建的ShiftResNet来估计SSL的能力。CSC模型是由一些夹在1*1用于维度上升卷积和1*1用于维度下降卷积之间的移位层组成。模型中只有移位层是平衡空间信息交换的，通过使用SSL来替代移位层，使用公式4中的可调整超参数λ，我们学习到至少需要多少的移位操作可以维持在ShiftResNet上的表现。</p><p>我们使用默认为6的扩产速率的ShiftResNet-20和ShiftResNet-56来作为消融学习的两个代表，我们使用128的mini-batch和0.1的学习率的两块GPU来进行训练，在三万两千次和四万八千次迭代后，学习率衰减了10倍，并且在六万四千次迭代后训练停止了。在四万八千次迭代后，我们专门停止了SSL训练来修复学习到的移位模式。我们只采用了水平方向的翻转和随机的剪枝来进行数据加强。因为发现L2范数的正则化要好于L1范数，所以我们在接下来的实验中使用了L2范数正则化来进行值的移位。</p><p>ImageNet2019是拥有1.28million张训练图像和5万张验证图像的大规模图像分类数据集。众所周知，在如此大的数据集中使用轻量化的神经网络来训练是很难得到好的表现的。为了更进一步增加使用SSL构建网络的表现能力，我们重新设计了如图三所示的神经网络架构来开发网络的潜力。</p><p>在ImageNet数据集的实验中，我们使用1024的mini-batch，0.00004的权重衰减和0.9的动量来进行训练。使用0.6的学习率来开始训练，在第480次epochs后线性衰减策略停止，在240次epochs后SSL的训练停止。训练的迭代次数与参考的其他同行论文中的次数相当。我们将图像的短边缩放到256并采用224*224的随机裁剪尺寸和水平翻转来加强训练的数据集。为了更进一步丰富训练图像，像在初始训练中的一样提供了更多的失真图像，但是会在最后的一些epochs中撤回。在验证阶段，我们仅裁剪调整为224*224的馈送图像的中心并以单视图的方式呈现结果。</p><h6 id="5-2-消融学习">5.2 消融学习</h6><p>我们从三个相似选项探索SSL的特征，组移位和稀疏移位，深层网络和浅层网络和λ的设置。</p><p><strong>组移位和稀疏移位</strong></p><p>不使用移位惩罚，如表1所示，移位学习的结果要由于在CIFAR10和CIFAR100上的启发式设置。通过不同的任务和不同的数据集，使用移位学习可以使网络来自适应调整位位置和移位操作的方向。使用移位惩罚机制，和原始网络相比也可以保持准确性尽管排除了移位操作的一大部分。即使是排除了超过90%空间上的移位操作，网络仍然可以维持好的表现，这提示我们在图像分类中只有一些移位操作在空间信息交换上发挥了关键性的作用。</p><p><strong>深层网络和浅层网络</strong></p><p>我们分别在深层和浅层网络上使用CIFAR10和CIFAR100数据集来分析SSL的稀疏性，比如ShiftResNet-20和ShiftResNet-56.如表1所示，在ShiftResNet-56上的移位稀疏性要多于ShiftResNet-20.它证实了在ShiftResNet-56上即使是超过95%的稀疏也可以有很好的表现。深度的增加带来了移位层上更多的冗余。</p><p><strong>λ的不同设置</strong></p><p>我们将λ从0增加到5e-4，尽管网络的准确率下降了一点，但是大部分的移位操作被排除了。λ为0的SSL相当于量化感知中的主动移位。当我们显著的增加λ的值时，我们将所有的位移缩小为0，意味着有1*1卷积组成的基础模型只有三个池化层在提供空间信息交换。在这个例子中，正确率下降较多，反映了在空间信息交换中，如此少的移位却发挥了很大的作用。拿CIFAR100数据集并使用ShiftResNet-56作为例子，仅使用3.9%的feature maps位移，准确率可以从56.1%提升到69.9%。</p><h6 id="5-3-案例分析">5.3 案例分析</h6><p>我们使用ShiftResNet-20网络在CIFAR10和CIFAR100数据集中使用5e-4的λ值进行更多的细节研究。在表2中，我们展示了每层上空间移位的细节。在某些计算块中，大多数的feature maps没有位移说明这些位置的移位层是不重要的。实际上，这些移位层可以看做衡量这些层的重要性指标。它可以决定当前的移位层是否重要且是否可以移除而没有正确率的下降。举例来说，在ShiftResNet-20网络上，块2-1的移位层远不如块2-2移位层的重要性。在图5中我们将2-2的移位层进行可视化，虽然大多数的通道没有移位，剩下的可以学习一种有意义的移位模式并提供更多的感受野。这也是移位层比卷积网络层的优势所在。</p><p>为了进一步的分析，我们根据表2中的稀疏性进行移除使用CIFAR10和CIFAR100数据集和ShiftResNet-20网络中最不重要的移位层的实验。如表3所示，当我们逐渐移出不重要的移位层时，准确率只下降了一点。甚至我们只保留块2-2中的一个移位层，准确率仍维持在可接受的程度。</p><h6 id="5-4-在ImageNet上的表现">5.4 在ImageNet上的表现</h6><p>我们重新设计的网络架构在ImageNet2012分类任务上的表现如表4.主要是由使用SSL的FE块组成。我们使用宽度乘数作为超参数来调节正确率和计算消耗之间的平衡。</p><p><strong>和其他同行相比</strong></p><p>如表5所示，使用改进的网络架构，我们在ShiftNet和AS-ResNet上的正确结果大幅度增加。更多的是，在我们的工作之前，浮点数运算和准确率一直由深层可分离卷积构成的网络所支配。我们第一次没有使用深度可分离卷积来构建紧凑网络，但是却达到了比其他使用深度可卷积构建网络的同行所没能达到的高度。就像表5，我们的网络超过了MobileNet和ShuffleNet等一系列网络，这表明SSL可以被当作超过深度可分离卷积的另外一种选择。</p><p>在训练的时候，我们主要和目前由深度可分离构建的紧凑网络的代表MobileNetV2相比，如图6所示，我们的网络在CPU和GPU上达到了最高的准确率且推理的速度显著增加，证实了SSL是一种更加友好的基础部件对于训练应用场景来说。</p><p><strong>FE-Net的消融学习</strong></p><p>我们同时使用了深度可卷积来训练网络来分解SSL和改进的网络之间的优越性。如表6所示，在SSL和基于DW网络的正确率差距随着训练时间的显著增大是差距很小的，这进一步验证了SSL和FE-Net的优越性。</p><h5 id="图片及表格">图片及表格</h5><h6 id="图1">图1</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu1" alt=""></p><h6 id="图2">图2</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu2" alt=""></p><h6 id="图3">图3</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu3" alt=""></p><h6 id="图4">图4</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu4" alt=""></p><h6 id="图5">图5</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu5" alt=""></p><h6 id="图6">图6</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu6" alt=""></p><h6 id="图7">图7</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tu7" alt=""></p><h6 id="表1">表1</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao1" alt=""></p><h6 id="表2">表2</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao2" alt=""></p><h6 id="表3">表3</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao3" alt=""></p><h6 id="表4">表4</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao4" alt=""></p><h6 id="表5">表5</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao5" alt=""></p><h6 id="表6">表6</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao6" alt=""></p><h6 id="表7">表7</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao7" alt=""></p><h6 id="表8">表8</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/biao8" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读第一部分</title>
      <link href="/2021/09/29/lun-wen-yue-du-di-yi-bu-fen/"/>
      <url>/2021/09/29/lun-wen-yue-du-di-yi-bu-fen/</url>
      
        <content type="html"><![CDATA[<h3 id="论文阅读第一部分">论文阅读第一部分</h3><h5 id="使用一些移位操作来设计更加高效的图片分类卷积神经网络">使用一些移位操作来设计更加高效的图片分类卷积神经网络</h5><h5 id="深度学习模型及相关术语">深度学习模型及相关术语</h5><h6 id="量化感知">量化感知</h6><p>我们经常将网络输出的范围在 0.0~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是<strong>量化</strong>。将一张 uint8 类型、数值范围在 0~255 的图片归一成 float32 类型、数值范围在 0.0~1.0 的张量，这个过程就是<strong>反量化</strong>。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lianghua1.png" alt=""></p><p>𝑆 是 scale，表示实数和整数之间的比例关系，𝑍 是 zero point，表示实数中的 0 经过量化后对应的整数。</p><p>这个网络只有三个模块，现在需要把 conv、fc、relu 量化。假设输入为 𝑥，我们可以事先统计样本的最大值和最小值，然后计算出 𝑆𝑥(scale) 和 𝑍𝑥(zero point)。同样地，假设 conv、fc 的参数为 𝑤1、𝑤2，以及 scale 和 zero point 为 𝑆𝑤1、𝑍𝑤1、𝑆𝑤2、𝑍𝑤2。中间层的 feature map 为 𝑎1，𝑎2，并且事先统计出它们的 scale 和 zero point 为 𝑆𝑎1、𝑍𝑎1、𝑆𝑎2、𝑍𝑎2。</p><p>卷积运算和全连接层的本质都是矩阵运算，因此我们可以把卷积运算表示成</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lianghua2.png" alt=""></p><p>根据之前的转换，我们可以得到：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lianghua3.png" alt=""></p><p>其中 𝑀=𝑆𝑤1𝑆𝑥/𝑆𝑎1，得到 conv 的输出后，我们不用反量化回 𝑎1，直接用 𝑞𝑎1 继续后面的计算即可。对于量化的 relu 来说，计算公式不再是 𝑞𝑎2=𝑚𝑎𝑥(𝑞𝑎1,0)，而是 𝑞𝑎2=𝑚𝑎𝑥(𝑞𝑎1,𝑍𝑎1)，并且 𝑆𝑎1=𝑆𝑎2，𝑍𝑎1=𝑍𝑎2 。另外，在实际部署的时候，relu 或者 bn 通常是会整合到 conv 中一起计算。</p><p>得到 𝑞𝑎2 后，我们可以继续来计算 fc 层。假设网络输出为 𝑦，对应的 scale 和 zero point 为 𝑆𝑦、𝑍𝑦，则量化后的 fc 层可以用如下公式计算：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lianghua4.png" alt=""></p><p>然后通过公式 𝑦=𝑆𝑦(𝑞𝑦−𝑍𝑦)把结果反量化回去，就可以得到近似原来全精度模型的输出了。</p><h6 id="消融学习">消融学习</h6><p>为了提升baseline的性能，给它加了两个模块A,B 为了验证A、B两个模块是不是真的都有用，你需要做ablation study。<br>实验1：在baseline的基础上加上模块A，看效果。 实验2：在baseline的基础上加上模块B，看效果。<br>实验3：在baseline的基础上同时加上模块AB，看效果。 然后结果可能是，实验1和实验2的结果都不如实验3，那么说明AB都是有用的；<br>然而也有可能你会发现实验1的结果和实验3一样，甚至更好。这就说明你的想法是有问题的，模块B其实并没有起到作用，提升只来自于模块A。</p><h6 id="剪枝技术">剪枝技术</h6><p><a href="https://blog.csdn.net/jacke121/article/details/79450321?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.no_search_link" target="_blank" rel="noopener">https://blog.csdn.net/jacke121/article/details/79450321?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-5.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-5.no_search_link</a></p><p>相关论文以后阅读。</p><h6 id="知识蒸馏">知识蒸馏</h6><p>识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li>原始模型训练: 训练"Teacher模型", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对"Teacher模型"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li>精简模型训练: 训练"Student模型", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><h6 id="正则化">正则化</h6><p>我们管Rempf(f)叫做<strong>经验风险</strong>，管上面我们思维导图的过程叫做<strong>正则化</strong>，所以顺其自然的管r(d)叫做<strong>正则化项</strong>，然后管Remp(f)+r(d) 叫做<strong>结构风险</strong>，所以顺其自然的<strong>正则化</strong>就是我们将<strong>结构风险</strong>最小化的过程，它们是等价的。</p><p>详细解答https://www.zhihu.com/question/20924039</p><h5 id="摘要">摘要</h5><p>移位操作是除了深度可分离卷积网络之外的另一种可供选择的网络。然而它的实现方式和记忆点是它发展的瓶颈。本篇论文作者引入了一种新的叫做SSL的卷积层来构建高效的卷积神经网络。在这种架构中，最基本的块是由只有一些移位操作并用于中间层feature map的1*1的卷积层构成。作者为了证实这种方法的可行性，介绍了在优化过程中的移位操作惩罚机制和进一步提出了一种量化感知的移位学习方法来使推理过程中的强加学习位移更友好。广泛的消融学习表明只有一些移位操作可以更高效的提供空间信息交流的能力。更进一步为了最大化SSL的能力，作者重新设计了一种改进式的网络架构来完全开发神经网络被限制的潜力，也就是FE-Net。使用SSL，网络可以在ImageNet数据集上达到第一位75%的准确率。就准确率和训练速度而言它超过了其他同行基于深度可分离的卷积构建的网络和NAS的搜索网络。</p><h5 id="引言">引言</h5><p>不多介绍。</p><h5 id="相关工作">相关工作</h5><p>最近几年，就存储，计算和训练推理时间问题提出了越来越多的方法去简化神经网络，同时保证它的强大的表现。我们通过观察是否有预训练的模型将这些方法分为两个部分。</p><h6 id="2-1-压缩神经网络">2.1 压缩神经网络</h6><p>有四种方法将给定的预训练模型简化。剪枝技术为了移除一些不必要的参数来稀疏权重矩阵。张量分解开发通道和权重矩阵的冗余空间来寻求低秩近似。量化对于每个权重参数采用低比特来代替浮点数。知识蒸馏将教师模型简化为学生模型。</p><p>这些方法都可以很高效的压缩神经网络，但是他们的表现都非常依赖预训练模型，没有架构的改善，准确率很难提高。</p><h6 id="2-2-紧凑网络发展">2.2 紧凑网络发展</h6><p>如何设计紧凑的神经元架构师当前的搜索热点话题。一些相关工作使用组卷积来构建紧凑网络。最出名的MobileNet采用了深层可分离卷积来构建准确且轻量化的网络，在领域上迈进了一大步。之后，很多的研究者效仿他们的工作设计出越来越多紧凑强大的架构，例如ShuffleNet，MobileNetV2，ShuffleNetV2，IGCV2。然而，即使深层可分离卷积只需要一点理论上的计算消耗，但由于算数强度太低也很难在训练中高效的实现。</p><p>2018cvpr上的一篇移位论文给出了一种可供选择的方法，移位操作，即移位feature map而不进行计算。通过交错这种移位和逐点卷积来构建紧凑的网络。2019 NIPS的一篇论文提出了一种方法来使移位操作是可以学习的，意味着每一层的感受野是可以自动学习的。由于它是通过内存移动来实现的，因此会占用大量的推理时间。而作者在论文中解决了这个问题。</p><h5 id="背景知识">背景知识</h5><p>移位操作的公式如下所示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yiwei1" alt=""></p><p>I为输入的feature maps，O为输出的feature maps。c为通道索引，i和j表示空间的位置。αc和βc表示相对于输入的第c个feature map的水平和垂直位移。α和β参数的数量和输入feature maps的通道数量一致，和卷积层的参数相比是几乎可以忽视的。</p><h6 id="组位移">组位移</h6><p>同样的在上面提到的cvpr2018论文中，对于移位操作使用了K尺寸的卷积核，输入的feature map最终被分为K^2个组，每组如图一所示指定一个位移方向，方向公式如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/weiyi2" alt=""></p><p>符号为向下取值，然而这种指定不是任务驱动的。每个移位操作的卷积核都是通过大量试错实验来确定好的，位移的均匀分配并不适合所有的任务。</p><h6 id="主动移位">主动移位</h6><p>为了解决这个问题，提出了一种将α和β的整数约束到放宽到实数和放宽移位操作到双线性插值来使α和β可微的方法。第一个公式可以放宽到如下所示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/weiyi3" alt=""></p><p>Ω是由四个相近的整数点组成的(i+αc,j+βc)的邻集。因此，α和β可以通过反向传播的梯度下降优化器来自适应优化。这种移位模式见图1中的b。</p><h5 id="使用少量的移位来设计高效的卷积神经网络">使用少量的移位来设计高效的卷积神经网络</h5><p>cvpr2018的移位论文中通过相关工作证实了在卷积神经网络中移位操作可以为空间信息交流提供感受野。然而，不是所有的feature map都需要移位，多余的移位操作将会带来多余的内存移动和进一步影响神经网络的推理时间。从这个点出发，我们应该设计一个方法来使用较少的移位操作构建高效的卷积神经网络。</p><h6 id="4-1-稀疏移位操作">4.1 稀疏移位操作</h6><p>为了避免无意义的内存移动，我们添加了位置惩罚机制来淘汰损失函数中没有用的移位操作。因为大的位移可以导致有用的信息边界的丢失，所以它还可以避免移位学习的扩散特别是对于低解析度的feature map。我们对α和β添加了L1正则化来淘汰多余的移位，公式如下所示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yiwei3" alt=""></p><p>(x,y)是输入的数据和对应的标签，W表示除了α和β之外的训练参数，f函数是输出标签的预测，L函数是神经网络的损失函数，λ平衡这两项。</p><p>通过这种稀疏性惩罚，我们可以采用最小的内存移动来构建准确且较快的神经网络。我们把它命名为SSL，如图1中的c所示。</p><h6 id="4-2-量化感知移位学习">4.2 量化感知移位学习</h6><p>尽管灵活性和稀疏性使用了，但还有问题没有解决。尽管为了学习移位操作将整数α和β放宽为实数，标准的移位操作在推理时虽然只需要内存移动但是插值仍然需要乘法，在某种程度上它弱化了移位操作的推理优势。</p><p>受量化训练神经网络的启发，我们提出了量化感知移位学习方法来解决这个问题。在方法中，我们在前馈时量化回整数，仍保持移位操作是可学习的。</p><p><strong>前馈</strong></p><p>我们使用α和β的近似整数来恢复移位操作代替插值，如下公式所示</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yiwei4" alt=""></p><p>绝对值代表实数的舍入近似值，这种方式公式3通过量化又转回了公式1，意味着可以使用移位操作而不是插值来计算网络损失。</p><p><strong>反向传播</strong></p><p>和前馈不同，实数移位需要计算他们的梯度并通过SGD优化。根据公式3，α和β的梯度损失如下所示：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yiwei5" alt=""></p><p>w和h是输入feature map的空间尺寸，sign函数根据输入值的标志来确定加一还是减一。</p><p>将损失梯度相对于特征图从高层反向传播到浅层，第三和第五公式都可以计算偏导数。考虑到在前馈过程中我们使用了公式5来代替公式3。所以使用公式5来计算梯度是比较合理且有效的，公式如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yiwei6" alt=""></p><p>和公式5相比这是一种逆内存移动。</p><p><strong>讨论</strong></p><p>在训练后，位置的舍入近似值被保存下来并且在推理时只有移位操作在执行。更多的是，另一个惊人的副作用是这种方法将L1正则化转变为了被截断式的正则化，将向精确0收缩更小的位移。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolov5源码解析一</title>
      <link href="/2021/09/27/yolov5-yuan-ma-jie-xi/"/>
      <url>/2021/09/27/yolov5-yuan-ma-jie-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="yolov5源码解析一">yolov5源码解析一</h3><h5 id="yolov5模型">yolov5模型</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yolov5.png" alt=""></p><p>**（1）输入端：**Mosaic数据增强、自适应锚框计算、自适应图片缩放<br>**（2）Backbone：**Focus结构，CSP结构<br>**（3）Neck：**FPN+PAN结构<br>**（4）Prediction：**GIOU_Loss</p><p><strong>Mosaic数据增强</strong></p><p>随机使用<strong>4张图片</strong>，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。</p><p><strong>自适应锚框计算</strong></p><p>使用自适应的锚框来得到更大的交并比。传统的固定尺寸的anchor对于目标检测时，若物体在尺寸内高度重合，就很难获取输出的特征，而不同尺寸的anchor可以较好的区分不同分类。通过不同的anchor尺寸和与人为标注的标签会产生不同的交并比，通过对交并比的比较可以较为准确的得到所属的分类。</p><p>在Yolo算法中，针对不同的数据集，都会有<strong>初始设定长宽的锚框</strong>。在网络训练中，网络在初始锚框的基础上输出预测框，进而和<strong>真实框groundtruth</strong>进行比对，计算两者差距，再反向更新，<strong>迭代网络参数</strong>。</p><p>Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。</p><p><strong>自适应图片缩放</strong></p><p>一般的算法中都是将不同的图片缩放到统一尺寸，这样的方法可能会导致较大的图片缩放的较小时产生额外的黑边，导致训练的速度变慢。在yolov5中通过自适应的图片的方法尽可能减少图像缩放时产生的黑边，从而加快运算速度。</p><pre class=" language-language-python"><code class="language-language-python"># 以color=(114, 114, 114)灰色进行填充def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):    # Resize and pad image while meeting stride-multiple constraints    shape = im.shape[:2]  # current shape [height, width]    if isinstance(new_shape, int):        new_shape = (new_shape, new_shape)    # 计算缩放比例 缩放尺寸除原始的尺寸获取较小的缩放系数    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])    if not scaleup:  # only scale down, do not scale up (for better val mAP)        r = min(r, 1.0)    # 填充黑边     ratio = r, r  # width, height ratios    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))    # 获取宽和高原本的填充高度     dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding    if auto:  # minimum rectangle        # stride=32 网络中进行了五次下采样 使用填充高度对32进行取余操作        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding    elif scaleFill:  # stretch        dw, dh = 0.0, 0.0        new_unpad = (new_shape[1], new_shape[0])        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios    # 取余后除2 得到对应的宽高的填充量    dw /= 2  # divide padding into 2 sides    dh /= 2    if shape[::-1] != new_unpad:  # resize        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border    return im, ratio, (dw, dh)</code></pre><p><strong>Backbone</strong></p><p><strong>1.Focus结构</strong></p><p>对图像进行切片操作后，使用32个卷积核对切片进行卷积</p><p><strong>2.CSP结构</strong></p><p>每个CSP模块前面的卷积核的大小都是3<em>3，stride=2，因此可以起到下采样的作用。因为Backbone有5个<strong>CSP模块</strong>，输入图像是<strong>608*608</strong>，所以特征图变化的规律是：<strong>608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19</strong>。经过5次CSP模块后得到19</em>19大小的特征图。</p><p>CSPNet的作者认为推理计算过高的问题是由于网络优化中的<strong>梯度信息重复</strong>导致的。因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。</p><p>CSP的模型图</p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/csp.png" target="_blank" rel="noopener">https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/csp.png</a></p><p><strong>3.SPP结构</strong></p><p>使用k={1*1,5*5,9*9,13*13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。最大池化采用<strong>padding操作</strong>，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，<strong>padding=2</strong>，因此池化后的特征图仍然是13×13大小。</p><p><strong>Neck</strong></p><p><strong>1.FPN结构</strong></p><p>对于FPN的理解来自于https://blog.csdn.net/WZZ18191171661/article/details/79494534，我觉得讲的很好</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fpn.png" alt=""></p><p>识别不同大小的物体是计算机视觉中的一个基本挑战，我们常用的解决方案是构造多尺度金字塔。如上图a所示，这是一个特征图像金字塔，整个过程是先对原始图像构造图像金字塔，然后在图像金字塔的每一层提出不同的特征，然后进行相应的预测（BB的位置）。这种方法的缺点是计算量大，需要大量的内存；优点是可以获得较好的检测精度。它通常会成为整个算法的性能瓶颈，由于这些原因，当前很少使用这种算法。如上图b所示，这是一种改进的思路，学者们发现我们可以利用卷积网络本身的特性，即对原始图像进行卷积和池化操作，通过这种操作我们可以获得不同尺寸的feature map，这样其实就类似于在图像的特征空间中构造金字塔。实验表明，浅层的网络更关注于细节信息，高层的网络更关注于语义信息，而高层的语义信息能够帮助我们准确的检测出目标，因此我们可以利用最后一个卷积层上的feature map来进行预测。这种方法存在于大多数深度网络中，比如VGG、ResNet、Inception，它们都是利用深度网络的最后一层特征来进行分类。这种方法的优点是速度快、需要内存少。它的缺点是我们仅仅关注深层网络中最后一层的特征，却忽略了其它层的特征，但是细节信息可以在一定程度上提升检测的精度。因此有了图c所示的架构，它的设计思想就是同时利用低层特征和高层特征，分别在不同的层同时进行预测，这是因为我的一幅图像中可能具有多个不同大小的目标，区分不同的目标可能需要不同的特征，对于简单的目标我们仅仅需要浅层的特征就可以检测到它，对于复杂的目标我们就需要利用复杂的特征来检测它。整个过程就是首先在原始图像上面进行深度卷积，然后分别在不同的特征层上面进行预测。它的优点是在不同的层上面输出对应的目标，不需要经过所有的层才输出对应的目标（即对于有些目标来说，不需要进行多余的前向操作），这样可以在一定程度上对网络进行加速操作，同时可以提高算法的检测性能。它的缺点是获得的特征不鲁棒，都是一些弱特征（因为很多的特征都是从较浅的层获得的）。讲了这么多终于轮到我们的FPN啦，它的架构如图d所示，整个过程如下所示，首先我们在输入的图像上进行深度卷积，然后对Layer2上面的特征进行降维操作（即添加一层1x1的卷积层），对Layer4上面的特征就行上采样操作，使得它们具有相应的尺寸，然后对处理后的Layer2和处理后的Layer4执行加法操作（对应元素相加），将获得的结果输入到Layer5中去。其背后的思路是为了获得一个强语义信息，这样可以提高检测性能。认真的你可能观察到了，这次我们使用了更深的层来构造特征金字塔，这样做是为了使用更加鲁棒的信息；除此之外，我们将处理过的低层特征和处理过的高层特征进行累加，这样做的目的是因为低层特征可以提供更加准确的位置信息，而多次的降采样和上采样操作使得深层网络的定位信息存在误差，因此我们将其结合其起来使用，这样我们就构建了一个更深的特征金字塔，融合了多层特征信息，并在不同的特征进行输出。这就是上图的详细解释。</p><p><strong>2.PAN结构</strong></p><p>见以后的分割算法详解。</p><p><strong>LOSS</strong></p><p><strong>1.IOU结构</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/IOU.png" alt=""></p><p>计算预测框和实际框的交集，存在无交集或交集相同的情况。</p><p><strong>2.GIOU_Loss</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/GIOU.png" alt=""></p><p>使用差集的方式计算，存在差集相同时退化为IOU。</p><p><strong>3.DIOU_Loss</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/DIOU.png" alt=""></p><p>DIOU_Loss考虑了<strong>重叠面积</strong>和<strong>中心点距离</strong>，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。存在距离相同的情况。</p><p><strong>4.CIOU_Loss</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/CIOU.png" alt=""></p><p>CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了</p><h5 id="python模块">python模块</h5><h6 id="argparse">argparse</h6><p>argarse用于通过命令行向程序中传参运行</p><pre class=" language-language-python"><code class="language-language-python">import argparseparser = argparse.ArgumentParser()#type是要传入的参数的数据类型  help是该参数的提示信息parser.add_argument('integers', type=str, help='传入的数字')args = parser.parse_args()#获得传入的参数print(args)</code></pre><p>yolov5中使用该模块获取用户传入的参数信息，如果该用户未传入该参数，则使用默认值</p><pre class=" language-language-python"><code class="language-language-python">    parser = argparse.ArgumentParser()    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='dataset.yaml path')    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')    parser.add_argument('--epochs', type=int, default=300)    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')    parser.add_argument('--rect', action='store_true', help='rectangular training')    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')    parser.add_argument('--noval', action='store_true', help='only validate final epoch')    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in "ram" (default) or "disk"')    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')    parser.add_argument('--project', default='runs/train', help='save to project/name')    parser.add_argument('--entity', default=None, help='W&B entity')    parser.add_argument('--name', default='exp', help='save to project/name')    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')    parser.add_argument('--quad', action='store_true', help='quad dataloader')    parser.add_argument('--linear-lr', action='store_true', help='linear LR')    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')    parser.add_argument('--save_period', type=int, default=-1, help='Log model after every "save_period" epoch')    parser.add_argument('--artifact_alias', type=str, default="latest", help='version of dataset artifact to be used')    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')    opt = parser.parse_known_args()[0] if known else parser.parse_args()</code></pre><p>参数说明</p><pre class=" language-language-python"><code class="language-language-python">weights：权重文件路径cfg：存储模型结构的配置文件data：存储训练、测试数据的文件hyp:超参数，包括lr、weight_decay、momentum和图像处理的参数等epochs：指的就是训练过程中整个数据集将被迭代多少次batch-size：一次看完多少张图片才进行权重更新，梯度下降的mini-batchimg-size：输入图片宽高rect：进行矩形训练resume：恢复最近保存的模型开始训练nosave：仅保存最终checkpointnoval：是否只验证最后的epochnoautoanchor：用于设置在目标检测任务中是否采用锚点 / 锚框evolve：进化超参数，寻找最优超参数的方式，方法是利用遗传算法自动搜索超参数。bucket：gsutil bucketcache-images：缓存图像以加快训练速度image-weights：生效后对于那些训练不好的图片，会在下一轮中增加一些权重。device：cuda device, i.e. 0 or 0,1,2,3 or cpu，训练设备multi-scale：多尺度训练，img-size +/- 50%single-cls：单类别的训练集adam：使用adam优化sync-bn：DDP模式下是否进行多 GPU 进行分布式训练。workers：线程数量project：保存训练结果的路径entity：wandb 库对应的东西name：训练结果的文件夹名称exist-ok：当不激活时为 false，在新命名的文件夹下保存。当激活时为 true，在 name 指定文件夹下保存，源码中保存在 exp 文件夹下quad：生效后可以在比前面 “--img-size” 部分设置的训练测试数据集更大的数据集上训练。linear-lr：用于对学习速率进行调整label-smoothing：对标签进行平滑处理upload_dataset：wandb 库对应的参数bbox_interval：wandb 库对应的参数save_period：记录训练日志信息artifact_alias：数据集artifact版本local_rank：设备索引freeze：是否冻结加快训练速度patience：训练了多少个epoch，如果模型效果未提升，就让模型提前停止训练</code></pre><h6 id="Wandb">Wandb</h6><p>主要功能</p><ul><li>保存训练运行中使用的超参数</li><li>搜索、比较和可视化训练的运行</li><li>在运行的同时分析系统硬件的情况如：CPU和GPU使用率</li><li>在团队中分享训练数据</li><li>永远保存可用的实验记录</li></ul><pre class=" language-language-python"><code class="language-language-python"># 检查当前文件是否存在，存在返回文件路径，如果是网络文件则进行下载，否则报错该文件不存在def check_file(file, suffix=''):    # Search/download file (if necessary) and return path    check_suffix(file, suffix)  # optional    file = str(file)  # convert to str()    if Path(file).is_file() or file == '':  # exists        return file    elif file.startswith(('http:/', 'https:/')):  # download        url = str(Path(file)).replace(':/', '://')  # Pathlib turns :// -> :/        file = Path(urllib.parse.unquote(file)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth        print(f'Downloading {url} to {file}...')        torch.hub.download_url_to_file(url, file)        assert Path(file).exists() and Path(file).stat().st_size > 0, f'File download failed: {url}'  # check        return file    else:  # search        files = glob.glob('./**/' + file, recursive=True)  # find file        assert len(files), f'File not found: {file}'  # assert file was found        assert len(files) == 1, f"Multiple files match '{file}', specify exact path: {files}"  # assert unique        return files[0]  # return file</code></pre><pre class=" language-language-python"><code class="language-language-python">def process_wandb_config_ddp_mode(opt):    # 获取参数中的data训练数据的文件位置并读取    with open(check_file(opt.data), errors='ignore') as f:        data_dict = yaml.safe_load(f)  # data dict    train_dir, val_dir = None, None    # 判断当前的数据存放位置是否为wandb的云端文件，如果为云端文件则进行下载并返回数据所在路径    if isinstance(data_dict['train'], str) and data_dict['train'].startswith(WANDB_ARTIFACT_PREFIX):        api = wandb.Api()        train_artifact = api.artifact(remove_prefix(data_dict['train']) + ':' + opt.artifact_alias)        train_dir = train_artifact.download()        train_path = Path(train_dir) / 'data/images/'        data_dict['train'] = str(train_path)    if isinstance(data_dict['val'], str) and data_dict['val'].startswith(WANDB_ARTIFACT_PREFIX):        api = wandb.Api()        val_artifact = api.artifact(remove_prefix(data_dict['val']) + ':' + opt.artifact_alias)        val_dir = val_artifact.download()        val_path = Path(val_dir) / 'data/images/'        data_dict['val'] = str(val_path)    if train_dir or val_dir:        ddp_data_path = str(Path(val_dir) / 'wandb_local_data.yaml')        with open(ddp_data_path, 'w') as f:            yaml.safe_dump(data_dict, f)        opt.data = ddp_data_path</code></pre><pre class=" language-language-python"><code class="language-language-python">def check_wandb_resume(opt):    # 检查opt中的参数data路径是否为云端路径    process_wandb_config_ddp_mode(opt) if RANK not in [-1, 0] else None    # isinstance 类型判断    if isinstance(opt.resume, str):        if opt.resume.startswith(WANDB_ARTIFACT_PREFIX):            if RANK not in [-1, 0]:  # For resuming DDP runs                entity, project, run_id, model_artifact_name = get_run_info(opt.resume)                api = wandb.Api()                artifact = api.artifact(entity + '/' + project + '/' + model_artifact_name + ':latest')                modeldir = artifact.download()                opt.weights = str(Path(modeldir) / "last.pt")            return True    return None</code></pre><h6 id="EMA">EMA</h6><p>指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。</p><p>在深度学习的优化过程中，θt是t时刻的模型权重weights，vt是t时刻的影子权重（shadow weights）。在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重并不会参与训练。基本的假设是，模型权重在最后的n步内，会在实际的最优点处抖动，所以我们取最后n步的平均，能使得模型更加的鲁棒。</p><h6 id="瞄点">瞄点</h6><pre class=" language-language-python"><code class="language-language-python">def check_anchors(dataset, model, thr=4.0, imgsz=640):    # Check anchor fit to data, recompute if necessary    prefix = colorstr('autoanchor: ')    print(f'\n{prefix}Analyzing anchors... ', end='')    m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))  # augment scale    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)])).float()  # wh    def metric(k):  # compute metric        r = wh[:, None] / k[None]        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric        best = x.max(1)[0]  # best_x        aat = (x > 1. / thr).float().sum(1).mean()  # anchors above threshold        bpr = (best > 1. / thr).float().mean()  # best possible recall        return bpr, aat    # h/h_a, w/w_a都要在(1/hyp['anchor_t'], hyp['anchor_t'])    anchors = m.anchor_grid.clone().cpu().view(-1, 2)  # current anchors    bpr, aat = metric(anchors)    print(f'anchors/target = {aat:.2f}, Best Possible Recall (BPR) = {bpr:.4f}', end='')    # 如果标签框满足上面条件的数量小于总数的98%，则根据k-mean算法聚类新的锚点anchor    if bpr < 0.98:  # threshold to recompute        print('. Attempting to improve anchors, please wait...')        na = m.anchor_grid.numel() // 2  # number of anchors        try:            anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)        except Exception as e:            print(f'{prefix}ERROR: {e}')        new_bpr = metric(anchors)[0]        if new_bpr > bpr:  # replace anchors            anchors = torch.tensor(anchors, device=m.anchors.device).type_as(m.anchors)            m.anchor_grid[:] = anchors.clone().view_as(m.anchor_grid)  # for inference            m.anchors[:] = anchors.clone().view_as(m.anchors) / m.stride.to(m.anchors.device).view(-1, 1, 1)  # loss            check_anchor_order(m)            print(f'{prefix}New anchors saved to model. Update model *.yaml to use these anchors in the future.')        else:            print(f'{prefix}Original anchors better than new anchors. Proceeding with original anchors.')    print('')  # newline</code></pre><h6 id="k-mean算法"><strong>k-mean算法</strong></h6><p>所以 K-means 的算法步骤为：</p><ol><li>选择初始化的 k 个样本作为初始聚类中心 a = a1,a2 … ak</li><li>针对数据集中每个样本xi计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；</li><li>针对每个类别 ，重新计算它的聚类中心（即属于该类的所有样本的质心）；</li><li>重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。</li></ol><pre class=" language-language-python"><code class="language-language-python">def kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=1000, verbose=True):    """ Creates kmeans-evolved anchors from training dataset        Arguments:            dataset: path to data.yaml, or a loaded dataset            n: number of anchors            img_size: image size used for training            thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0            gen: generations to evolve anchors using genetic algorithm            verbose: print all results        Return:            k: kmeans evolved anchors        Usage:            from utils.autoanchor import *; _ = kmean_anchors()    """    from scipy.cluster.vq import kmeans    thr = 1. / thr    prefix = colorstr('autoanchor: ')    def metric(k, wh):  # compute metrics        r = wh[:, None] / k[None]        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric        # x = wh_iou(wh, torch.tensor(k))  # iou metric        return x, x.max(1)[0]  # x, best_x    def anchor_fitness(k):  # mutation fitness        _, best = metric(torch.tensor(k, dtype=torch.float32), wh)        return (best * (best > thr).float()).mean()  # fitness    def print_results(k):        k = k[np.argsort(k.prod(1))]  # sort small to large        x, best = metric(k, wh0)        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # best possible recall, anch > thr        print(f'{prefix}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr')        print(f'{prefix}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, '              f'past_thr={x[x > thr].mean():.3f}-mean: ', end='')        for i, x in enumerate(k):            print('%i,%i' % (round(x[0]), round(x[1])), end=',  ' if i < len(k) - 1 else '\n')  # use in *.cfg        return k    if isinstance(dataset, str):  # *.yaml file        with open(dataset, errors='ignore') as f:            data_dict = yaml.safe_load(f)  # model dict        from utils.datasets import LoadImagesAndLabels        dataset = LoadImagesAndLabels(data_dict['train'], augment=True, rect=True)    # Get label wh    shapes = img_size * dataset.shapes / dataset.shapes.max(1, keepdims=True)    wh0 = np.concatenate([l[:, 3:5] * s for s, l in zip(shapes, dataset.labels)])  # wh    # Filter    i = (wh0 < 3.0).any(1).sum()    if i:        print(f'{prefix}WARNING: Extremely small objects found. {i} of {len(wh0)} labels are < 3 pixels in size.')    wh = wh0[(wh0 >= 2.0).any(1)]  # filter > 2 pixels    # wh = wh * (np.random.rand(wh.shape[0], 1) * 0.9 + 0.1)  # multiply by random scale 0-1    # Kmeans calculation    print(f'{prefix}Running kmeans for {n} anchors on {len(wh)} points...')    s = wh.std(0)  # sigmas for whitening    k, dist = kmeans(wh / s, n, iter=30)  # points, mean distance    assert len(k) == n, f'{prefix}ERROR: scipy.cluster.vq.kmeans requested {n} points but returned only {len(k)}'    k *= s    wh = torch.tensor(wh, dtype=torch.float32)  # filtered    wh0 = torch.tensor(wh0, dtype=torch.float32)  # unfiltered    k = print_results(k)    # Plot    # k, d = [None] * 20, [None] * 20    # for i in tqdm(range(1, 21)):    #     k[i-1], d[i-1] = kmeans(wh / s, i)  # points, mean distance    # fig, ax = plt.subplots(1, 2, figsize=(14, 7), tight_layout=True)    # ax = ax.ravel()    # ax[0].plot(np.arange(1, 21), np.array(d) ** 2, marker='.')    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))  # plot wh    # ax[0].hist(wh[wh[:, 0]<100, 0],400)    # ax[1].hist(wh[wh[:, 1]<100, 1],400)    # fig.savefig('wh.png', dpi=200)    # Evolve    npr = np.random    f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma    pbar = tqdm(range(gen), desc=f'{prefix}Evolving anchors with Genetic Algorithm:')  # progress bar    for _ in pbar:        v = np.ones(sh)        while (v == 1).all():  # mutate until a change occurs (prevent duplicates)            v = ((npr.random(sh) < mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)        kg = (k.copy() * v).clip(min=2.0)        fg = anchor_fitness(kg)        if fg > f:            f, k = fg, kg.copy()            pbar.desc = f'{prefix}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}'            if verbose:                print_results(k)    return print_results(k)</code></pre><h6 id="warmup-预热学习率"><strong>warmup 预热学习率</strong></h6><p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p><pre class=" language-language-python"><code class="language-language-python">            # 热身训练(前nw次迭代)            # 在前nw次迭代中，根据以下方式选取accumulate和学习率            if ni <= nw:                xi = [0, nw]  # x interp                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())                for j, x in enumerate(optimizer.param_groups):                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])                    if 'momentum' in x:                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])</code></pre><h6 id="余弦退火"><strong>余弦退火</strong></h6><p>执行完Ti个epoch之后就会开始热重启（warm restart），而下标i就是指的第几次restart，其中重启并不是重头开始，而是通过增加学习率来模拟，并且重启之后使用旧的xt作为初始解，这里的xt就是通过梯度下降求解loss函数的解，也就是神经网络中的权重，因为重启就是为了通过增大学习率来跳过局部最优，所以需要将xt置为旧值。</p><p><img src="https://dingdm.online/fire.png" alt=""></p><pre class=" language-language-python"><code class="language-language-python">def one_cycle(y1=0.0, y2=1.0, steps=100):    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1</code></pre><h5 id="train函数解析">train函数解析</h5><h6 id="resume">resume</h6><pre class=" language-language-python"><code class="language-language-python">  # 参数中设置了resume且resume不是wandb云端模型且未设置超参数进化  if opt.resume and not check_wandb_resume(opt) and not opt.evolve:        # 获取训练产生的last模型并替换opt参数中的参数        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent pat        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'        with open(Path(ckpt).parent.parent / 'opt.yaml') as f:            opt = argparse.Namespace(**yaml.safe_load(f))  # replace        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate        LOGGER.info(f'Resuming training from {ckpt}')    else:        # 否则opt参数直接采集用户的输入信息得到        opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp)  # check YAMLs        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'        if opt.evolve:            opt.project = 'runs/evolve'            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))</code></pre><h6 id="DDP模式单机多卡训练">DDP模式单机多卡训练</h6><pre class=" language-language-python"><code class="language-language-python">    device = select_device(opt.device, batch_size=opt.batch_size)    if LOCAL_RANK != -1:        from datetime import timedelta        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'        assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'        assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'        assert not opt.evolve, '--evolve argument is not compatible with DDP training'        torch.cuda.set_device(LOCAL_RANK)        device = torch.device('cuda', LOCAL_RANK)        dist.init_process_group(backend="nccl" if dist.is_nccl_available() else "gloo")</code></pre><h6 id="train">train</h6><pre class=" language-language-python"><code class="language-language-python">    # 没有设置超参数优化，直接进行训练    if not opt.evolve:        train(opt.hyp, opt, device, callbacks)        if WORLD_SIZE > 1 and RANK == 0:            LOGGER.info('Destroying process group... ')            dist.destroy_process_group()    # Evolve hyperparameters (optional)    else:        # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)        meta = {'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)                'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)                'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1                'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay                'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)                'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum                'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr                'box': (1, 0.02, 0.2),  # box loss gain                'cls': (1, 0.2, 4.0),  # cls loss gain                'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight                'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)                'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight                'iou_t': (0, 0.1, 0.7),  # IoU training threshold                'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold                'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)                'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)                'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)                'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)                'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)                'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)                'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)                'scale': (1, 0.0, 0.9),  # image scale (+/- gain)                'shear': (1, 0.0, 10.0),  # image shear (+/- deg)                'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001                'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)                'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)                'mosaic': (1, 0.0, 1.0),  # image mixup (probability)                'mixup': (1, 0.0, 1.0),  # image mixup (probability)                'copy_paste': (1, 0.0, 1.0)}  # segment copy-paste (probability)        with open(opt.hyp) as f:            hyp = yaml.safe_load(f)  # load hyps dict            if 'anchors' not in hyp:  # anchors commented in hyp.yaml                hyp['anchors'] = 3        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices        evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'        if opt.bucket:            os.system(f'gsutil cp gs://{opt.bucket}/evolve.csv {save_dir}')  # download evolve.csv if exists        for _ in range(opt.evolve):  # generations to evolve            if evolve_csv.exists():  # if evolve.csv exists: select best hyps and mutate                # Select parent(s)                parent = 'single'  # parent selection method: 'single' or 'weighted'                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)                n = min(5, len(x))  # number of previous results to consider                x = x[np.argsort(-fitness(x))][:n]  # top n mutations                w = fitness(x) - fitness(x).min() + 1E-6  # weights (sum > 0)                if parent == 'single' or len(x) == 1:                    # x = x[random.randint(0, n - 1)]  # random selection                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection                elif parent == 'weighted':                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination                # Mutate                mp, s = 0.8, 0.2  # mutation probability, sigma                npr = np.random                npr.seed(int(time.time()))                g = np.array([meta[k][0] for k in hyp.keys()])  # gains 0-1                ng = len(meta)                v = np.ones(ng)                while all(v == 1):  # mutate until a change occurs (prevent duplicates)                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)                    hyp[k] = float(x[i + 7] * v[i])  # mutate            # Constrain to limits            for k, v in meta.items():                hyp[k] = max(hyp[k], v[1])  # lower limit                hyp[k] = min(hyp[k], v[2])  # upper limit                hyp[k] = round(hyp[k], 5)  # significant digits            # Train mutation            results = train(hyp.copy(), opt, device, callbacks)            # Write mutation results            print_mutation(results, hyp.copy(), save_dir, opt.bucket)        # Plot results        plot_evolve(evolve_csv)        print(f'Hyperparameter evolution finished\n'              f"Results saved to {colorstr('bold', save_dir)}\n"              f'Use best hyperparameters example: $ python train.py --hyp {evolve_yaml}')</code></pre><pre class=" language-language-python"><code class="language-language-python">def train(hyp,  # path/to/hyp.yaml or hyp dictionary          opt,          device,          callbacks          ):    # 获取用户参数    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze    # 保存训练完成的模型地址和名称    w = save_dir / 'weights'  # weights dir    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir    last, best = w / 'last.pt', w / 'best.pt'    # 获取超参数    if isinstance(hyp, str):        with open(hyp) as f:            hyp = yaml.safe_load(f)  # load hyps dict    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))    # 持久化训练时的参数设置    with open(save_dir / 'hyp.yaml', 'w') as f:        yaml.safe_dump(hyp, f, sort_keys=False)    with open(save_dir / 'opt.yaml', 'w') as f:        yaml.safe_dump(vars(opt), f, sort_keys=False)    data_dict = None    # 日志    if RANK in [-1, 0]:        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance        if loggers.wandb:            data_dict = loggers.wandb.data_dict            if resume:                weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp        # Register actions        for k in methods(loggers):            callbacks.register_action(k, callback=getattr(loggers, k))    # Config    plots = not evolve  # create plots    cuda = device.type != 'cpu'    init_seeds(1 + RANK)    # 判断提供的数据配是否为空    with torch_distributed_zero_first(RANK):        data_dict = data_dict or check_dataset(data)  # check if None    # 更新数据地址    train_path, val_path = data_dict['train'], data_dict['val']    # 用户是否选择某单个类别并更新，不是则获取默认的nc数量    nc = 1 if single_cls else int(data_dict['nc'])  # number of classes    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check    is_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset    # 检查传入的模型权重，本地无则网络下载    check_suffix(weights, '.pt')  # check weights    pretrained = weights.endswith('.pt')    # 传入的为模型数据    if pretrained:        with torch_distributed_zero_first(RANK):            weights = attempt_download(weights)  # download if not found locally        ckpt = torch.load(weights, map_location=device)  # load checkpoint        # 构造模型 模型数据 通道 类别 瞄框        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect        model.load_state_dict(csd, strict=False)  # load        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report    else:        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create    # 冻结参数的迭代次数    freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze    for k, v in model.named_parameters():        v.requires_grad = True  # train all layers        if any(x in k for x in freeze):            print(f'freezing {k}')            v.requires_grad = False    # 构造优化器    # nbs为模拟的batch_size;     # 就比如默认的话上面设置的opt.batch_size为16,这个nbs就为64，    # 也就是模型梯度累积了64/16=4(accumulate)次之后    # 再更新一次模型，变相的扩大了batch_size    nbs = 64  # nominal batch size    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay    LOGGER.info(f"Scaled weight_decay = {hyp['weight_decay']}")    g0, g1, g2 = [], [], []  # optimizer parameter groups    for v in model.modules():        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias            g2.append(v.bias)        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)            g0.append(v.weight)        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)            g1.append(v.weight)# 选择不同的优化器    if opt.adam:        optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum    else:        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)# 添加权重和偏差的优化方式    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay    optimizer.add_param_group({'params': g2})  # add g2 (biases)    LOGGER.info(f"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups "                f"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias")    del g0, g1, g2       if opt.linear_lr:        # 调整学习速率 使用超参数学习调整        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear    else:        # 余弦退火方式进行衰减 lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1        lf = one_cycle(1, hyp['lrf'], epochs)    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)    # EMA    ema = ModelEMA(model) if RANK in [-1, 0] else None    # best_fitness是以[0.0, 0.0, 0.1, 0.9]为系数并乘以[精确度, 召回率, mAP@0.5, mAP@0.5:0.95]再求和所得    # 根据best_fitness来保存best.pt    start_epoch, best_fitness = 0, 0.0    if pretrained:        # 加载优化器        if ckpt['optimizer'] is not None:            optimizer.load_state_dict(ckpt['optimizer'])            best_fitness = ckpt['best_fitness']        # 加载ema        if ema and ckpt.get('ema'):            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())            ema.updates = ckpt['updates']        # 如果新设置epochs小于加载的epoch，        # 则视新设置的epochs为需要再训练的轮次数而不再是总的轮次数        start_epoch = ckpt['epoch'] + 1        if resume:            assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'        if epochs < start_epoch:            LOGGER.info(f"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.")            epochs += ckpt['epoch']  # finetune additional epochs        del ckpt, csd    # Image sizes    gs = max(int(model.stride.max()), 32)  # grid size (max stride)    nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple    # 单机多卡训练    if cuda and RANK == -1 and torch.cuda.device_count() > 1:        logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\n'                        'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')        model = torch.nn.DataParallel(model)    # SyncBatchNorm    if opt.sync_bn and cuda and RANK != -1:        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)        LOGGER.info('Using SyncBatchNorm()')    # 构建train_loader    train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,                                              hyp=hyp, augment=True, cache=opt.cache, rect=opt.rect, rank=RANK,                                              workers=workers, image_weights=opt.image_weights, quad=opt.quad,                                              prefix=colorstr('train: '))    # 判断类别与配置中的最大类别nc进行比价，大于nc则错误    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class    nb = len(train_loader)  # number of batches    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'    # Process 0    if RANK in [-1, 0]:        # 加载测试集        val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,                                       hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,                                       workers=workers, pad=0.5,                                       prefix=colorstr('val: '))[0]        if not resume:            # 获取所有样本类别进行可视化            labels = np.concatenate(dataset.labels, 0)            # c = torch.tensor(labels[:, 0])  # classes            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency            # model._initialize_biases(cf.to(device))            if plots:                plot_labels(labels, names, save_dir)            # Anchors            if not opt.noautoanchor:                # 计算默认锚点anchor与数据集标签框的长宽比值                # 标签的长h宽w与anchor的长h_a宽w_a的比值, 即h/h_a, w/w_a都要在(1/hyp['anchor_t'], hyp['anchor_t'])是可以接受的                # 如果标签框满足上面条件的数量小于总数的98%，则根据k-mean算法聚类新的锚点anchor                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)            model.half().float()  # pre-reduce anchor precision        callbacks.run('on_pretrain_routine_end')    if cuda and RANK != -1:        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)    # 模型参数    hyp['box'] *= 3. / nl  # scale to layers    hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers    hyp['label_smoothing'] = opt.label_smoothing    model.nc = nc  # attach number of classes to model    model.hyp = hyp  # attach hyperparameters to model    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights    model.names = names    # Start training    t0 = time.time()    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training    last_opt_step = -1    # 初始化类别和结果    maps = np.zeros(nc)  # mAP per class    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)      # 设置学习率衰减所进行到的轮次，    # 目的是打断训练后，--resume接着训练也能正常的衔接之前的训练进行学习率衰减    scheduler.last_epoch = start_epoch - 1  # do not move    scaler = amp.GradScaler(enabled=cuda)    stopper = EarlyStopping(patience=opt.patience)    compute_loss = ComputeLoss(model)  # init loss class    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\n'                f'Using {train_loader.num_workers} dataloader workers\n'                f"Logging results to {colorstr('bold', save_dir)}\n"                f'Starting training for {epochs} epochs...')    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------        model.train()        # 如果设置进行图片采样策略，        # 则根据前面初始化的图片采样权重model.class_weights以及maps配合每张图片包含的类别数        # 通过random.choices生成图片索引indices从而进行采样        if opt.image_weights:            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx        mloss = torch.zeros(3, device=device)  # mean losses        if RANK != -1:            train_loader.sampler.set_epoch(epoch)        pbar = enumerate(train_loader)        LOGGER.info(('\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))        if RANK in [-1, 0]:            pbar = tqdm(pbar, total=nb)  # progress bar        optimizer.zero_grad()        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------            ni = i + nb * epoch  # number integrated batches (since train start)            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0            # 热身训练(前nw次迭代)            # 在前nw次迭代中，根据以下方式选取accumulate和学习率            if ni <= nw:                xi = [0, nw]  # x interp                # bias的学习率从0.1下降到基准学习率lr*lf(epoch)，                # 其他的参数学习率从0增加到lr*lf(epoch).                # lf为上面设置的余弦退火的衰减函数                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())                for j, x in enumerate(optimizer.param_groups):                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])                    if 'momentum' in x:                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])            # 多尺度训练 参考上方的FPN            if opt.multi_scale:                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size                sf = sz / max(imgs.shape[2:])  # scale factor                if sf != 1:                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)                    # 插值运算进行上采样                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)            # Forward            with amp.autocast(enabled=cuda):                pred = model(imgs)  # forward                # 正向传播 计算损失                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size                if RANK != -1:                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode                if opt.quad:                    loss *= 4.            # 混精度反向传播            scaler.scale(loss).backward()            # 参数更新            if ni - last_opt_step >= accumulate:                scaler.step(optimizer)  # optimizer.step                scaler.update()                # 梯度归0                optimizer.zero_grad()                if ema:                    ema.update(model)                last_opt_step = ni            # Log            if RANK in [-1, 0]:                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))                callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots, opt.sync_bn)            # end batch ------------------------------------------------------------------------------------------------        # Scheduler        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers        # 学习率衰减        scheduler.step()        if RANK in [-1, 0]:            # 使用ema训练测试数据计算map            callbacks.run('on_train_epoch_end', epoch=epoch)            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop            if not noval or final_epoch:  # Calculate mAP                results, maps, _ = val.run(data_dict,                                           batch_size=batch_size // WORLD_SIZE * 2,                                           imgsz=imgsz,                                           model=ema.ema,                                           single_cls=single_cls,                                           dataloader=val_loader,                                           save_dir=save_dir,                                           save_json=is_coco and final_epoch,                                           verbose=nc < 50 and final_epoch,                                           plots=plots and final_epoch,                                           callbacks=callbacks,                                           compute_loss=compute_loss)            # Update best mAP            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]            if fi > best_fitness:                best_fitness = fi            log_vals = list(mloss) + list(results) + lr            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)            # 保存模型            if (not nosave) or (final_epoch and not evolve):  # if save                ckpt = {'epoch': epoch,                        'best_fitness': best_fitness,                        'model': deepcopy(de_parallel(model)).half(),                        'ema': deepcopy(ema.ema).half(),                        'updates': ema.updates,                        'optimizer': optimizer.state_dict(),                        'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None}                # Save last, best and delete                torch.save(ckpt, last)                if best_fitness == fi:                    torch.save(ckpt, best)                del ckpt                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)            # Stop Single-GPU            if RANK == -1 and stopper(epoch=epoch, fitness=fi):                break            # Stop DDP TODO: known issues shttps://github.com/ultralytics/yolov5/pull/4576            # stop = stopper(epoch=epoch, fitness=fi)            # if RANK == 0:            #    dist.broadcast_object_list([stop], 0)  # broadcast 'stop' to all ranks        # Stop DPP        # with torch_distributed_zero_first(RANK):        # if stop:        #    break  # must break all DDP ranks        # end epoch ----------------------------------------------------------------------------------------------------    # end training -----------------------------------------------------------------------------------------------------    if RANK in [-1, 0]:        LOGGER.info(f'\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')        if not evolve:            # coco数据集            if is_coco:                 for m in [last, best] if best.exists() else [last]:  # speed, mAP tests                    results, _, _ = val.run(data_dict,                                            batch_size=batch_size // WORLD_SIZE * 2,                                            imgsz=imgsz,                                            model=attempt_load(m, device).half(),                                            iou_thres=0.7,  # NMS IoU threshold for best pycocotools results                                            single_cls=single_cls,                                            dataloader=val_loader,                                            save_dir=save_dir,                                            save_json=True,                                            plots=False)            # Strip optimizers            for f in last, best:                if f.exists():                    strip_optimizer(f)  # strip optimizers        callbacks.run('on_train_end', last, best, plots, epoch)        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}")    torch.cuda.empty_cache()    return results</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人类库工具文档(持续更新)</title>
      <link href="/2021/09/21/ge-ren-lei-ku-gong-ju-wen-dang/"/>
      <url>/2021/09/21/ge-ren-lei-ku-gong-ju-wen-dang/</url>
      
        <content type="html"><![CDATA[<h3 id="个人类库工具文档-持续更新">个人类库工具文档(持续更新)</h3><h4 id="文件操作类-FileUtils">文件操作类(FileUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>FileUtils.ImgUpload()</td><td>文件上传工具类</td><td>file</td><td>MultipartFile文件类</td><td>Map</td><td>返回Map类型数据，文件uuid名称(uName),封装文件路径(url),文件原名称(filename),文件后缀(suffix),文件类型(type,1为图片,2为文档,3为视频,4为音频)。</td></tr><tr><td></td><td></td><td>path</td><td>文件存储路径</td><td></td><td></td></tr><tr><td></td><td></td><td>mvc</td><td>文件映射路径</td><td></td><td></td></tr><tr><td>FileUtils.getFileType()</td><td>获取文件类型</td><td>suffix</td><td>文件后缀</td><td>String</td><td>返回文件类型</td></tr></tbody></table><h4 id="日期操作类-DateUtils">日期操作类(DateUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>DateUtils.format()</td><td>日期格式化 日期格式为：yyyy-MM-dd</td><td>Date</td><td>日期</td><td>String</td><td>返回yyyy-MM-dd格式日期</td></tr><tr><td></td><td></td><td>String</td><td>格式，如：DateUtils.DATE_TIME_PATTERN</td><td></td><td></td></tr><tr><td>DateUtils.stringToDate()</td><td>日期解析</td><td>String</td><td>日期</td><td>Date</td><td>返回Date</td></tr><tr><td></td><td></td><td>String</td><td>格式，如：DateUtils.DATE_TIME_PATTERN</td><td></td><td></td></tr><tr><td>DateUtils.getWeekStartAndEnd()</td><td>根据周数，获取开始日期、结束日期</td><td>int</td><td>周期  0本周，-1上周，-2上上周，1下周，2下下周</td><td>Date数组</td><td>返回date[0]开始日期、date[1]结束日期</td></tr><tr><td>DateUtils.addDateSeconds()</td><td>对日期的【秒】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>加/减几秒后的日期</td></tr><tr><td></td><td></td><td>int</td><td>秒数，负数为减</td><td></td><td></td></tr><tr><td>DateUtils.addDateMinutes()</td><td>对日期的【分钟】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>加/减几分钟后的日期</td></tr><tr><td></td><td></td><td>int</td><td>分钟数，负数为减</td><td></td><td></td></tr><tr><td>DateUtils.addDateHours()</td><td>对日期的【小时】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>加/减几小时后的日期</td></tr><tr><td></td><td></td><td>int</td><td>小时数，负数为减</td><td></td><td></td></tr><tr><td>DateUtils.addDateDays()</td><td>对日期的【天】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>加/减几天后的日期</td></tr><tr><td></td><td></td><td>int</td><td>天数，负数为减</td><td></td><td></td></tr><tr><td>DateUtils.addDateWeeks()</td><td>对日期的【周】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>加/减几周后的日期</td></tr><tr><td></td><td></td><td>int</td><td>加/减几周后的日期</td><td></td><td></td></tr><tr><td>DateUtils.addDateMonths()</td><td>对日期的【月】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>对日期的【月】进行加/减</td></tr><tr><td></td><td></td><td>int</td><td>加/减几月后的日期</td><td></td><td></td></tr><tr><td>DateUtils.addDateYears()</td><td>对日期的【年】进行加/减</td><td>Date</td><td>日期</td><td>Date</td><td>加/减几年后的日期</td></tr><tr><td></td><td></td><td>int</td><td>年数，负数为减</td><td></td><td></td></tr></tbody></table><h4 id="Word导出类-WordUtils">Word导出类(WordUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>WordUtils.exportWord()</td><td>获取word文件下载地址</td><td>Map</td><td>封装的需要的Map参数</td><td>String</td><td>word文件下载地址</td></tr><tr><td></td><td></td><td>String</td><td>word模板文件所在地址</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>word文件导出地址</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>word模板名称</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>文件映射路径</td><td></td><td></td></tr></tbody></table><h4 id="Spring容器类-SpringContextUtils">Spring容器类(SpringContextUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>SpringContextUtils.getBean()</td><td>获取java bean</td><td>Class<t></t></td><td>java类</td><td><t></t></td><td>返回java bean</td></tr><tr><td>SpringContextUtils.getBean()</td><td>获取java bean</td><td>String</td><td>类名称</td><td><t></t></td><td>返回java bean</td></tr><tr><td></td><td></td><td>Class<t></t></td><td>java类</td><td></td><td></td></tr><tr><td>SpringContextUtils.containsBean()</td><td>是否包含java bean</td><td>String</td><td>类名称</td><td>boolean</td><td>是否包含</td></tr><tr><td>SpringContextUtils.isSingleton()</td><td>bean是否为单例</td><td>String</td><td>类名称</td><td>boolean</td><td>是否为单例</td></tr><tr><td>SpringContextUtils.getType()</td><td>获取java类型</td><td>String</td><td>类名称</td><td>Class&lt;? extends Object&gt;</td><td>java类型</td></tr></tbody></table><h4 id="Excel工具类-ExcelUtils">Excel工具类(ExcelUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>ExcelUtils.exportExcel()</td><td>Excel导出</td><td>HttpServletResponse</td><td>response</td><td>void</td><td></td></tr><tr><td></td><td></td><td>String</td><td>文件名</td><td></td><td></td></tr><tr><td></td><td></td><td>Collection&lt;?&gt;</td><td>数据List</td><td></td><td></td></tr><tr><td></td><td></td><td>Class&lt;?&gt;</td><td>对象Class</td><td></td><td></td></tr><tr><td>ExcelUtils.getMyCellDate()</td><td>excel日期转换</td><td>Cell</td><td>获取excel的日期列</td><td>Date</td><td>日期</td></tr></tbody></table><h4 id="个人邮件类-MailUtils">个人邮件类(MailUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>new Thread(new MailUtils(email,sendEmail,password,host,html,title,qq)).start();</td><td>创建线程发送邮件</td><td>String</td><td>接收人邮箱</td><td>void</td><td>控制台打印信息</td></tr><tr><td></td><td></td><td>String</td><td>发送人邮箱</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>发送人邮箱授权码</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td><a href="http://xn--smtp-zh5ft2bu2us9p2xpj2jcm6bd61dj3b.qq.com" target="_blank" rel="noopener">指定发送邮件的主机smtp.qq.com</a>(QQ)|<a href="http://smtp.163.com" target="_blank" rel="noopener">smtp.163.com</a>(网易)</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>邮件html内容</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>邮件标题</td><td></td><td></td></tr><tr><td></td><td></td><td>boolean</td><td>是否为qq邮箱</td><td></td><td></td></tr></tbody></table><h4 id="MessageUtils工具类">MessageUtils工具类</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>SnowFlake.nextId()</td><td>获取雪花id</td><td>无</td><td>无</td><td>Long</td><td>雪花id</td></tr></tbody></table><h4 id="Ip工具类-IpUtils">Ip工具类(IpUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>IpUtils.getIpAddr()</td><td>获取ip地址</td><td>HttpServletRequest</td><td>request请求对象</td><td>String</td><td>ip地址</td></tr></tbody></table><h4 id="加密工具类-MD5Utils">加密工具类(MD5Utils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>MD5Utils.MD5()</td><td>md5加密</td><td>String</td><td>加密内容</td><td>String</td><td>加密结果</td></tr></tbody></table><h4 id="类型转换工具类-TransformUtils-XmlUtils">类型转换工具类(TransformUtils,XmlUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>TransformUtils.mapToXml()</td><td>将Map转换为XML格式的字符串</td><td>Map&lt;String, String&gt;</td><td>原数据</td><td>String</td><td>返回xml</td></tr><tr><td>TransformUtils.xmlToMap()</td><td>XML格式字符串转换为Map</td><td>String</td><td>xml数据</td><td>Map&lt;String, String&gt;</td><td>Map数据</td></tr></tbody></table><h4 id="支付工具类-AliPayUtils-WxPayUtils">支付工具类(AliPayUtils,WxPayUtils)</h4><p>创建代理对象</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>new AliPay()</td><td>创建ali支付参数类</td><td>String</td><td>appId</td><td>AliPay</td><td>Alipay对象</td></tr><tr><td></td><td></td><td>String</td><td>网关</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>商户私钥</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>编码格式</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>支付宝公钥</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>签名方式</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>异步通知页面</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>同步通知页面</td><td></td><td></td></tr></tbody></table><p>封装响应数据</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>AliPayUtils.aliParam()</td><td>构造响应内容</td><td>String</td><td>biz_content，请求参数的集合，最大长度不限，除公共参数外所有请求参数都必须放在这个参数中传递</td><td>String</td><td>响应内容</td></tr><tr><td></td><td></td><td>AliPay</td><td>alipay对象</td><td></td><td></td></tr></tbody></table><p>创建微信参数对象</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>new WxPay()</td><td>创建微信支付参数类</td><td>String</td><td>微信id</td><td>WxPay</td><td>WxPay对象</td></tr><tr><td></td><td></td><td>String</td><td>商户号</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>key</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>异步通知页面</td><td></td><td></td></tr></tbody></table><p>创建微信签名</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>WxPayUtils.createWxParam()</td><td>创建微信签名</td><td>HttpServletRequest</td><td>request请求参数</td><td>String</td><td>微信支付签名</td></tr><tr><td></td><td></td><td>WxPay</td><td>微信请求参数对象</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>body</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>订单编号</td><td></td><td></td></tr><tr><td></td><td></td><td>BigDecimal</td><td>订单总价格</td><td></td><td></td></tr></tbody></table><p>返回微信支付响应数据</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>WxPayUtils.wxParam()</td><td>创建响应内容</td><td>HttpServletRequest</td><td>request请求参数</td><td>String</td><td>支付响应内容</td></tr><tr><td></td><td></td><td>WxPay</td><td>微信请求参数对象</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>微信签名</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>body</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>订单编号</td><td></td><td></td></tr><tr><td></td><td></td><td>BigDecimal</td><td>订单总价格</td><td></td><td></td></tr></tbody></table><h4 id="七牛云文件上传工具类-QiniuUtils">七牛云文件上传工具类(QiniuUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>QiniuUtils.uploadQiniu()</td><td>上传文件到七牛云</td><td>MultipartFile</td><td>@RequestParam(“file”)：文件</td><td>String</td><td>返回的七牛云链接</td></tr><tr><td></td><td></td><td>Map&lt;String,String&gt;</td><td>key说明：QiniuAccessKey：公钥，QiniuSecretKey：私钥，QiniuBucketName：空间名，QiniuDomain：域名，QiniuPrefix：文件分组辨识前缀</td><td></td><td></td></tr></tbody></table><h4 id="阿里云短信发送工具类-SmsUtils">阿里云短信发送工具类(SmsUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>SmsUtils.sendSms()</td><td>阿里云发送短信</td><td>Map&lt;String,String&gt;</td><td>阿里云短信配置map。key说明：time：超时时间，accessKeyId，accessKeySecret：短信个人配置信息，sign：短信签名</td><td>int</td><td>1为成功，0为失败</td></tr><tr><td></td><td></td><td>Map&lt;String,String&gt;</td><td>阿里云短信个人信息map。key说明：mobile：接收手机号，template：短信模板，json：变量替换JSON串,如模板内容为"亲爱的用户,您的验证码为${code}“时,此处的json值为”{“code”:"" + code + “”}"，outId：提供给业务方扩展字段,最终在短信回执消息中将此值带回给调用者</td><td></td><td></td></tr></tbody></table><h4 id="集合第三方qq登录工具类-QQLoginUtils">集合第三方qq登录工具类(QQLoginUtils)</h4><p>创建代理对象</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>new QQHttpClient(String APPID,String APPKEY);</td><td>创建QQ配置对象</td><td>APPID</td><td>qq互联平台应用id</td><td>client</td><td>QQHttpClient对象</td></tr><tr><td></td><td></td><td>APPKEY</td><td>qq互联平台应用key</td><td></td><td></td></tr></tbody></table><p>登录方法</p><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>QQLoginUtils.getBackUrl(String domain, QQHttpClient qqHttpClient);</td><td>获取登录返回链接</td><td>String</td><td>应用配置域名</td><td>Map&lt;String,String&gt;</td><td>key说明：url：返回的链接；state：用于第三方应用防止CSRF攻击的uuid</td></tr><tr><td></td><td></td><td>QQHttpClient</td><td>QQHttpClient配置对象</td><td></td><td></td></tr><tr><td>QQLoginUtils.qqCallBack(Map&lt;String,String&gt; map,String backUrlState,String domain,QQHttpClient client);</td><td>qq回调函数</td><td>Map&lt;String,String&gt;</td><td>request获取的qq信息携带参数的map集合。key说明：code：<em>Authorization Code</em>；state：uuid；注意：数据获取来自qq返回的信息</td><td>Map&lt;String,String&gt;</td><td>key说明：openid：用户唯一标识；nickname：qq用户名；figureurl_qq_2：100*100像素头像链接</td></tr><tr><td></td><td></td><td>String</td><td>获取登录链接方法中创建的uuid</td><td></td><td></td></tr><tr><td></td><td></td><td>String</td><td>应用配置的域名</td><td></td><td></td></tr><tr><td></td><td></td><td>QQHttpClient</td><td>QQHttpClient配置对象</td><td></td><td></td></tr></tbody></table><h4 id="redis工具类-RedisUtils">redis工具类(RedisUtils)</h4><table><thead><tr><th>方法名称</th><th>方法注释</th><th>传入参数</th><th>参数注释</th><th>返回结果</th><th>结果注释</th></tr></thead><tbody><tr><td>expire(key,time)</td><td>指定缓存失效时间</td><td>String,long</td><td>键，时间</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>getExpire(key)</td><td>获取key的过期时间</td><td>String</td><td>键</td><td>long</td><td>返回过期时间，若为0则永久有效</td></tr><tr><td>hasKey(key)</td><td>判断key是否存在</td><td>String</td><td>键</td><td>boolean</td><td>true：存在，false：不存在</td></tr><tr><td>del(key)</td><td>删除缓存</td><td>String/String []</td><td>单键，键数组</td><td>void</td><td>无返回结果</td></tr><tr><td>get(key)</td><td>获取缓存值</td><td>String</td><td>键</td><td>Object</td><td>缓存值</td></tr><tr><td>set(key,value)</td><td>存放缓存</td><td>String,Object</td><td>键，值</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>set(key,value,time)</td><td>存放缓存并设置失效时间</td><td>String,Object,long</td><td>键，值，失效时间(当time小于等于0时，键永久有效)</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>hmset(key,map)</td><td>存放Map类型键值对</td><td>String,Map&lt;String,Object&gt;</td><td>键，Map数据值</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>hmset(key,map,time)</td><td>存放Map类型键值对并设置过期时间</td><td>String,Map&lt;String,Object&gt;,long</td><td>键，Map数据值，过期时间(当time小于等于0时，键永久有效)</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>hmget(key)</td><td>获取key对应的Map键值</td><td>String</td><td>键</td><td>Map&lt;Object,Object&gt;</td><td>Map键值对</td></tr><tr><td>hget(key,item)</td><td>获取key对应的map键的值</td><td>String,String</td><td>键，map键</td><td>Object</td><td>map值</td></tr><tr><td>hset(key,item,value)</td><td>向hash表中存放数据，如果不存在则创建</td><td>String,String,Object</td><td>键，项，值</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>hset(key,item,value,time)</td><td>向hash表中存放数据，如果不存在则创建</td><td>String,String,Object,long</td><td>键，项，值，时间(如果已存在的hash表有时间,这里将会替换原有的时间)</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>hdel(key,item)</td><td>删除hash表中的值</td><td>String,String/String []</td><td>键，单项或项数组</td><td>void</td><td>无返回结果</td></tr><tr><td>hHasKey(key,item)</td><td>判断hash表中是否有项的值</td><td>String,String</td><td>键，项</td><td>boolean</td><td>true：存在，false：不存在</td></tr><tr><td>sSet(key,values)</td><td>将数据放入set缓存</td><td>String,Object/Object []</td><td>键，值</td><td>long</td><td>返回成功的个数</td></tr><tr><td>sSetAndTime(key,time,values)</td><td>将数据放入set缓存并设置过期时间</td><td>String,time,Object/Object []</td><td>键，时间，值</td><td>long</td><td>返回成功个数</td></tr><tr><td>sHasKey(key,value)</td><td>判断set中的value是否存在</td><td>String,Object</td><td>键，值</td><td>boolean</td><td>true：存在，false：不存在</td></tr><tr><td>sGet(key)</td><td>根据key获取Set中的所有值</td><td>String</td><td>键</td><td>Set<object></object></td><td>set值</td></tr><tr><td>sGetSetSize(key)</td><td>获取set缓存长度</td><td>String</td><td>键</td><td>long</td><td>缓存长度</td></tr><tr><td>setRemove(key,values)</td><td>移除set缓存中值为values的</td><td>String,Object/Object p[]</td><td>键，值/值数组</td><td>long</td><td>返回移除的个数</td></tr><tr><td>lSet(key,value)</td><td>向list中存放缓存</td><td>String,Object</td><td>键，值</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>lSet(key, value,time)</td><td>向list中存放缓存并设置缓存时间</td><td>String,Object,long</td><td>键，值，时间(time小于等于0为永久有效)</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>lSet(key,value)</td><td>向list中存放缓存，值为list</td><td>String,List<object></object></td><td>键，值</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>lSet(key, value,time)</td><td>向list中存放缓存，值为list并设置缓存时间</td><td>String,List<object>,long</object></td><td>键，值，时间(time小于等于0为永久有效)</td><td>boolean</td><td>rue：成功，false：失败</td></tr><tr><td>lGet(key, start,end)</td><td>获取list缓存的内容</td><td>String,long,long</td><td>键，开始，结束(0 到 -1代表所有值)</td><td>List<object></object></td><td>获取的缓存值</td></tr><tr><td>lGetListSize(key)</td><td>获取list缓存的长度</td><td>String</td><td>键</td><td>long</td><td>缓存长度</td></tr><tr><td>lGetIndex(key,index)</td><td>通过索引获取list中的缓存值</td><td>String,long</td><td>键，索引(索引 index&gt;=0时， 0 表头，1 第二个元素，依次类推；index&lt;0时，-1，表尾，-2倒数第二个元素，依次类推)</td><td>Object</td><td>缓存值</td></tr><tr><td>lUpdateIndex(key, index,value)</td><td>修改list中索引对应的值</td><td>String,long,Object</td><td>键，索引，值</td><td>boolean</td><td>true：成功，false：失败</td></tr><tr><td>lRemove(key, count,value)</td><td>移除list缓存中值为value的n个缓存</td><td>String,long,Object</td><td>键，个数，值</td><td>long</td><td>移除的个数</td></tr><tr><td>removeAll(keyPrefix)</td><td>删除指定前缀的一系列key</td><td>String</td><td>键前缀</td><td>void</td><td>无返回结果</td></tr></tbody></table><h3 id="类库文件">类库文件</h3><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/dingdm-1.2.jar" target="_blank" rel="noopener">https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/dingdm-1.2.jar</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像全景拼接</title>
      <link href="/2021/09/12/tu-xiang-pin-jie-ru-men/"/>
      <url>/2021/09/12/tu-xiang-pin-jie-ru-men/</url>
      
        <content type="html"><![CDATA[<h4 id="图像全景拼接">图像全景拼接</h4><h5 id="前提知识">前提知识</h5><h6 id="泰勒公式">泰勒公式</h6><p>使用多次多项式来近似表达原函数f(x)。</p><p>对于一个函数，在某一点处可以找到一个函数的值与它相同，但是在该点的领域内可能找到的图像不尽相同，而导数反应的是变化率，因此可以对当前近似表示的函数求导，对于近似函数进行多阶求导后，近似于原函数。</p><p>对于函数中某一点其一阶导数为0，二阶导数不为0，那么该点必定为极值点。再进一步分析，如果二阶导数大于0，那么该点为极小值点，如果二阶导数小于0，那么该点为极大值点。</p><p>有一阶泰勒公式到二阶泰勒，即二阶导数的x,y求导构成黑塞矩阵。</p><h6 id="hessian">hessian</h6><p>黑塞是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。 <strong>构建Hessian矩阵的目的是为了生成图像稳定的边缘点(突变点)</strong></p><p>对于二阶泰勒公式的二阶求导，可以近似为一个二次型函数。中间的矩阵为黑塞矩阵。根据二次型矩阵的等高线图可以看出，特征值较大的地方变化速度快，而特征值较小的地方变化速度慢。</p><p>可以使用黑塞矩阵来进行图像灰度图的特征点检测。特征值较大的地方像素图像变化明显，而特征值小的地方像素图像变化慢。surf即利用了黑塞来检测突变点和突变线。</p><h6 id="盒式滤波器">盒式滤波器</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/heshi.png" alt=""></p><p>1、给定一张图像，宽高为（M,N），确定待求矩形模板的宽高(m,n)，如图紫色矩形。图中每个黑色方块代表一个像素，红色方块是假想像素。</p><p>2、开辟一段大小为M的数组，记为buff, 用来存储计算过程的中间变量，用红色方块表示</p><p>3、将矩形模板（紫色）从左上角（0，0）开始，逐像素向右滑动，到达行末时，矩形移动到下一行的开头（0，1），如此反复，每移动到一个新位置时，计算矩形内的像素和，保存在数组A中。以(0,0)位置为例进行说明：首先将绿色矩形内的每一列像素求和，结果放在buff内（红色方块），再对蓝色矩形内的像素求和，结果即为紫色特征矩形内的像素和，把它存放到数组A中，如此便完成了第一次求和运算。</p><p>4、每次紫色矩形向右移动时，实际上就是求对应的蓝色矩形的像素和，此时只要把上一次的求和结果减去蓝色矩形内的第一个红色块，再加上它右面的一个红色块，就是当前位置的和了，用公式表示 sum[i] = sum[i-1] - buff[x-1] + buff[x+m-1]</p><p>5、当紫色矩形移动到行末时，需要对buff进行更新。因为整个绿色矩形下移了一个像素，所以对于每个buff[i], 需要加上一个新进来的像素，再减去一个出去的像素，然后便开始新的一行的计算了。</p><h5 id="SURF算法步骤">SURF算法步骤</h5><h6 id="对图像进行高斯滤波，构造黑塞矩阵">对图像进行高斯滤波，构造黑塞矩阵</h6><p>通过前提知识通过黑塞矩阵找到像素极值。</p><h6 id="特征点过滤">特征点过滤</h6><p>通过黑塞矩阵得到的像素点与同层的8个像素点和不同层的18个像素点进行比较，找出稳定的特征点。</p><h6 id="计算特征点主方向">计算特征点主方向</h6><p><strong>统计特征点圆形邻域内的Harr小波特征，即在特征点的圆形邻域内，统计60度扇形内所有点的水平、垂直Harr小波特征总和，然后扇形以0.2弧度大小的间隔进行旋转并再次统计该区域内Harr小波特征值之后，最后将值最大的那个扇形的方向作为该特征点的主方向。</strong></p><h6 id="描述特征点">描述特征点</h6><p><strong>提取特征点周围4×4×4个矩形区域块，所取得矩形区域方向是沿着特征点的主方向，</strong></p><p><strong>每个子区域统计25个像素点水平方向和垂直方向的Haar小波特征。</strong></p><p><strong>该Harr小波特征为水平方向值之和、垂直方向值之和、水平方向值绝对值之和以及垂直方向绝对之和4个方向。把这4个值作为每个子块区域的特征向量，所以一共有4×4×4=64维向量作为SURF特征的描述子。</strong></p><pre class=" language-language-python"><code class="language-language-python">    starttime = time.clock()    left = cv2.imread('/Users/dinggongcheng/Downloads/1/left.jpeg')    # left=cv2.resize(left,dsize=(512,512))    left_gray = cv2.cvtColor(left, cv2.COLOR_BGR2GRAY)    right = cv2.imread('/Users/dinggongcheng/Downloads/1/right.jpeg')    # right=cv2.resize(right,dsize=(512,512))    right_gray = cv2.cvtColor(right, cv2.COLOR_BGR2GRAY)    # 提取左右图像的surf特征点    detector = cv2.xfeatures2d_SURF.create(hessianThreshold=400)    left_kps, left_dess = detector.detectAndCompute(left_gray, None)    right_kps, right_dess = detector.detectAndCompute(right_gray, None)</code></pre><h5 id="KNN特征匹配算法">KNN特征匹配算法</h5><p>KNN算法的三要素：k值的选取，距离度量的方式和分类决策规则。</p><p>k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。</p><p>使用欧式距离计算领域内与当前点的距离，并递增排序</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/oushi.png" alt=""></p><h6 id="knn的优化算法kd树和kd球">knn的优化算法kd树和kd球</h6><p>kd树</p><p>比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：</p><p>1）找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上方差更大，用第1维特征建树。</p><p>2）确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：划分点维度的直线x=7；</p><p>3）确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x&lt;=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}。</p><p>4）用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)，(8,1)}。最终得到KD树。</p><p>当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。对于一个目标点，我们首先在KD树里面找到包含目标点的叶子节点。以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个超球体内部。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。如果不相交那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</p><p>从上面的描述可以看出，KD树划分后可以大大减少无效的最近邻搜索，很多样本点由于所在的超矩形体和超球体不相交，根本不需要计算距离。大大节省了计算时间。</p><p>我们用建立的KD树，来看对点(2,4.5)找最近邻的过程。</p><p>先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;(7,2)，(5,4)，(4,7)&gt;，但 （4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点； 以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找，也就是将（2,3）节点加入搜索路径中得&lt;(7,2)，(2,3)&gt;；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。</p><pre class=" language-language-python"><code class="language-language-python">    # 利用knn对左右图像的特征点进行匹配    matcher = cv2.FlannBasedMatcher_create()    knn_matchers = matcher.knnMatch(left_dess, right_dess, 2)</code></pre><h5 id="RANSAC算法">RANSAC算法</h5><p>ransac算法去除特征点匹配的噪声印象。</p><p>RANSAC算法的具体描述是：给定N个数据点组成的集合P，假设集合中大多数的点都是可以通过一个模型来产生的，且最少通过n个点（n&lt;N）可以拟合出模型的参数，则可以通过以下的迭代方式拟合该参数。<br>对下面的操作执行kk次：<br>（1）从P中随机选择n个数据点；<br>（2）用这n个数据点拟合出一个模型M；<br>（3）对P中剩余的数据点，计算每个点与模型M的距离，距离超过阈值的则认定为局外点，不超过阈值的认定为局内点，并记录该模型M所对应的局内点的值m；<br>迭代k次以后，选择m最大的模型M作为拟合的结果。<br>因为在实际应用中N的值通常会很大，那么从其中任选n个数据点的组合就会很大，如果对所有组合都进行上面的操作运算量就会很大，因此对于k的选择就很重要。通常情况下，只要保证模型估计需要的n个点都是点的概率足够高即可。因此设w为N个数据中局内点的比例，z为进行k次选取后，至少有一次选取的n个点都是局内点的概率。则有</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/gailvone.png" alt=""></p><p>其中1−w^n表示一次选取不都是局内点的概率，(1−w^n)^k表示k次选取中没有一次都是局内点的概率。<br>则有</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/gailvone.png" alt=""></p><p>这里 zz一般要求满足大于95%即可。</p><p>通过四对匹配点还原H矩阵。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hm.png" alt=""></p><pre class=" language-language-python"><code class="language-language-python">    # 挑出好的匹配点    for m, n in knn_matchers:        if m.distance < 0.5 * n.distance:            good_keypoints.append(m)    left_points = np.zeros(shape=(len(good_keypoints), 2), dtype=np.float32)    right_points = np.zeros(shape=(len(good_keypoints), 2), dtype=np.float32)    outimg = cv2.drawMatches(left, left_kps, right, right_kps, good_keypoints,None)    #cv2.imshow('knn',outimg)    #cv2.waitKey(0)    for i in range(len(good_keypoints)):        left_points[i][0] = left_kps[good_keypoints[i].queryIdx].pt[0]        left_points[i][1] = left_kps[good_keypoints[i].queryIdx].pt[1]        right_points[i][0] = right_kps[good_keypoints[i].trainIdx].pt[0]        right_points[i][1] = right_kps[good_keypoints[i].trainIdx].pt[1]    # 求取单应矩阵    H, _ = cv2.findHomography(right_points, left_points)</code></pre><h5 id="拼接图像的透视变换，求出四个透视点即可。">拼接图像的透视变换，求出四个透视点即可。</h5><pre class=" language-language-python"><code class="language-language-python"># 求出右图像的透视变化顶点warp_point = warp_corner(H, right)# 求出右图像的透视变化图像imagewarp = cv2.warpPerspective(right, H, (left.shape[1] + right.shape[1], left.shape[0]))</code></pre><h5 id="opencv-stitcher-api使用">opencv stitcher api使用</h5><pre class=" language-language-python"><code class="language-language-python">import cv2import timestart = time.clock()st = time.time()img1 = cv2.imread('/Users/dinggongcheng/Downloads/1/4.jpeg')img2 = cv2.imread('/Users/dinggongcheng/Downloads/1/3.jpeg')stitcher = cv2.createStitcher(True)(_result, pano) = stitcher.stitch((img1, img2))end = time.clock()et = time.time()print(et - st)print(end - start)cv2.imshow('pic',pano)cv2.waitKey(0)</code></pre><h5 id="ORB算法步骤">ORB算法步骤</h5><h6 id="FAST具体计算过程">FAST具体计算过程</h6><ol><li>从图片中选取一个像素点P，判断它是否是一个特征点。我们首先把它的密度（即灰度值）设为Ip。</li><li>设定一个合适的阙值t ：当2个点的灰度值之差的绝对值大于t时，我们认为这2个点不相同。</li><li>考虑该像素点周围的16个像素。</li><li>现在如果这16个点中有连续的n个点都和选定的像素点不同，那么它就是一个角点。 这里n设定为12。</li><li>我们现在提出一个高效的测试，来快速排除一大部分非特征点的点。该测试仅仅检查在位置1、9、5和13四个位置的像素（首先检查1和9，看它们是否和点相同。如果是，再检查5和13）。如果是一个角点，那么上述四个像素点中至少有3个应该和点相同。如果都不满足，那么不可能是一个角点。</li></ol><p>fast算法的数学解析</p><p><a href="https://blog.csdn.net/qq_37764129/article/details/80970032" target="_blank" rel="noopener">https://blog.csdn.net/qq_37764129/article/details/80970032</a></p><h6 id="特征点描述">特征点描述</h6><p>1.以关键点P为圆心，以d为半径做圆O。</p><p>2.在圆O内某一模式选取N个点对。这里为方便说明，N=4，实际应用中N可以取512.</p><p>假设当前选取的4个点对如上图所示分别标记为：p1(A,B) p2(A,B) p3(A,B) p4(A,B)</p><p>3.定义操作T</p><p>A点的灰度值大于B点，T为1，反之为0；</p><p>4.分别对已选取的点对进行T操作，将得到的结果进行组合。最终得到对应的二进制编码来表示特征点。</p><p>BRISK算法数学解析</p><p><a href="https://blog.csdn.net/weixin_40196271/article/details/84143545" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40196271/article/details/84143545</a></p><p>ORB的相似度为90%</p><pre class=" language-language-python"><code class="language-language-python">    # 提取左右图像的ORB特征点    detector = cv2.ORB.create()    left_kps, left_dess = detector.detectAndCompute(left_gray, None)    right_kps, right_dess = detector.detectAndCompute(right_gray, None)</code></pre><pre class=" language-language-python"><code class="language-language-python">    # 利用knn对左右图像的特征点进行匹配    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)    knn_matchers = bf.knnMatch(left_dess, right_dess, k=2)</code></pre><pre class=" language-language-python"><code class="language-language-python">    # 挑出好的匹配点    for m in range(len(knn_matchers) - 1):        if knn_matchers[m][0].distance < 0.9 * knn_matchers[m][1].distance:            good_keypoints.append(knn_matchers[m])</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像拼接 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于云服务器的远程连接搭建</title>
      <link href="/2021/08/26/ji-yu-yun-fu-wu-qi-de-yuan-cheng-lian-jie-da-jian/"/>
      <url>/2021/08/26/ji-yu-yun-fu-wu-qi-de-yuan-cheng-lian-jie-da-jian/</url>
      
        <content type="html"><![CDATA[<h3 id="基于云服务器的远程连接搭建">基于云服务器的远程连接搭建</h3><p>本来打算直接路由器的，但是梅林的frpc要虚拟内存，而我手上又没有好的u盘，所以先用最直接的吧。</p><h4 id="域名解析">域名解析</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yuming.png" alt=""></p><h4 id="服务器端frpc设置">服务器端frpc设置</h4><p>下载自己对应服务器系统的文件，使用filezilla进行上传。我这里的服务器的系统为ubuntu。</p><p><a href="https://github.com/fatedier/frp/releases/download/v0.37.1/frp_0.37.1_linux_amd64.tar.gz" target="_blank" rel="noopener">frp_0.37.1_linux_amd64.tar.gz</a></p><p>解压后修改配置文件frpc.ini中的common部分，设置端口为7000。</p><pre class=" language-language-ini"><code class="language-language-ini">[common]bind_port = 7000</code></pre><p><strong>注意：需要在服务器的安全组中放行7000和3389远程连接端口，使用其他端口则放行其他端口。</strong></p><pre class=" language-language-shell"><code class="language-language-shell">nohup ./frps -c frps.ini &</code></pre><p>后台启动服务器端frpc服务，exit退出ssh即可。</p><h4 id="客户端frpc设置">客户端frpc设置</h4><p><a href="https://github.com/fatedier/frp/releases/download/v0.37.1/frp_0.37.1_windows_386.zip" target="_blank" rel="noopener">frp_0.37.1_windows_386.zip</a></p><p>个人的客户端为windows</p><p>解压后修改配置文件frpc.ini的common部分，并添加远程连接配置信息</p><pre class=" language-language-ini"><code class="language-language-ini">[common]server_addr = 自己的ip地址server_port = 7000 #自己使用的端口号</code></pre><p>添加的配置</p><pre class=" language-language-ini"><code class="language-language-ini">[dinggc] 一个描述符号 自定义type = tcplocal_ip = 127.0.0.1local_port = 3389 #远程控制的端口remote_port = 3389 #自定义端口，用于后期远程控制使用</code></pre><h5 id="使用winsw将客户端frpc注册为服务自启动">使用winsw将客户端frpc注册为服务自启动</h5><p><a href="https://github.com/winsw/winsw/releases/download/v3.0.0-alpha.10/WinSW-x64.exe" target="_blank" rel="noopener">WinSW-x64.exe</a></p><p>创建与exe同名同目录的配置文件xml</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/winsw.png" alt=""></p><pre class=" language-language-xml"><code class="language-language-xml"><service><!-- 该服务的唯一标识 -->    <id>frp</id>    <!-- 该服务的名称 -->    <name>frp_0.37.1_windows_386.zip</name>    <!-- 该服务的描述 -->    <description>frpc客户端服务</description>    <!-- 要运行的程序路径 -->    <executable>D:\frpc\frpc.exe</executable>    <!-- 携带的参数 -->    <arguments>-c frpc.ini</arguments>    <!-- 第一次启动失败 60秒重启 -->    <onfailure action="restart" delay="60 sec"/>    <!-- 第二次启动失败 120秒后重启 -->    <onfailure action="restart" delay="120 sec"/>    <!-- 日志模式 -->    <logmode>append</logmode>    <!-- 指定日志文件目录(相对于executable配置的路径) -->    <logpath>logs</logpath></service></code></pre><p>注册服务即可</p><pre class=" language-language-shell"><code class="language-language-shell">winsw.exe install</code></pre><p>通过控制台即可ctrl+f即可搜索frpc服务。</p><h4 id="远程访问">远程访问</h4><p>使用域名+端口的形式即可远程访问自己的电脑。我这垃圾1M的服务器卡死我了。等开学买u盘用路由器内网穿透吧。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frpc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解放双手，从我做起</title>
      <link href="/2021/06/20/jie-fang-shuang-shou-cong-wo-zuo-qi/"/>
      <url>/2021/06/20/jie-fang-shuang-shou-cong-wo-zuo-qi/</url>
      
        <content type="html"><![CDATA[<h3 id="解放双手，从我做起">解放双手，从我做起</h3><h4 id="有爱自取">有爱自取</h4><h4 id="明日方舟">明日方舟</h4><p><a href="https://github.com/SprBoot/arknight" target="_blank" rel="noopener">https://github.com/SprBoot/arknight</a></p><pre class=" language-language-python"><code class="language-language-python">import utilimport timeimport randomtotal = int(input("请输入当前总体力:"))need = int(input("请输入每次刷图所需体力:"))number = int(total / need)# 首次模拟点击开始运行util.singleClick('./current.jpg', "./single.jpg", "Qt5QWindowIcon", "明日方舟 - MuMu模拟器", 'current.jpg')time.sleep(1)util.singleClick('./startMission.jpg', "./start.jpg","Qt5QWindowIcon", "明日方舟 - MuMu模拟器",'startMission.jpg')second = util.getStep("Qt5QWindowIcon", "明日方舟 - MuMu模拟器", "./single.jpg","./start.jpg")initial = 2while initial <= number:    time.sleep(second + random.uniform(1.2,2.8))    util.turn("Qt5QWindowIcon", "明日方舟 - MuMu模拟器","./single.jpg","./start.jpg")    initial = initial + 1print('本次刷图结束，一共刷图{}次'.format(number))</code></pre><pre class=" language-language-python"><code class="language-language-python">#对后台窗口截图import win32gui, win32ui, win32conimport cv2import numpyimport pyautoguiimport timeimport randomdef img(cls,name):    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls,name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    width = right - left    height = bot - top    # 返回句柄窗口的设备环境，覆盖整个窗口，包括非客户区，标题栏，菜单，边框    hWndDC = win32gui.GetWindowDC(hWnd)    # 创建设备描述表    mfcDC = win32ui.CreateDCFromHandle(hWndDC)    # 创建内存设备描述表    saveDC = mfcDC.CreateCompatibleDC()    # 创建位图对象准备保存图片    saveBitMap = win32ui.CreateBitmap()    # 为bitmap开辟存储空间    saveBitMap.CreateCompatibleBitmap(mfcDC, width, height)    # 将截图保存到saveBitMap中    saveDC.SelectObject(saveBitMap)    # 保存bitmap到内存设备描述表    saveDC.BitBlt((0, 0), (width, height), mfcDC, (0, 0), win32con.SRCCOPY)    ##方法三（第一部分）：opencv+numpy保存    ###获取位图信息    signedIntsArray = saveBitMap.GetBitmapBits(True)    ##方法三（第二部分）：opencv+numpy保存    ###PrintWindow成功，保存到文件，显示到屏幕    im_opencv = numpy.frombuffer(signedIntsArray, dtype='uint8')    im_opencv.shape = (height, width, 4)    im_opencv = cv2.cvtColor(im_opencv, cv2.COLOR_BGRA2RGB)    cv2.imwrite("im_opencv.jpg",im_opencv,[int(cv2.IMWRITE_JPEG_QUALITY), 100]) #保存    # cv2.namedWindow('im_opencv') #命名窗口    # cv2.imshow("im_opencv",im_opencv) #显示    template = cv2.imread('./finish.jpg')    res = cv2.matchTemplate(im_opencv, template, cv2.TM_CCOEFF_NORMED)    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)    #H1 = cv2.calcHist([im_opencv], [1], None, [256], [0, 256])    #H1 = cv2.normalize(H1, H1, 0, 1, cv2.NORM_MINMAX, -1)  #    #H2 = cv2.calcHist([template], [1], None, [256], [0, 256])    #H2 = cv2.normalize(H2, H2, 0, 1, cv2.NORM_MINMAX, -1)    # 利用compareHist（）进行比较相似度    #similarity = cv2.compareHist(H1, H2, 0)    cv2.waitKey(0)    cv2.destroyAllWindows()    # 内存释放    win32gui.DeleteObject(saveBitMap.GetHandle())    saveDC.DeleteDC()    mfcDC.DeleteDC()    win32gui.ReleaseDC(hWnd, hWndDC)    return max_valdef singleClick(path,single,cls,name,picName):    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls,name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    width = right - left    height = bot - top    # 返回句柄窗口的设备环境，覆盖整个窗口，包括非客户区，标题栏，菜单，边框    hWndDC = win32gui.GetWindowDC(hWnd)    # 创建设备描述表    mfcDC = win32ui.CreateDCFromHandle(hWndDC)    # 创建内存设备描述表    saveDC = mfcDC.CreateCompatibleDC()    # 创建位图对象准备保存图片    saveBitMap = win32ui.CreateBitmap()    # 为bitmap开辟存储空间    saveBitMap.CreateCompatibleBitmap(mfcDC, width, height)    # 将截图保存到saveBitMap中    saveDC.SelectObject(saveBitMap)    # 保存bitmap到内存设备描述表    saveDC.BitBlt((0, 0), (width, height), mfcDC, (0, 0), win32con.SRCCOPY)    ##方法三（第一部分）：opencv+numpy保存    ###获取位图信息    signedIntsArray = saveBitMap.GetBitmapBits(True)    ##方法三（第二部分）：opencv+numpy保存    ###PrintWindow成功，保存到文件，显示到屏幕    im_opencv = numpy.frombuffer(signedIntsArray, dtype='uint8')    im_opencv.shape = (height, width, 4)    cv2.cvtColor(im_opencv, cv2.COLOR_BGRA2RGB)    cv2.imwrite(picName,im_opencv,[int(cv2.IMWRITE_JPEG_QUALITY), 100]) #保存    current = cv2.imread(path)    template = cv2.imread(single)    template = cv2.cvtColor(template, cv2.COLOR_BGRA2RGB)    h, w, l = template.shape    # 当前捕捉截图与模板匹配，找到再次出击按钮，模拟鼠标点击操作    res = cv2.matchTemplate(current, template, cv2.TM_CCOEFF_NORMED)    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)    top_left = max_loc    bottom_right = (top_left[0] + w, top_left[1] + h)    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls, name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    pyautogui.click(random.uniform(left + top_left[0],left + bottom_right[0]),random.uniform(top + top_left[1],top + bottom_right[1]))def getStep(cls,name,single,start):    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls,name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    flag = True    total = time.time()    startTime = time.time()    while(flag):        similarity = img(cls,name)        if(similarity > 0.95):            total = time.time() - startTime            time.sleep(4 + random.uniform(0.2,1.8))            pyautogui.click(left + random.uniform(120,250), bot - random.uniform(120,250))            flag = False            time.sleep(4 + random.uniform(0.2,1.8))            singleClick('./current.jpg',single,cls,name,'current.jpg')            time.sleep(2 + random.uniform(0.2,1.8))            singleClick('./startMission.jpg',start,cls,name,'startMission.jpg')            print('间隔时间计算完成',total)    return totaldef turn(cls,name,single,start):    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls,name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    flag = True    while(flag):        similarity = img(cls, name)        if (similarity > 0.95):            time.sleep(4 + random.uniform(0.2,1.8))            pyautogui.click(left + random.uniform(120,250), bot - random.uniform(120,250))            time.sleep(4 + random.uniform(0.2,1.8))            singleClick('./current.jpg', single, cls, name, 'current.jpg')            time.sleep(2 + random.uniform(0.2,1.8))            singleClick('./startMission.jpg', start, cls, name, 'startMission.jpg')            break</code></pre><h4 id="碧蓝航线">碧蓝航线</h4><p><a href="https://github.com/SprBoot/blhx" target="_blank" rel="noopener">https://github.com/SprBoot/blhx</a></p><pre class=" language-language-python"><code class="language-language-python">import win32import timewhile 1 == 1:    flag = True    win32.singleClick('./current.jpg', "./seven.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')    time.sleep(1)    win32.singleClick('./current.jpg', "./start.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')    time.sleep(1)    win32.singleClick('./current.jpg', "./start.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')    time.sleep(5)    boss = True    initial = 1    while(boss):        tempFlag = True        while(tempFlag):            onestargold = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./1stargold.jpg")            onestarqing = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./1starqing.jpg")            onestarzhong = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./1starzhong.jpg")            twostarqing = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./2starqing.jpg")            twostarzhong = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./2starzhong.jpg")            path = "./1stargold.jpg"            tu = '金子船'            if(onestargold > 0.4):                path = "./1stargold.jpg"            elif(onestarqing > 0.4):                path = "./1starqing.jpg"                tu = '1星轻'            elif(onestarzhong > 0.4):                path = "./1starzhong.jpg"                tu = '1星重'            elif(twostarqing > 0.4):                path = "./2starqing.jpg"                tu = '2星轻'            elif (twostarzhong > 0.4):                path = "./2starzhong.jpg"                tu = '2星重'            wei = True            while wei:                tuo = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./info.jpg")                if tuo > 0.8:                    time.sleep(1)                    win32.singleClick('./current.jpg', "./sure.jpg", "Qt5QWindowIcon",                                      "碧蓝航线 - MuMu模拟器", 'current.jpg')                else:                    wei = False            win32.singleClick('./current.jpg', path, "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')            print('选择了{}'.format(tu))            temp = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./chu.jpg")            if (temp > 0.8):                print('到达图准备出击')                flag = True                win32.singleClick('./current.jpg', "./chu.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                while (flag):                    max = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./win.jpg")                    if (max > 0.85):                        print('出击成功')                        initial = initial + 1                        time.sleep(1)                        win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                        time.sleep(3)                        win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                        confirmFlag = True                        while(confirmFlag):                            srFlag = True                            while (srFlag):                                sr = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./sr.jpg")                                if (sr > 0.8):                                    time.sleep(1)                                    win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                else:                                    srFlag = False                            time.sleep(2)                            con = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./confirm.jpg")                            if(con > 0.9):                                win32.singleClick('./current.jpg', "./confirm.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器",                                                  'current.jpg')                                time.sleep(5)                                iboss = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./boss.jpg")                                if (initial == 6):                                    boss = False                                    tempFlag = False                                if (iboss > 0.95):                                    tempFlag = False                                    boss = False                                confirmFlag = False                                flag = False            danger = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./guibi.jpg")            if (danger > 0.95):                win32.singleClick('./current.jpg', './guibi.jpg', "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                print('规避成功')                time.sleep(3)                chu = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./chu.jpg")                if (chu > 0.95):                    print('规避失败，过图')                    win32.singleClick('./current.jpg', "./chu.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                    dangerFlag = True                    while (dangerFlag):                        max = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./win.jpg")                        if (max > 0.85):                            time.sleep(1)                            win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                            time.sleep(3)                            win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                            confirmFlag = True                            while (confirmFlag):                                srFlag = True                                while (srFlag):                                    sr = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./sr.jpg")                                    if (sr > 0.8):                                        time.sleep(1)                                        win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                    else:                                        srFlag = False                                time.sleep(2)                                con = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./confirm.jpg")                                if (con > 0.9):                                    win32.singleClick('./current.jpg', "./confirm.jpg", "Qt5QWindowIcon",                                                      "碧蓝航线 - MuMu模拟器",                                                      'current.jpg')                                    time.sleep(5)                                    iboss = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./boss.jpg")                                    if (iboss > 0.95):                                        tempFlag = False                                        boss = False                                    if (initial == 6):                                        boss = False                                        tempFlag = False                                    confirmFlag = False                                    dangerFlag = False    time.sleep(5)    wen = True    while(wen):        number = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./wenhao.jpg")        if(number < 0.6):            print('物资采集完成')            wei = True            while (wei):                tuo = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./info.jpg")                if (tuo > 0.8):                    time.sleep(1)                    win32.singleClick('./current.jpg', "./sure.jpg", "Qt5QWindowIcon",                                      "碧蓝航线 - MuMu模拟器", 'current.jpg')                else:                    wei = False            win32.singleClick('./current.jpg', "./boss.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')            print('boss图')            temp = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./chuji.jpg")            if (temp > 0.8):                print('到达boss')                tflag = True                win32.singleClick('./current.jpg', "./chu.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器",                                  'current.jpg')                while (tflag):                    max = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./win.jpg")                    if (max > 0.85):                        print('boss出击成功')                        time.sleep(1)                        win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                        time.sleep(3)                        win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                        confirmFlag = True                        while (confirmFlag):                            srFlag = True                            while (srFlag):                                sr = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./sr.jpg")                                if (sr > 0.8):                                    time.sleep(1)                                    win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                else:                                    srFlag = False                            time.sleep(2)                            con = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./confirm.jpg")                            if (con > 0.9):                                win32.singleClick('./current.jpg', "./confirm.jpg", "Qt5QWindowIcon",                                                  "碧蓝航线 - MuMu模拟器",                                                  'current.jpg')                                time.sleep(5)                                confirmFlag = False                                tflag = False                                wenFlag = False                                wen = False        else:            wenFlag = True            while(wenFlag):                danger = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./guibi.jpg")                if (danger > 0.95):                    win32.singleClick('./current.jpg', './guibi.jpg', "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                    print('规避成功')                    time.sleep(3)                    chu = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./chu.jpg")                    if (chu > 0.95):                        print('规避失败，过图')                        win32.singleClick('./current.jpg', "./chu.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器",                                          'current.jpg')                        dangerFlag = True                        while (dangerFlag):                            max = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./win.jpg")                            if (max > 0.85):                                time.sleep(1)                                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                time.sleep(3)                                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                confirmFlag = True                                while (confirmFlag):                                    srFlag = True                                    while (srFlag):                                        sr = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./sr.jpg")                                        if (sr > 0.8):                                            time.sleep(1)                                            win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                        else:                                            srFlag = False                                    time.sleep(2)                                    con = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./confirm.jpg")                                    if (con > 0.9):                                        win32.singleClick('./current.jpg', "./confirm.jpg", "Qt5QWindowIcon",                                                          "碧蓝航线 - MuMu模拟器",                                                          'current.jpg')                                        time.sleep(5)                                        iboss = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./boss.jpg")                                        if (iboss > 0.95):                                            tempFlag = False                                            boss = False                                        confirmFlag = False                                        dangerFlag = False                win32.singleClick('./current.jpg', './wenhao.jpg', "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                error = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./error.jpg")                if (error > 0.6):                    wei = True                    while (wei):                        tuo = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./info.jpg")                        if (tuo > 0.8):                            time.sleep(1)                            win32.singleClick('./current.jpg', "./sure.jpg", "Qt5QWindowIcon",                                              "碧蓝航线 - MuMu模拟器", 'current.jpg')                        else:                            wei = False                    win32.singleClick('./current.jpg', "./boss.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                    print('boss图')                    temp = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./chu.jpg")                    if (temp > 0.8):                        print('到达boss')                        tflag = True                        win32.singleClick('./current.jpg', "./chu.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器",                                          'current.jpg')                        while (tflag):                            max = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./win.jpg")                            if (max > 0.85):                                print('boss出击成功')                                time.sleep(1)                                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                time.sleep(3)                                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                confirmFlag = True                                while (confirmFlag):                                    srFlag = True                                    while (srFlag):                                        sr = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./sr.jpg")                                        if (sr > 0.8):                                            time.sleep(1)                                            win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                        else:                                            srFlag = False                                    time.sleep(2)                                    con = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./confirm.jpg")                                    if (con > 0.9):                                        win32.singleClick('./current.jpg', "./confirm.jpg", "Qt5QWindowIcon",                                                          "碧蓝航线 - MuMu模拟器",                                                          'current.jpg')                                        time.sleep(5)                                        confirmFlag = False                                        tflag = False                                        wenFlag = False                                        wen = False                time.sleep(2)                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                test = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./wenhao.jpg")                if (test < 0.5):                    win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                    time.sleep(1)                    wei = True                    while (wei):                        tuo = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./info.jpg")                        if (tuo > 0.8):                            time.sleep(1)                            win32.singleClick('./current.jpg', "./sure.jpg", "Qt5QWindowIcon",                                              "碧蓝航线 - MuMu模拟器", 'current.jpg')                        else:                            wei = False                    win32.singleClick('./current.jpg', "./boss.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", 'current.jpg')                    print('boss图')                    temp = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./chu.jpg")                    if (temp > 0.8):                        print('到达boss')                        tflag = True                        win32.singleClick('./current.jpg', "./chu.jpg", "Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器",                                          'current.jpg')                        while (tflag):                            max = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./win.jpg")                            if (max > 0.8):                                print('boss出击成功')                                time.sleep(1)                                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                time.sleep(3)                                win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                confirmFlag = True                                while (confirmFlag):                                    srFlag = True                                    while (srFlag):                                        sr = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./sr.jpg")                                        if (sr > 0.8):                                            time.sleep(1)                                            win32.click("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器")                                        else:                                            srFlag = False                                    time.sleep(2)                                    con = win32.img("Qt5QWindowIcon", "碧蓝航线 - MuMu模拟器", "./confirm.jpg")                                    if (con > 0.9):                                        win32.singleClick('./current.jpg', "./confirm.jpg", "Qt5QWindowIcon",                                                          "碧蓝航线 - MuMu模拟器",                                                          'current.jpg')                                        time.sleep(5)                                        confirmFlag = False                                        tflag = False                                        wenFlag = False                                        wen = False    time.sleep(5)</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 脚本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习笔记四</title>
      <link href="/2021/06/14/shen-du-xue-xi-bi-ji-si/"/>
      <url>/2021/06/14/shen-du-xue-xi-bi-ji-si/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习笔记四">深度学习笔记四</h3><h4 id="Deep-Learning">Deep Learning</h4><h5 id="深度学习的network">深度学习的network</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/dee.jpg" alt=""></p><p>神经元的权重和偏差可以视为矩阵运算。即得到激活函数(activation function)。将激活函数进行sigmoid得到最终结果。将上方的网络用该种形式进行计算即为</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/dee2.jpg" alt=""></p><p>通过矩阵运行交由GPU加快运算过程。</p><h5 id="神经元网络的选取">神经元网络的选取</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/cry.jpg" alt=""></p><p>通过神经元网络计算得到的输入矩阵和根据图像得到的target矩阵进行交叉熵计算结果，调整参数使得最后计算得到的交叉熵最小以确定最优的神经元网络函数。</p><p>通过不同组的数据进行训练得到所有的交叉熵进行累加，使用gradient descent获取w与b。</p><h5 id="gradient-descent">gradient descent</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/grad.jpg" alt=""></p><h4 id="Backpropagation">Backpropagation</h4><p>帮助计算gradient descent中的参数的偏微分。</p><h5 id="chain-rule">chain rule</h5><p>即复合函数的微分过程。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/we.jpg" alt=""></p><p>C为最后的交叉熵。</p><p>z对于w求偏微分，z对于哪一个w求微分，得到的即为w对应的输入数据。</p><p>C对于z求偏微分。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/Cw.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/ch.jpg" alt=""></p><h5 id="Backward-pass">Backward pass</h5><p>以未知的两项作为输入，得出的最终结果为输出。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fa2.jpg" alt=""></p><p>具体的放大倍率即为对应z的sigmoid的求导。反求通过已知的C即可快速计算。</p><h4 id="Recipe-OF-Deep-Learning">Recipe OF Deep Learning</h4><p>对于得到神经元网络应先应用于训练数据判断是否合适，防止直接运用于测试数据出现的非过拟合现象。</p><h5 id="New-activation-function">New activation function</h5><p>vanishing gradient problem</p><p>靠近输入的神经元gradient参数的更新速度较慢或靠近输入的神经元gradient参数更新较大。</p><p>使用sigmoid时，较大的输入经过sigmoid后会进行衰减，越深的层数对最后的C的结果的影响就越小。</p><h6 id="ReLU">ReLU</h6><p>更改激活函数，当输入小于0时，输出为0，当输入大于0时，输出等于输入。</p><p>当神经元的输出为0时，不会影响整个神经元网络，可以直接去掉。而输入等于输出不会产生递减问题。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/rejy.jpg" alt=""></p><h6 id="maxout">maxout</h6><p>每层神经元的输出为分组中的最大值。</p><p>maxout也可以使用ReLU，即某个参数始终为0，其他参数为线性函数，即选取最大时满足ReLU。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/bax.jpg" alt=""></p><h6 id="Maxout-training">Maxout training</h6><p>选取的最大值为最终的输出值，最终简化神经元网络。</p><h5 id="Adaptive-Learning-Rate">Adaptive Learning Rate</h5><p>使用adagrad</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/adagrad.png" alt=""></p><h6 id="RMSProp">RMSProp</h6><p>防止在某个变量变化的过程中出现的变化较小和变化较大同时存在的情况。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/TRB.jpg" alt=""></p><h6 id="Momentum">Momentum</h6><p>利用惯性继续向前移动。</p><p>使用V记录移动距离。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/MVE.jpg" alt=""></p><h6 id="Adam">Adam</h6><p>RMSProp + Momentum</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/adam.png" alt=""></p><h5 id="Early-Stopping">Early Stopping</h5><p>在error上升前停住，即一个if判断即可。</p><h5 id="Regularization">Regularization</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/reg.jpg" alt=""></p><h5 id="DropOut">DropOut</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/st1.jpg" alt=""></p><p>在新的神经元网络上进行训练。表现的结果可能变差。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sy2.jpg" alt=""></p><p>训练时使用所有的神经元，而在测试时需要去掉不合理的神经元。</p><p>训练数据时使用dropout，而测试时不需要dropout，因此对于所有的输入没有dropout，需要使用(1-p)%。</p><p>在训练时对于每组不同的数据会得到较多的神经元网络，而在测试时对于测试数据应用于所有的神经元网络不现实，因此采用了(1-p)%的方式进行计算，而得到的结果与前种方式得到的结果是相似的。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经元网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读一</title>
      <link href="/2021/06/14/lun-wen-yue-du-yi/"/>
      <url>/2021/06/14/lun-wen-yue-du-yi/</url>
      
        <content type="html"><![CDATA[<h3 id="Learning-a-Deep-ConvNet-for-Multi-label-Classification-with-Partial-Labels论文阅读">Learning a Deep ConvNet for Multi-label Classification with Partial Labels论文阅读</h3><h4 id="知识储备">知识储备</h4><h5 id="Partial-Multi-Label-Learning">Partial Multi-Label Learning</h5><p>在多分类标记中，存在一种情况即在对图片中的物体进行标记时会出现难以标记的情况，图片中较明显的事物可以很容易的标记，而对于较为模糊的事物在标记者知识不足的情况下可能无法进行标记，或者当前图片中的事物过于抽象而无法确定所属。被称为偏标记学习。</p><p>在不确定的情况下，可以选择不标注、或者随机标注。但是不标注意味着我们丢失了所有信息，随机标注意味着可能带来噪声，对学习的影响更大。所以PML选择的是让标注者提供所有可能的标签，当然加了一个较强的假设：所有的标签都应该被包含在候选标签集中。</p><h5 id="Curriculum-Learning">Curriculum Learning</h5><p><strong>主张让模型先从容易的样本开始学习，并逐渐进阶到复杂的样本和知识。</strong></p><p>在以后的学习中进一步深入学习，目前仅限于了解即可。</p><h5 id="crowdsourcing">crowdsourcing</h5><p>互联网带来的新的生产组织形式。连线(Wired)杂志2006年发明的一个专业术语，用来描述一种新的商业模式，即企业利用互联网来将工作分配出去、发现创意或解决技术问题。通过互联网控制，这些组织可以利用志愿员工大军的创意和能力——这些志愿员工具备完成任务的技能，愿意利用业余时间工作，满足于对其服务收取小额报酬，或者暂时并无报酬，仅仅满足于未来获得更多报酬的前景。尤其对于软件业和服务业，这提供了一种组织劳动力的全新方式</p><h5 id="归一化">归一化</h5><p>归一化方法有两种形式，一种是把数变为（0，1）之间的<a href="https://baike.baidu.com/item/%E5%B0%8F%E6%95%B0/2172615" target="_blank" rel="noopener">小数</a>，一种是把有量纲表达式变为无量纲表达式。主要是为了<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/944504" target="_blank" rel="noopener">数据处理</a>方便提出来的，把数据<a href="https://baike.baidu.com/item/%E6%98%A0%E5%B0%84/20402621" target="_blank" rel="noopener">映射</a>到0～1范围之内处理，更加<a href="https://baike.baidu.com/item/%E4%BE%BF%E6%8D%B7/2966338" target="_blank" rel="noopener">便捷</a>快速</p><h5 id="GNN">GNN</h5><p>图神经网络是一种直接作用于图结构上的神经网络。GNN的一个典型应用是节点分类,本质上，图中的每个节点都与一个标签相关联，我们希望预测未标记节点的标签。</p><p>在节点分类问题中，每个节点v都可以用其特征x_v表示并且与已标记的标签t_v相关联。给定部分标记的图G，目标是利用这些标记的节点来预测未标记的节点标签。它通过学习得到每个节点的d维向量（状态）表示为h_v，同时包含其相邻节点的信息。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/GNN.png" alt=""></p><h4 id="论文的研究步骤或过程情况">论文的研究步骤或过程情况</h4><p>目前imageNet的单标签注释忽略了图像固有的多标签性质，所以导致目前的较为先进的分类器在imageNet上的表现不佳。而MS COCO和Open Images提供复杂的图片可以满足作者的需求，因此作者选择了这两个数据集。</p><p>在收集多标签注释上的难度是要大于单标签注释的，因此作者选择使用偏标记学习，通过众包平台收集偏标记标签是比较容易和扩展的。作者选择了两种策略来提升图片分类的表现，设计更好的模型架构和通过大量的标签数据进行学习。在收集多标签注释时由于难度大，因此作者使用了网络监督的方式自动生成标签，而这种方式生成的标签由于噪音和不详细的影响，会导致比较差的分类表现。因此需要对噪音进行学习。</p><p>在当前已存的方法对于大规模的数据集上不可扩展且无法微调卷机神经网络的情况下，论文提出了一种新的loss函数和方法来修补缺失的标签。第一步作者发现通过给定一个固定的标签预算，实验表明，部分注释所有图像比完全注释一个小子集要好。第二步论文中引入了一个损失函数，它通过利用标签比例信息来概括标准二元交叉熵损失。这个损失函数可以自动适应每张图片中已知标签的比例，并且在学习所有的标签时可以使用同样的训练设置。第三步则是预测丢失的标签，由于卷机神经网络对噪音非常敏感，所以模型的预测准确性较好，作者使用了一种基于GNNS的方法来明确模型在不同类别中的相关性。</p><p>作者说明了几种方法来进行丢失标签的学习，首先是在网络监督学习中广泛运用的比较简单的方法，将丢失的标签作为负面标签，这种方法的标准假设是查询的类别存在图像中而其他类别不存在，由于在初始化时许多正面标签被标记为负面标签导致这种方法的表现较差。第二种方法是使用二元关联，把每一个标签作为一个独立的二元分类器，但是这种方法当分类的数量增加时是不易扩展的，而且它忽视了标签和实例之间的相关性。第三种则是利用训练数据中标签的相关性将已提供标签信息转移到缺失的标签中，使用矩阵补全算法来填充缺失的标签，这种方法利用了标签和实例之间的相关性对标签矩阵进行低秩正则化来完成实例标签矩阵。引入了低秩经验风险最小化，使用了混合图来对标签依赖网络进行编码，学习分类之间的相关性来预测丢失的标签。</p><p>这几种方法都是不适用于深度卷机网络的，它们需要内村中的训练集来解决优化问题，所以不可能使用一种小批量策略来微调模型，而微调对于转移先前的预测训练架构又是至关重要的且一些方法是不可扩展的，因为他们需要解决大规模数据集难以处理的凸二次优化问题。因此作者提出了自己的模型来解决该问题。</p><h4 id="算法内容">算法内容</h4><h5 id="符号说明">符号说明</h5><p>C：分类的数量N：训练样本的数量</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/D.png" alt=""></p><p>I (i)为第i张图片，y (i) = [y (i) 1 , . . . , y (i) C ] 为第i张图片的C个分类</p><p>y (i) c = 1 (resp. −1 and 0) 1为确定，-1为丢失，0为未知</p><p>[y (1); . . . ; y (N) ] 为训练集的标签矩阵</p><p>fw为权重，x (i) = [x (i) 1 , . . . , x (i) C ] = fw(I (i) )为未sigmoid之前的第i张图片的输出</p><h5 id="Binary-cross-entropy-for-partial-labels">Binary cross-entropy for partial labels</h5><p>为了与类别数量无关，BCE 损失由类别数量归一化，导致了反向传播的梯度较小，通过对已知标签比例的标准化，作者提出了一种部分BCE损失。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/parBCE.png" alt=""></p><p>py ∈ [0, 1]是y中已知标签的比例，实例独立于已知标签数量，当图像标签的比例不固定时比较有用。</p><p>正规化函数g设计</p><p>通过标签比例标准化部分BCE的Loss函数，当全部标签显示时，为了使部分BCE和BCE的变现相同，作者提出了g函数</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/G.png" alt=""></p><p>通过给定 γ 值和给定比例的权重，可以找到对应的α 和β满足约束。γ 控制关于标签比例的归一化行为，对应的每个参数说明在后文。</p><h4 id="后续的阅读和编码在系统性学习玩GNN后进行。">后续的阅读和编码在系统性学习玩GNN后进行。</h4><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 偏标记学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模拟器后台挂机刷图小提示</title>
      <link href="/2021/06/03/mo-ni-qi-hou-tai-gua-ji-shua-tu-xiao-ti-shi/"/>
      <url>/2021/06/03/mo-ni-qi-hou-tai-gua-ji-shua-tu-xiao-ti-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="模拟器后台挂机刷图小提示">模拟器后台挂机刷图小提示</h3><h4 id="使用Spy-获取后台窗口类和标题名称，根据句柄信息对窗口进行截图。">使用Spy++获取后台窗口类和标题名称，根据句柄信息对窗口进行截图。</h4><pre class=" language-language-python"><code class="language-language-python">    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls,name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    width = right - left    height = bot - top    hWndDC = win32gui.GetWindowDC(hWnd)    # 创建设备描述表    mfcDC = win32ui.CreateDCFromHandle(hWndDC)    # 创建内存设备描述表    saveDC = mfcDC.CreateCompatibleDC()    # 创建位图对象准备保存图片    saveBitMap = win32ui.CreateBitmap()    # 为bitmap开辟存储空间    saveBitMap.CreateCompatibleBitmap(mfcDC, width, height)    # 将截图保存到saveBitMap中    saveDC.SelectObject(saveBitMap)    # 保存bitmap到内存设备描述表    saveDC.BitBlt((0, 0), (width, height), mfcDC, (0, 0), win32con.SRCCOPY)    signedIntsArray = saveBitMap.GetBitmapBits(True)    im_opencv = numpy.frombuffer(signedIntsArray, dtype='uint8')    im_opencv.shape = (height, width, 4)    cv2.cvtColor(im_opencv, cv2.COLOR_BGRA2RGB)</code></pre><h4 id="模板图像与当前获取的截图进行相似度匹配，匹配成功进行刷图完成提示">模板图像与当前获取的截图进行相似度匹配，匹配成功进行刷图完成提示</h4><pre class=" language-language-python"><code class="language-language-python">    template = cv2.imread(path)    H1 = cv2.calcHist([im_opencv], [1], None, [256], [0, 256])    H1 = cv2.normalize(H1, H1, 0, 1, cv2.NORM_MINMAX, -1)    H2 = cv2.calcHist([template], [1], None, [256], [0, 256])    H2 = cv2.normalize(H2, H2, 0, 1, cv2.NORM_MINMAX, -1)    # 利用compareHist（）进行比较相似度    similarity = cv2.compareHist(H1, H2, 0)</code></pre><h4 id="首次进行循环不断截图获取刷图的时长，根据时长进行优化截图匹配时间段">首次进行循环不断截图获取刷图的时长，根据时长进行优化截图匹配时间段</h4><pre class=" language-language-python"><code class="language-language-python">def getStep(cls,name,path):    step = 0    flag = True    while(flag):        similarity = img(cls,name,path)        if(similarity > 0.9):            flag = False            print(1)        else:            step = step + 1    return step</code></pre><h4 id="根据首次时间段运行当前代码进行提示">根据首次时间段运行当前代码进行提示</h4><pre class=" language-language-python"><code class="language-language-python">def turn(cls,name,path):    similarity = img(cls, name, path)    if(similarity > 0.9):        print(1)</code></pre><h4 id="完整代码">完整代码</h4><pre class=" language-language-python"><code class="language-language-python">#对后台窗口截图import win32gui, win32ui, win32conimport cv2import numpydef img(cls,name,path):    # 获取后台窗口的句柄，注意后台窗口不能最小化    hWnd = win32gui.FindWindow(cls,name)    # 获取句柄窗口的大小信息    left, top, right, bot = win32gui.GetWindowRect(hWnd)    width = right - left    height = bot - top    hWndDC = win32gui.GetWindowDC(hWnd)    # 创建设备描述表    mfcDC = win32ui.CreateDCFromHandle(hWndDC)    # 创建内存设备描述表    saveDC = mfcDC.CreateCompatibleDC()    # 创建位图对象准备保存图片    saveBitMap = win32ui.CreateBitmap()    # 为bitmap开辟存储空间    saveBitMap.CreateCompatibleBitmap(mfcDC, width, height)    # 将截图保存到saveBitMap中    saveDC.SelectObject(saveBitMap)    # 保存bitmap到内存设备描述表    saveDC.BitBlt((0, 0), (width, height), mfcDC, (0, 0), win32con.SRCCOPY)    signedIntsArray = saveBitMap.GetBitmapBits(True)    im_opencv = numpy.frombuffer(signedIntsArray, dtype='uint8')    im_opencv.shape = (height, width, 4)    cv2.cvtColor(im_opencv, cv2.COLOR_BGRA2RGB)    # cv2.imwrite("im_opencv.jpg",im_opencv,[int(cv2.IMWRITE_JPEG_QUALITY), 100]) #保存    # cv2.namedWindow('im_opencv') #命名窗口    # cv2.imshow("im_opencv",im_opencv) #显示    template = cv2.imread(path)    H1 = cv2.calcHist([im_opencv], [1], None, [256], [0, 256])    H1 = cv2.normalize(H1, H1, 0, 1, cv2.NORM_MINMAX, -1)  #    H2 = cv2.calcHist([template], [1], None, [256], [0, 256])    H2 = cv2.normalize(H2, H2, 0, 1, cv2.NORM_MINMAX, -1)    # 利用compareHist（）进行比较相似度    similarity = cv2.compareHist(H1, H2, 0)    cv2.waitKey(0)    cv2.destroyAllWindows()    # 内存释放    win32gui.DeleteObject(saveBitMap.GetHandle())    saveDC.DeleteDC()    mfcDC.DeleteDC()    win32gui.ReleaseDC(hWnd, hWndDC)    return similaritydef getStep(cls,name,path):    step = 0    flag = True    while(flag):        similarity = img(cls,name,path)        if(similarity > 0.9):            flag = False            print(1)        else:            step = step + 1    return stepdef turn(cls,name,path):    similarity = img(cls, name, path)    if(similarity > 0.9):        print(1)</code></pre><h4 id="后续补充完善">后续补充完善</h4><p>根据不同的关卡设置不同的时间段。根据不同游戏和玩法选择对应的模板。补上语音提示。</p><p>简单加一个刷图完成模拟鼠标点击操作，如果不封我，我就直接写脚本，如果封我，那我就不写了。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 脚本 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>青春纪念册</title>
      <link href="/2021/05/28/qing-chun-ji-nian-ce/"/>
      <url>/2021/05/28/qing-chun-ji-nian-ce/</url>
      
        <content type="html"><![CDATA[<h3 id="青春纪念册">青春纪念册</h3><video id="video" controls="" preload="none">    <source id="mp4" src="https://media.githubusercontent.com/media/SprBoot/hexo_image/refs/heads/master/%E9%9D%92%E6%98%A5%E7%BA%AA%E5%BF%B5%E5%86%8C.mp4" type="video/mp4"></video><p>视频来源自https://www.bilibili.com/video/BV19K4y1G759?t=6189，防止和谐，自己缓存留作纪念以后重温。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 影视 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 老友记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习笔记三</title>
      <link href="/2021/05/21/shen-du-xue-xi-bi-ji-san/"/>
      <url>/2021/05/21/shen-du-xue-xi-bi-ji-san/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习笔记三">深度学习笔记三</h3><h4 id="逻辑回归的计算过程-Discriminative">逻辑回归的计算过程 Discriminative</h4><h5 id="Step1-选取Loss-function">Step1 选取Loss function</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step1loss.png" alt=""></p><h5 id="Step2-选取变量计算最小值">Step2 选取变量计算最小值</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step2min.png" alt=""></p><p>为了方便gradient计算最小值，将w，b的最大值公式转换为最小值进行计算。对function进行ln化。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step2ln.png" alt=""></p><p>以yhat进行区分分类并对ln的function进行交叉熵。</p><h5 id="Step3-gradient">Step3 gradient</h5><p>对w进行微分，其中w由上篇笔记分析线性回归而来。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step3w.png" alt=""></p><p>微分后对w进行gradient的公式为</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step3gradient.png" alt=""></p><h5 id="区分">区分</h5><p>在使用逻辑回归时，yhat与预测的值的最大差距来计算较为合理。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step3class.png" alt=""></p><p>而square对于yhat和预测的差距在误差较大时仍为0，导致得到的loss function较为平缓，在进行gradient时下降较慢。</p><p>判别模型对于数据的量的敏感度较高，而生成模型对于数据量的敏感度不高。</p><h4 id="Multi-class-Classification-3-classes">Multi-class Classification(3 classes)</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/moreclass.png" alt=""></p><p>计算交叉熵</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/jiaocha.png" alt=""></p><h4 id="逻辑回归的限制">逻辑回归的限制</h4><p>对于预测的数据在回归预测中处于对角线时，逻辑回归就无法进行预测，取不到合适的值来进行区分。</p><p>对于这种方法可以通过合适的函数或者方法进行数学计算，将对角线的处的输入值进行分类到相似区域。但是函数的获取较难，机器学习无法产生或产生的代价过高。</p><p>可以通过对输入特征串联的方法得到新的输入特征，新的输入特征可以在相似区域的情况下在进行逻辑回归。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习第二次编码</title>
      <link href="/2021/05/20/shen-du-xue-xi-di-er-ci-bian-ma/"/>
      <url>/2021/05/20/shen-du-xue-xi-di-er-ci-bian-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习第二次编码">深度学习第二次编码</h3><h4 id="使用爬虫爬取明日方舟干员属性，进行逻辑回归预测">使用爬虫爬取明日方舟干员属性，进行逻辑回归预测</h4><pre class=" language-language-python"><code class="language-language-python"># author:dinggc# date:2021/5/20 下午12:03import data as gd# w之前的所有干员的数据 分类为重装和其他职业 不考虑星级的影响下# 输入特征为6个，分别为初始特征和费用。生命 攻击 防御 法抗# 爬取数据# print(gd.getData('https://wiki.biligame.com/arknights/%E5%B9%B2%E5%91%98%E6%95%B0%E6%8D%AE%E8%A1%A8'))# μ和ε在训练数据上计算高斯分布的最大可能性import pymysql# 建立数据库连接conn = pymysql.Connect(host='127.0.0.1',                       port=3306,                       user='root',                       passwd='dgc123.com',                       db='python',                       charset='utf8')# 获取游标对象cursor = conn.cursor(pymysql.cursors.DictCursor)# 插入数据语句query = """SELECT arknights.life,        arknights.attack,arknights.defense,arknights.resistance,arknights.profession FROM arknights WHERE id < 978"""cursor.execute(query)list = cursor.fetchall()# 计算重装和其他数目# print(gd.calcCount(list,'重装'))# print(gd.calcCount(list,'其他'))# 17# 133zCount = gd.calcCount(list,1)oCount = gd.calcCount(list,0)# 计算p(重装) 0.12781954887218044# print(gd.calcSP(list,zCount))pZ = gd.calcSP(list,zCount)# 计算p(其他) 0.8721804511278195#print(gd.calcSP(list,oCount))pO = gd.calcSP(list,oCount)print(gd.calμ(list,zCount))μStarZ = gd.calμ(list,int(zCount))# 计算重装# 重装μ的最大可能性 [6933.294117647059, 1889.5882352941176, 908.7058823529412, 29.41176470588235]的转置# print(gd.calε(list,μStarZ,zCount,1))# εStarZ =gd.calε(list,μStarZ,zCount,1)print(gd.calε(list,μStarZ,zCount))εStarZ = gd.calε(list,μStarZ,zCount)# 重装ε的最大可能性# [[1.21203507e+06 3.75984385e+07 7.80255296e+07 2.86753473e+08]#  [3.30847431e+05 1.02180807e+07 2.12961342e+07 7.80255296e+07]#  [1.59228689e+05 4.95178622e+06 1.02180807e+07 3.75984385e+07]#  [5.45824344e+03 1.59228689e+05 3.30847431e+05 1.21203507e+06]]# 计算一般print(gd.calμ(list,int(oCount)))μStarO = gd.calμ(list,int(oCount))# 其他μ的最大可能性 [1016.0862068965517, 276.92241379310343, 133.17241379310346, 4.310344827586207]的转置# print(gd.calε(list,μStarO,oCount))# 其他ε的最大可能性# [[-1.47530265e+02  1.76302963e+04  1.35486771e+04  1.17346441e+05]#  [ 3.62113940e+01  5.64593090e+02  8.26326463e+03  1.35486771e+04]#  [ 3.55354873e+01  5.82516993e+03  5.64593090e+02  1.76302963e+04]#  [ 4.57816536e+01  3.55354873e+01  3.62113940e+01 -1.47530265e+02]]# εStarO = gd.calε(list,μStarO,oCount,1)εStarO = gd.calε(list,μStarO,oCount)print(gd.calε(list,μStarO,oCount))# 计算高斯分布并计算准确率# 测试数据中的数据为7,预测应该有的数据为19,正确率为36.84210526315789%queryTest = """SELECT arknights.initial,arknights.perfect,arknights.life,        arknights.attack,arknights.defense,arknights.resistance,arknights.profession FROM arknights WHERE id > 977"""cursor.execute(queryTest)test = cursor.fetchall()# print(gd.calcResult(μStarZ,εStarZ,μStarO,εStarO,pZ,pO,test))matrix = gd.matrix_add(gd.calcSame(εStarZ,pZ),gd.calcSame(εStarO,pO))print(gd.calcResult(μStarZ,matrix,μStarO,matrix,pZ,pO,test))cursor.close()conn.commit()conn.close()</code></pre><p>使用到的工具函数</p><pre class=" language-language-python"><code class="language-language-python"># author:dinggc# date:2021/5/20 下午12:23import requests    #requests是HTTP库import refrom bs4 import BeautifulSoup as bs   #bs:通过解析文档为用户提供需要抓取的数据import osimport mathimport ioimport sysimport pymysqlimport numpy as npsys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8') #改变标准输出的默认编码#我们开始利用requests.get（）来获取网页并利用bs4解析网页：def getData(src):    # 建立数据库连接    conn = pymysql.Connect(host='127.0.0.1',                           port=3306,                           user='root',                           passwd='dgc123.com',                           db='python',                           charset='utf8')    # 获取游标对象    cursor = conn.cursor()    # 插入数据语句    query = """insert into arknights (initial,perfect,life,attack,defense,resistance,username,profession) values (%s,%s,%s,%s,%s,%s,%s,%s)"""    html = requests.get(src).content    # requests.get(src)返回的是状态码<Response [200]>，加上.content以字节形式（二进制返回数据。   和前端一样，分为get post等    soup = bs(html,'lxml')   # lxml解析器解析字节形式的数据，得到完整的类似页面的html代码结构的数据    introductions = soup.find_all("tr", class_="divsort")    for introduction in introductions:      name = introduction.find_all("td")[1].find_all("a")[0].text.replace(' ','')      data = requests.get('https://wiki.biligame.com/arknights/' + name).content      dataSoup = bs(data,'lxml')      profession = dataSoup.find_all("table", class_="wikitable")[1].find_all("tr")[0].find_all("td")[0].text.replace(' ','')      life = dataSoup.find_all("table", class_="wikitable")[3].find_all("tr")[1].find_all("td")[1].text.replace(' ','')      attack = dataSoup.find_all("table", class_="wikitable")[3].find_all("tr")[2].find_all("td")[1].text.replace(' ','')      defense = dataSoup.find_all("table", class_="wikitable")[3].find_all("tr")[3].find_all("td")[1].text.replace(' ','')      resistance = dataSoup.find_all("table", class_="wikitable")[3].find_all("tr")[4].find_all("td")[1].text.replace(' ','')      values = (int(float(life)),int(float(attack)),int(float(defense)),                int(float(resistance)),str(name),str(profession))      cursor.execute(query,values)    cursor.close()    conn.commit()    conn.close()    return "success"def calμ(list,count):    μ = []    sumLife = sumAttack = sumDefense = sumResistance = 0    for item in list:        sumLife = sumLife + item['life']        sumAttack = sumAttack + item['attack']        sumDefense = sumDefense + item['defense']        sumResistance = sumResistance + item['resistance']    μ.append(sumLife / count)    μ.append(sumAttack / count)    μ.append(sumDefense / count)    μ.append(sumResistance / count)    return μdef calε(list,μStar,count):    matrix = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]    for item in list:      front = []      front.append(item['life'] - μStar[0])      front.append(item['attack'] - μStar[1])      front.append(item['defense'] - μStar[2])      front.append(item['resistance'] - μStar[3])      matrix = matrix_add(matrix,OneMult(front,front[::-1]))    return np.multiply(matrix,np.array([[1/count,1/count,1/count,1/count],[1/count,1/count,1/count,1/count],[1/count,1/count,1/count,1/count],[1/count,1/count,1/count,1/count]]))def calcCount(list,flag):    sum = 0    for item in list:        if('重装' == item['profession']):            sum = sum + 1    if (flag == 1):        return sum    else:        return len(list) - sumdef calcSP(list,count):    return count / len(list)def calcResult(μStarZ,εStarZ,μStarO,εStarO,pZ,pO,test):    sum = right = 0    for item in test:      resultZ = calcGauss(μStarZ,εStarZ,item)      resultO = calcGauss(μStarO,εStarO,item)      result = (resultZ * pZ) / (resultZ * pZ + resultO + pO)      if(item['profession'] == '重装'):        sum = sum + 1      if(result > 0.5):        right = right + 1    return '测试数据中的数据为{0},预测应该有的数据为{1},正确率为{2}{3}'.format(sum,right,sum / right * 100,'%')def calcGauss(μStar,εStar,item):    front = []    front.append(item['life'] - μStar[0])    front.append(item['attack'] - μStar[1])    front.append(item['defense'] - μStar[2])    front.append(item['resistance'] - μStar[3])    εStarTemp = np.linalg.inv(εStar)    temp = np.dot(front,np.dot(εStarTemp,front))    result = math.e ** (temp * (-0.5))    other = (1 / (pow(math.pi * 2, 2))) * (1 / pow(np.linalg.det(εStar), 0.5))    return result * otherdef matrix_add(matrix1,matrix2):    total_element = [matrix1[i][j] + matrix2[i][j] for i in range(len(matrix1)) for j in range(len(matrix1))]    new_matrix = [total_element[x:x+len(matrix1)] for x in range(0,len(total_element),len(matrix1))]    return new_matrixdef OneMult(list1,list2):    result = []    for i in list1:        temp = []        for j in list2:            temp.append(i * j)        result.append(temp)    return resultdef calcSame(list,p):    return np.multiply(list,np.array([[p,p,p,p],[p,p,p,p],[p,p,p,p],[p,p,p,p]]))</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习笔记二</title>
      <link href="/2021/05/19/shen-du-xue-xi-bi-ji-er/"/>
      <url>/2021/05/19/shen-du-xue-xi-bi-ji-er/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习笔记二">深度学习笔记二</h3><h4 id="New-Optimization-for-Deep-Learning">New Optimization for Deep Learning</h4><p>On-line：one pair of (xt,yt) at a time step (单输入)<br>Off-line：pour all into the model(全输入)</p><p>SGD：选取初始位置，计算gradient，根据结果向更低的loss方向走，迭代计算最低的loss值</p><p>SGDM：计算移动的距离加v，v0 = 0。较为稳定。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/step.png" alt=""></p><p>使用v的作用防止θ计算的结果为0，不再移动和有更低的点防止不再向前移动。</p><p>Adagrad：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/adagrad.png" alt=""></p><p>分母为之前计算的所有gradient的平方和，即走的比较急的时候gradient比较大，走的时候使用较小的learning rate，而走的比较小的时候使用较大的learning rate。</p><p>RMSProp：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/rmsprop.png" alt=""></p><p>在adagrad中如果gradient较大，那么计算的分母较小，下降很慢，而rmsprop保证不会因过大的gradient而下降过慢。而为了防止不再移动和向前移动的问题，可以结合SGDM使用，即Adam。</p><p>Adam：训练较快。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/adam.png" alt=""></p><p>SWATS：begin with Adam，end with SGDM</p><p>Towards improving Adam：AMSGrad 只考虑了learning rate较大的情况</p><p>当gradient未突变或gradient一直较小时，移动的距离很慢，移动方向不定，直到gradient突变，才会确定方向。vt =  max(vt-1,vt)，选取最大的gradient即可(记住过去较大的gradient)。也会走回头路。</p><p>Towards improving Adam：AdaBound</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/adabound.png" alt=""></p><p>Towards improving SGDM：LR range test，Cyclical LR，SGDR，One-cycle LR</p><p>Adam with warm-up：RAdam</p><p>即在方向不确定即vt不确定时，移动的距离短，当vt确定平缓时，移动的距离大。使用rt来判断方向情况。rt与gradient无关。先SGDM在Radam。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/radam1.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/radam2.png" alt=""></p><p>Lookahead：universal wrapper for all optimizers 每k步回退检查</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/look.png" alt=""></p><p>Look into the future：NAG</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/nag.png" alt=""></p><p>Nadam：预测</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/nadam.png" alt=""></p><p>Adamw and SGDW：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/w.png" alt=""></p><h4 id="Classification：Probabilistic-Generative-Model">Classification：Probabilistic Generative Model</h4><p>案例：对于明日方舟干员属性预测分类</p><p>Training data for Classification：夕之前的干员</p><h5 id="Ideal-Alternatives">Ideal Alternatives</h5><p>function：对于训练得到的g函数进行预测，g(x) &gt; 0 输出class1，反之输出class2<br>Loss function：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/loss.png" alt=""></p><p>Find the best function</p><h5 id="Generative-model">Generative model</h5><p>概率问题，即输入的值为从预测结果中提取的概率。对于二元分类，即训练每个单元的概率和从单元中提取输入值x的概率。</p><p>方舟干员的各项属性作为feature。</p><p>从训练数据中训练得到高斯分布的μ和ε。根据输入的特征x通过高斯计算得出机率。</p><h5 id="Maximum-Likelihood">Maximum Likelihood</h5><p>根据给定的μ和ε在训练数据上计算高斯分布的最大可能性。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/likelihood.png" alt=""></p><p>找出最合适的μ和ε。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/maxium.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/gailv.png" alt=""></p><p>根据概率公式以及求的的高斯分布进行计算。</p><h5 id="优化">优化</h5><p>不同的高斯分布选用同样的ε，因为当特征较多时，计算的结果较大，容易产生过拟合。</p><p>Loss function</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/newLoss.png" alt=""></p><h5 id="选用同样的ε呈现线性分界的数学原理">选用同样的ε呈现线性分界的数学原理</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/math1.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/math2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/math3.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习第一次学习编码</title>
      <link href="/2021/04/29/shen-du-xue-xi-di-yi-ci-xue-xi-bian-ma/"/>
      <url>/2021/04/29/shen-du-xue-xi-di-yi-ci-xue-xi-bian-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习第一次学习编码">深度学习第一次学习编码</h3><p>由于受到硬件设备的缘故，故得出的结果并不准确，开学之后再测，本次记录仅仅对之前学习内容的总结和编码。</p><h4 id="通过韩服lol15分钟经济差预测本场时长">通过韩服lol15分钟经济差预测本场时长</h4><p>本来应该使用正规比赛数据，但是我没找到接口。于是使用rank的接口来编码一下整个线性回归的计算过程。最近很忙，因此只是计算了rank的15分钟经济差的一次和二次型，对于更高次或者其他特征(英雄，熟练度，小龙等)以后有时间再补。</p><h5 id="整理的十场数据">整理的十场数据</h5><pre class=" language-language-python"><code class="language-language-python"># 使用数据(15分钟经济差)# 1.8755 1123    2.1955 1682    3.8538 1151   4.2793 1518   5.13468 928   6.7521 913   7.11818 1118   8.12918 928   9.1842 1671   10.2795 1535</code></pre><h5 id="根据数据估计权重和偏差得到function函数">根据数据估计权重和偏差得到function函数</h5><pre class=" language-language-python"><code class="language-language-python"># 可能得到的函数 将时长放大10倍便于计算# min W -1.4270724029380903 max W -0.6708197905381004# min B 17224.94131455399 max B 19338.66736621196</code></pre><h5 id="计算一次型Loss">计算一次型Loss</h5><pre class=" language-language-python"><code class="language-language-python">ca.calLoss([16710,16820,15180,15350,9130,11510,11230,11180,9280,9280],-1.42,18338,[1842,1955,2793,2795,7521,8538,8755,11818,12918,13468],[],[],0.01,1800,342903024,0)</code></pre><pre class=" language-language-python"><code class="language-language-python">def calLoss(list,w,b,list2,listW,listB,step,stepB,oldLoss,number):    loss = 0    for (m,n) in zip(list,list2):      loss = ma.pow((m - (b + w * n)),2) + loss    print("----------------------------------------------Loss的值{}".format(loss))    print("W的值{}".format(w))    print("B的值{}".format(b))    resultW = 0    resultB = 0    stW = 0    stB = 0    for (i,z) in zip(list,list2):      resultW = 2 * (i - (b + w * z)) * (-1) * (z) + resultW      resultB = 2 * (i - (b + w * z)) * (-1) + resultB    listW.append(resultW)    listB.append(resultB)    for j in listW:      stW = stW + pow(j,2)    for k in listB:      stB = stB + pow(k,2)    print("偏微分W的值{}".format(resultW))    print("偏微分B的值{}".format(resultB))    print("w的学习率{}".format(((step/ma.sqrt(len(listW))) / ma.sqrt(stW / len(listW)))))    print("b的学习率{}".format(((stepB/ma.sqrt(len(listB))) / ma.sqrt(stB / len(listB)))))    number = number + 1    print("----------------------------------------------递归次数{}".format(number))    if loss < oldLoss:      calLoss(list, (w - ((step / ma.sqrt(len(listW))) / ma.sqrt(stW / len(listW))) * resultW),        (b - ((stepB / ma.sqrt(len(listB))) / ma.sqrt(stB / len(listB))) * resultB), list2, listW, listB, step,stepB,loss,number)</code></pre><p><strong>硬件限制下的结果</strong></p><pre class=" language-language-python"><code class="language-language-python"># w -0.6160429356395202 b 17027.335670323457 递归次数27295</code></pre><h5 id="计算二次型Loss">计算二次型Loss</h5><pre class=" language-language-python"><code class="language-language-python">ca.calSecondLoss([16710,16820,15180,15350,9130,11510,11230,11180,9280,9280],-2.2399005635135905,0.001108632563302861,20274,[1842,1955,2793,2795,7521,8538,8755,11818,12918,13468],[],[],[],0.1,0.001,1800,937810727010,0)</code></pre><pre class=" language-language-python"><code class="language-language-python">def calSecondLoss(list,w1,w2,b,list2,listW1,listW2,listB,step,step2,stepB,oldLoss,number):    loss = 0    for (m,n) in zip(list,list2):      loss = ma.pow((m - (b + w1 * n + w2 * ma.pow(n,2))),2) + loss    print("----------------------------------------------Loss的值{}".format(loss))    print("W1的值{}".format(w1))    print("W2的值{}".format(w2))    print("B的值{}".format(b))    resultW1 = 0    resultW2 = 0    resultB = 0    stW1 = 0    stW2 = 0    stB = 0    for (i,z) in zip(list,list2):      resultW1 = 2 * (i - (b + w1 * z + w2 * ma.pow(z,2))) * (-1) * z + resultW1      resultW2 = 2 * (i - (b + w1 * z + w2 * ma.pow(z, 2))) * (-1) * (ma.pow(z, 2)) + resultW2      resultB = 2 * (i - (b + w1 * z + w2 * ma.pow(z, 2))) * (-1) + resultB    listW1.append(resultW1)    listW2.append(resultW2)    listB.append(resultB)    for j in listW1:      stW1 = stW1 + pow(j,2)    for p in listW2:      stW2 = stW2 + pow(p,2)    for k in listB:      stB = stB + pow(k,2)    print("偏微分W1的值{}".format(resultW1))    print("偏微分W2的值{}".format(resultW2))    print("偏微分B的值{}".format(resultB))    print("w1的学习率{}".format(((step/ma.sqrt(len(listW1))) / ma.sqrt(stW1 / len(listW1)))))    print("w2的学习率{}".format(((step2/ma.sqrt(len(listW2))) / ma.sqrt(stW2 / len(listW2)))))    print("b的学习率{}".format(((stepB/ma.sqrt(len(listB))) / ma.sqrt(stB / len(listB)))))    print(w1 - ((step / ma.sqrt(len(listW1))) / ma.sqrt(stW1 / len(listW1))))    print(w2 - ((step2 / ma.sqrt(len(listW2))) / ma.sqrt(stW2 / len(listW2))))    number = number + 1    print("----------------------------------------------递归次数{}".format(number))    if loss < oldLoss:      calSecondLoss(list, (w1 - ((step / ma.sqrt(len(listW1))) / ma.sqrt(stW1 / len(listW1))) * resultW1),(w2 - ((step2 / ma.sqrt(len(listW2))) / ma.sqrt(stW2 / len(listW2))) * resultW2),        (b - ((stepB / ma.sqrt(len(listB))) / ma.sqrt(stB / len(listB))) * resultB), list2, listW1,listW2, listB,step,step2,stepB,loss,number)</code></pre><p><strong>硬件限制下的结果</strong></p><pre class=" language-language-python"><code class="language-language-python"> w1 -1.5788327598874479 w2 0.00006534227083065703 b 19239.004622493012 递归 20404</code></pre><h5 id="Regularization">Regularization</h5><pre class=" language-language-python"><code class="language-language-python">ca.calSecondLossWithExtraParam([16710,16820,15180,15350,9130,11510,11230,11180,9280,9280],-2.2399005635135905,0.001108632563302861,1,20274,[1842,1955,2793,2795,7521,8538,8755,11818,12918,13468],[],[],[],0.1,0.001,1800,937810727010,0)</code></pre><pre class=" language-language-python"><code class="language-language-python">def calSecondLossWithExtraParam(list,w1,w2,b,λ,list2,listW1,listW2,listB,step,step2,stepB,oldLoss,number):    loss = 0    for (m,n) in zip(list,list2):      loss = ma.pow((m - (b + w1 * n + w2 * ma.pow(n,2))),2) + λ * (pow(w1,2) + pow(w2,2)) + loss    print("----------------------------------------------Loss的值{}".format(loss))    print("W1的值{}".format(w1))    print("W2的值{}".format(w2))    print("B的值{}".format(b))    resultW1 = 0    resultW2 = 0    resultB = 0    stW1 = 0    stW2 = 0    stB = 0    for (i,z) in zip(list,list2):      resultW1 = 2 * (i - (b + w1 * z + w2 * ma.pow(z,2))) * (-1) * z + 2 * λ * w1 + resultW1      resultW2 = 2 * (i - (b + w1 * z + w2 * ma.pow(z, 2))) * (-1) * (ma.pow(z, 2)) + 2 * λ * w2 + resultW2      resultB = 2 * (i - (b + w1 * z + w2 * ma.pow(z, 2))) * (-1) + resultB    listW1.append(resultW1)    listW2.append(resultW2)    listB.append(resultB)    for j in listW1:      stW1 = stW1 + pow(j,2)    for p in listW2:      stW2 = stW2 + pow(p,2)    for k in listB:      stB = stB + pow(k,2)    print("偏微分W1的值{}".format(resultW1))    print("偏微分W2的值{}".format(resultW2))    print("偏微分B的值{}".format(resultB))    print("w1的学习率{}".format(((step/ma.sqrt(len(listW1))) / ma.sqrt(stW1 / len(listW1)))))    print("w2的学习率{}".format(((step2/ma.sqrt(len(listW2))) / ma.sqrt(stW2 / len(listW2)))))    print("b的学习率{}".format(((stepB/ma.sqrt(len(listB))) / ma.sqrt(stB / len(listB)))))    number = number + 1    print("----------------------------------------------递归次数{}".format(number))    if loss < oldLoss:      calSecondLossWithExtraParam(list, (w1 - ((step / ma.sqrt(len(listW1))) / ma.sqrt(stW1 / len(listW1))) * resultW1),(w2 - ((step2 / ma.sqrt(len(listW2))) / ma.sqrt(stW2 / len(listW2))) * resultW2),        (b - ((stepB / ma.sqrt(len(listB))) / ma.sqrt(stB / len(listB))) * resultB),1, list2, listW1,listW2, listB,step,step2,stepB,loss,number)</code></pre><p><strong>硬件限制下的结果</strong></p><pre class=" language-language-python"><code class="language-language-python">w1 -1.4104840997551048 w2 0.000006414564199538763 B 18875.117024219377</code></pre><h5 id="测试数据">测试数据</h5><pre class=" language-language-python"><code class="language-language-python"># 第二组测试数据# 1.5256 1135    2.846 1184     3.6595 1180   4.6920 922    5.3711 2107   6.2434 1340  7.6908 963     8.3249 1343   9.4805 1688   10.2670 1568</code></pre><pre class=" language-language-python"><code class="language-language-python">first erro:281.1950413121548second erro:261.33671410611953regular erroe:239.89150059024587</code></pre><h5 id="额外计算方法">额外计算方法</h5><pre class=" language-language-python"><code class="language-language-python">def calcFirstTest(list,list2,w,b):    sum = 0    for (i,j) in zip(list,list2):        sum = sum + abs((w * i + b) / 10 - j)    print("first erroecode:{}".format(sum / len(list)))def calcSecondTest(list,list2,w1,w2,b):    sum = 0    for (i,j) in zip(list,list2):        sum = sum + abs((w2 * ma.pow(i,2) + w1 * i + b) / 10 - j)    print("second erroecode:{}".format(sum / len(list)))def calSecond(list1,list2,list3):    number1 = ma.pow(list3[0],2) / ma.pow(list2[0],2)    list2[1] = list2[1] * number1    first = list2[0] * number1    tAT = list2[1] - list3[1]    tATF = first - list3[0]    tATB = number1 - 1    number2 = ma.pow(list3[0],2) / ma.pow(list1[0],2)    list1[1] = list1[1] * number2    second = list1[0] * number2    oAT = list1[1] - list3[1]    oATF = second - list3[0]    oATB = number2 - 1    number3 = oATF / tATF    last = tAT * number3 - oAT    c = last / ((tATB * number3) - oATB)    b = (oAT - (oATB * c)) / oATF    a = (list3[1] - c - list3[0] * b) / pow(list3[0],2)    print("{0}x2 + {1}x + {2}".format(a,b,c))</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV学习笔记四</title>
      <link href="/2021/04/21/opencv-xue-xi-bi-ji-si/"/>
      <url>/2021/04/21/opencv-xue-xi-bi-ji-si/</url>
      
        <content type="html"><![CDATA[<h3 id="OpenCV学习笔记四">OpenCV学习笔记四</h3><h4 id="Fast和ORB算法">Fast和ORB算法</h4><h5 id="Fast算法">Fast算法</h5><p><strong>原理：</strong></p><p>Fast是一种用于角点检测的算法，该算法的原理是取图像中检测点，以该点为圆心的周围邻域内像素点判断检测点是否为角点，通俗的讲就是若一个像素周围有一定数量的像素与该点像素值不同，则认为其为角点。</p><p><strong>基本流程：</strong></p><p>1.在图像中选取一个像素点p，来判断它是不是关键点。Ip等于像素点p的灰度值。<br>2.以r为半径画圆，覆盖p点周围的M个像素，通常情况下，设置r=3，则M=16。<br>3.设置一个阈值t，如果在这16个像素点中存在n个连续像素点的灰度值都高于Ip+t，或者低于Ip-t，那么像素点p就被认为是一个角点。<br>4.由于在检测特征点时是需要对图像中所有的像素点进行检测，然后图像中的绝大多数点都不是特征点，如果对每个像素点都进行上述的检测过程，那显然会浪费许多时间，因此采用一种进行非特征点判别的方法：首先对候选点的周围每个90度的点：1，9，5，13进行测试。如果p是角点，那么这四个点中至少有3个要符合阈值要求，否则直接剔除。对保留下来的点在继续进行测试。</p><p>虽然这个检测器的效率很高，但它有以下几条缺点：</p><p>获得的候选点比较多。<br>特征点的选取不是最优的，因为它的效果取决于要解决的问题和角点的分布情况。<br>进行非特征点判别时大量的点被丢弃。<br>检测到的很多特征点都是相邻的。</p><p>前三个问题可以通过机器学习的方法解决，最后一个问题可以使用非最大值抑制的方法解决。</p><p><strong>机器学习的角点检测器</strong></p><p>1.选择一组训练图片<br>2.使用Fast算法找出每幅图像的特征点，将其周围的16个像素存储构成一个向量p。<br>3.每一个特征点的16个像素点都属于下列三类中的一种<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sab.PNG" alt=""><br>4.根据这些像素点的分类，特征向量p也被分为3个子集：Pd，Ps，Pb<br>5.定义一个新的布尔变量Kp，如果p是角点就设置为True，如果不是就设置为false。<br>6.利用特征值向量p，目标值是$K_p$，训练ID3树。<br>7.将构建好的决策树运用于其他图像的快速检测。</p><p><strong>非极大值抑制</strong></p><p>在筛选出来的候选角点中有很多是紧挨在一起的，需要通过非极大值抑制来消除这种影响。</p><p>为所有的候选角点都确定一个打分函数V，V的值可这样计算：先分别计算Ip于圆上16个点的像素值差值，取绝对值，再将这16个绝对值相加，就得到了V的值。最后比较毗邻候选角点的V值，把V值较小的候选点pass掉。</p><p>Fast算法的思想与我们对角点的直观认识非常接近，化繁为简。Fast算法比其他角点的检测算法快，但是在噪声较高时不够稳定，这需要设置合适的阈值。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python"># 实例化fastfast = cv.FastFeatureDetector_create(threshlod,nonmaxSuppression)# 利用fast.detect检测关键点，没有对应的关键点描述kp = fast.detect(grayImg,None)# 将关键点检测结果绘制在图像上，与在sift中是一样的cv.drawKeypoints(image,keypoints,outputimages,color,flags)</code></pre><p>threshold：阈值t，有默认值10<br>nonmaxSuppression：是否进行非极大值抑制，默认值true<br>grayImg：进行关键点检测的图像，注意是灰度图像</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport numpy as npimport  matplotlib.pyplot as plt# 读取图像，并转换为灰度图像img = cv.imread('/Users/dinggongcheng/Downloads/lou.jpeg')# fast角点检测# 创建一个Fast对象，传入阈值，注意：可以处理彩色空间图像fast = cv.FastFeatureDetector_create(threshold=30)# 检测图像上的关键点kp = fast.detect(img,None)# 在图像上绘制关键点img2 = cv.drawKeypoints(img,kp,None,color=(0,0,255))# 输出默认参数print("threshold:{}".format(fast.getThreshold()))print("nonmaxSuppression:{}".format(fast.getNonmaxSuppression()))print("neighborhood:{}".format(fast.getType()))print("Total Keypoints with nonmaxSuppression:{}".format(len(kp)))# 关闭非极大值抑制fast.setNonmaxSuppression(0)kp = fast.detect(img,None)print("Total Keypoints with nonmaxSuppression:{}".format(len(kp)))# 绘制为进行非极大值抑制的结果img3 = cv.drawKeypoints(img,kp,None,color=(0,0,255))# 绘制图像fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(10,8),dpi=100)axes[0].imshow(img2[:,:,::-1])axes[0].set_title("setMax")axes[1].imshow(img3[:,:,::-1])axes[1].set_title("removeNonMax")plt.show()</code></pre><pre class=" language-language-bash"><code class="language-language-bash">threshold:30nonmaxSuppression:Trueneighborhood:2Total Keypoints with nonmaxSuppression:4194Total Keypoints with nonmaxSuppression:8710</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fast.png" alt=""></p><h5 id="ORB算法">ORB算法</h5><p><strong>原理：</strong></p><p>SIFT和SURF算法是受专利保护的，在使用它们时是要付费的，但是ORB不需要，它可以用来对图像中的关键点快速创建特征向量，并用这些特征向量来识别图像中的对象。</p><p><strong>ORB算法流程</strong></p><p>ORB算法结合了Fast和Brief算法，提出了构造金字塔，为Fast特征点添加了方向，从而使得关键点具有了尺度不变性和旋转不变性。具体流程为：</p><p>构造尺度金字塔，金字塔共有n层，与SIFT不同的是，每一层仅有一幅图像。第s层的尺度为σs = σs0。σ0是初始尺度，默认为1.2，原图在第0层。</p><p>第s层图像的大小：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/s1.PNG" alt=""></p><p>在不同的尺度上利用Fast算法检测特征点，采用Harris角点响应函数，根据角点的响应值排序，选取前N个特征点，作为本尺度的特征点。<br>计算特征点的主方向，计算以特征点为圆心，半径为r的圆形邻域内的灰度质心位置，将从特征点位置到质心位置的方向做特征点的主方向。</p><p>计算方法如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/s2.PNG" alt=""></p><p>质心位置：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/s3.PNG" alt=""></p><p>主方向：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/s4.PNG" alt=""></p><p>为了解决旋转不变性，将特征点的邻域旋转到主方向上利用Brief算法构建特征描述符，至此就得到了ORB的特征描述向量。</p><p><strong>BRIEF算法</strong></p><p>BRIEF算法是一种特征描述子提取算法，并非特征点的提取算法，一种生成二值化描述子的算法，不提取代价低，匹配只需要使用简单的汉明距离利用比特之间的异或操作就可以完成。因此，时间代价低，空间代价低，效果还挺好是最大的优点。</p><p>算法的步骤如下：</p><p>1.图像滤波：原始图像中存在噪声时，会对结果产生影响，所以需要对图像进行滤波，去除部分噪声。<br>2.选取点对：以特征点为中心，区S*S的领域窗口，在窗口内随机选取N组点对，一般N=128，256，512，默认是256，关于如何选取随机点对，提供了五种形式，结果如下<br>x，y方向平均分布采样<br>x，y均服从Gauss(0,S^2/25)各向同性采样<br>x服从Gauss(0,S^2/25)，y服从Gauss(0,S^2/100)<br>x，y从网格中随机获取<br>x一直在(0,0)，y从网格中随机选取<br>3.构建描述符：假设x，y是某个点对的两个端点，px和py是两点对应的像素值，则<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/byt.PNG" alt=""><br>对每一个点对都进行上述的二进制赋值，形成BRIEF的关键点的描述特征向量，该向量一般为128-512位的字符串，其中仅包含1和0。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python"># 1.实例化orborb = cv.xfeatures2d.orb_create(nfeatures)# 2.利用orb.detectAndCompute()检测关键点并计算kp,des = orb.detectAndCompute(gray,None)# 3.将关键点检测结果绘制在图像上cv.drawKeypoints(image,keypoints,outputimages,color,flags)</code></pre><p>gray：进行关键点检测的图像，注意是灰度图像</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport numpy as npimport matplotlib.pyplot as plt# 读取图像img = cv.imread('/Users/dinggongcheng/Downloads/lou.jpeg')# ORB角点检测# 实例化ORB对象orb = cv.ORB_create(nfeatures=5000)# 检测关键点，并计算特征描述符kp,des = orb.detectAndCompute(img,None)print(des.shape)# 将关键点绘制在图像上img2 = cv.drawKeypoints(img,kp,None,flags=0)# 绘制图像plt.figure(figsize=(10,8),dpi=100)plt.imshow(img2[:,:,::-1])plt.xticks([]),plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/orb.png" alt=""></p><h4 id="视频读写">视频读写</h4><h5 id="从文件中读取视频并播放">从文件中读取视频并播放</h5><p>在OpenCV中我们要获取一个视频，需要创建一个VideoCapture对象，指定你要读取的视频文件：</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python"># 1.创建读取视频的对象cap = cv.VideoCapture(filepath)# 2.获取视频的某些属性retval = cap.get(propId)# 3.修改视频的属性信息cap.set(propId,value)# 4.判断图像是否读取成功isornot = cap.isOpened()# 5.获取视频的一帧图像ret,frame = cap.read()# 6.调用cv.imshow()显示图像，在显示图像时使用cv.waitkey()设置适当的持续时间，如果太低视频会播放的非常快，如果太高就会播放的非常慢，通常情况下设置25ms# 7.释放视频cap.realease()</code></pre><p>filepath：视频文件路径<br>propId：从0到18的数字，每个数字表示视频的属性</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cv# 1.获取视频对象cap = cv.VideoCapture('/Users/dinggongcheng/Downloads/1.mp4')# 2.判断是否读取成功while(cap.isOpened()):    # 3.获取每一帧图像    ret,frame = cap.read()    # 4.获取成功显示图像    if ret == True:        cv.imshow('frame',frame)    # 5.每一帧间隔为25ms    if cv.waitKey(25) & 0xFF == ord('q'):        break# 6.释放视频对象cap.release()cv.destroyAllWindows()</code></pre><h5 id="保存视频">保存视频</h5><p>在OpenCV中我们保存视频使用的是VideoWriter对象，在其中指定输出文件的名称。</p><pre class=" language-language-python"><code class="language-language-python">out = cv2.VideoWriter(filename,fourcc,fps,framesize)</code></pre><p>filename：视频保存的位置<br>fourcc：指定视频编解码的4字节代码<br>fps：帧率<br>framesize：帧大小</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport numpy as np# 1.读取视频cap = cv.VideoCapture('/Users/dinggongcheng/Downloads/1.mp4')# 2.获取图像的属性，并将其转换为整数frame_width = int(cap.get(3))frame_height = int(cap.get(4))# 3.创建保存视频的对象，设置编码格式，帧率，图像的宽高等out = cv.VideoWriter('/Users/dinggongcheng/Downloads/output.mp4',cv.VideoWriter_fourcc('M','J','P','G'),10,(frame_width,frame_height))while(True):  # 4.获取视频中的每一帧图像  ret,frame = cap.read()  if ret == True:    # 5.将每一帧图像写入到输出文件中    out.write(frame)  else:    break# 6.释放资源cap.release()out.release()cv.destoryAllWindows()</code></pre><h4 id="视频追踪">视频追踪</h4><h5 id="meanshift">meanshift</h5><p><strong>原理</strong></p><p>meanshift算法的原理很简单。假设你有一堆点集，还有一个小的窗口，这个窗口可能是圆形的，现在你可能要移动这个窗口到点集密度最大的区域当中。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/c1.PNG" alt=""></p><p>最开始的窗口是蓝色圆环的区域，命名为C1。蓝色圆环的圆心用一个蓝色的矩形标注，命名为C1_o。而窗口中所有点的点集构成的质心在蓝色圆形点C1_r处，显然圆环的形心和质心并不重合。所以，移动蓝色的窗口，使得形心与之前得到的质心重合。在新移动后的圆环区域当中再次寻找圆环所包围点集的质心，然后再次移动，通常情况下，形心和质心是不重合的。不断执行上面的移动过程，直到形心和质心大致重合结束。这样，最后圆形的窗口会落到像素分布最大的地方，也就是图中的绿色圆圈，命名为C2.</p><p>meanshift算法除了应用在视频追踪当中，在聚类，平滑等等各种涉及到数据以及非监督学习的场合当中均有重要应用，是一个应用广泛的算法。</p><p>图像是一个矩阵信息，如果在一个视频当中使用meanshift算法来追踪一个运动的物体呢？<br>1.首先在图像上选定一个目标区域<br>2.计算选定区域的直方图分布，一般是HSV色彩空间的直方图。<br>3.对下一帧图像b同样计算直方图分布。<br>4.计算图像b当中与选定区域直方图分布最为相似的区域，使用meanshift算法将选定区域沿着最为相似的部分进行移动，直到找到最相似的区域，便完成了在图像b中的目标追踪。<br>5.重复3到4的过程，就完成整个视频目标追踪。</p><p>通常情况下我们使用直方图反向投影得到的图像和第一帧目标对象的起始位置，当目标对象的移动会反映到直方图反向投影图中，meanshift算法就把我们的窗口移动到反向投影图像中灰度密度最大的区域了。</p><p>直方图反向投影的流程是：</p><p>1.从输入图像的左上角(0,0)开始，切割一块(0,0)到(10,10)的临时图像<br>2.生成临时图像的直方图<br>3.用临时图像的直方图和模板图像的直方图对比，对比结果记为c<br>4.直方图对比结果c，就是结果图像(0,0)处的像素值<br>5.切割输入图像从(0,1)至(10,11)的临时图像，对比直方图，并记录到结果图像<br>6.重复1-5步知道输入图像的右下角，就形成了直方图的反向投影。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">cv.meanShift(probImage,window,criteria)</code></pre><p>probImage：ROI区域，即目标的直方图的反向投影<br>window：初始搜索窗口，就是定义ROI的rect<br>criteria：确定窗口搜索停止的准则，主要有迭代次数达到设置的最大值，窗口中心的漂移值大于某个设定的限制等。</p><p>实现meanShift的主要流程</p><p>1.读取视频文件<br>2.感兴趣区域设置：获取第一帧图像，并设置目标区域，即感兴趣区域<br>3.计算直方图：计算感兴趣区域的HSV直方图，并进行归一化<br>4.目标追踪：设置窗口搜索停止条件，直方图反向投影，进行目标追踪，并在目标位置绘制矩阵框。</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cv# 1.获取视频对象cap = cv.VideoCapture('/Users/dinggongcheng/Downloads/1.mp4')# 2.获取第一帧图像，并指定目标位置ret,frame = cap.read()# 2.1 目标位置r,h,c,w = 197,141,0,208track_window = (c,r,w,h)# 2.2 指定目标的感兴趣区域roi = frame[r:r+h,c:c+w]# 3.计算直方图# 3.1 转换色彩空间hsv_roi = cv.cvtColor(roi,cv.COLOR_BGR2HSV)# 3.2 去除低亮度的值# 3.3 计算直方图roi_hist = cv.calcHist([hsv_roi],[0],None,[180],[0,180])# 3.4 归一化cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)# 4.目标追踪# 4.1 设置查窗口搜索终止条件：最大迭代次数，窗口中心漂移最小值term_crit = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT,10,1)while(True):    # 4.2 获取每一帧图像    ret,frame = cap.read()    if ret == True:        # 4.3 计算直方图的反向投影        hsv = cv.cvtColor(frame,cv.COLOR_BGR2HSV)        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)        # 4.4 进行meanshift追踪        ret,track_window = cv.meanShift(dst,track_window,term_crit)        # 4.5 将追踪的位置绘制在视频上，并进行显示        x,y,w,h = track_window        img2 = cv.rectangle(frame,(x,y),(x+w,y+h),255,2)        cv.imshow('frame',img2)        if cv.waitKey(60) & 0xFF == ord('q'):            break    else:        break# 5.释放资源cap.release()cv.destroyAllWindows()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/mean.png" alt=""></p><h5 id="Camshift">Camshift</h5><p>camshift算法是对meanshift算法的改进算法，可随着跟踪目标的大小变化实时调整搜索窗口的大小，具有较好的跟踪效果。Camshift算法首先应用meanshift，一旦meanshift收敛，他就会更新窗口的大小，还计算最佳拟合椭圆的方向，从而根据目标的位置和大小更新搜索窗口。</p><pre class=" language-language-python"><code class="language-language-python"># 4.4 进行camshift追踪        ret,track_window = cv.CamShift(dst,track_window,term_crit)        # 4.5 将追踪的位置绘制在视频上，并进行显示        pts = cv.boxPoints(ret)        pts = np.int0(pts)        img2 = cv.polylines(frame,[pts],True,255,2)</code></pre><h4 id="人脸检测">人脸检测</h4><p><strong>基础：</strong></p><p>我们使用机器学习的方法完成人脸检测，首先需要大量的正样本图像和负样本图像来训练分类器。我们需要从其中提取特征。下图中的Haar特征会被使用，就像我们的卷积核，每一个特征是一个值，这个值等于黑色矩形中的像素值之后减去白色矩形中的像素值之和。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/re.PNG" alt=""></p><p>Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩阵特征简单的描述，眼睛要比脸颊颜色更深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。</p><p>Haar特征可用于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模板类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python"># 1.读取图片，并转换为灰度图# 2.实例化人脸和眼睛检测的分类对象# 实例化级联分类器classifier = cv.CascadeClassifier("haarcascade_frontalface_default.xml")# 加载分类器classifier.load('haarcascade_frontalface_default.xml')# 3.进行人脸和眼睛的检测rect = classifier.detectMultiScale(gray,scaleFactor,minNeighbors,minSize,maxSize)</code></pre><p>gray：要进行检测的人脸图像<br>scaleFactor：前后两次扫描中，搜索窗口的比例系数<br>minneighbors：目标至少被检测到minNeighbors次才会被认为是目标<br>minSize和maxSize：目标的最小尺寸和最大尺寸</p><pre class=" language-language-python"><code class="language-language-python">import matplotlib.pyplot as pltimport cv2 as cv# 1.以灰度图的形式读取图片img = cv.imread('/Users/dinggongcheng/Downloads/pepole.jpeg')gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)# 2.实例化OpenCV人脸和眼睛识别的分类器face_cas  = cv.CascadeClassifier("/Users/dinggongcheng/Downloads/haarcascade_frontalface_default.xml")face_cas.load('/Users/dinggongcheng/Downloads/haarcascade_frontalface_default.xml')eyes_cas = cv.CascadeClassifier("/Users/dinggongcheng/Downloads/haarcascade_eye.xml")eyes_cas.load('/Users/dinggongcheng/Downloads/haarcascade_eye.xml')# 3.调用识别人脸faceRects = face_cas.detectMultiScale(gray,scaleFactor=1.5,minNeighbors=3,minSize=(50,50))for faceRect in faceRects:    x,y,w,h = faceRect    # 框出人脸    cv.rectangle(img,(x,y),(x+h,y+w),(0,255,0),3)    # 4.在识别出的人脸中进行眼睛的检测    roi_color = img[y:y+h,x:x+w]    roi_gray = gray[y:y+h,x:x+w]    eyes = eyes_cas.detectMultiScale(roi_gray,scaleFactor=1.3)    for (ex,ey,ew,eh) in eyes:        cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,0,255),1)# 5.检测结果的绘制plt.figure(figsize=(8,6),dpi=100)plt.imshow(img[:,:,::-1]),plt.title('result')plt.xticks([]),plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/pepole.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习笔记三</title>
      <link href="/2021/04/20/python-xue-xi-bi-ji-san/"/>
      <url>/2021/04/20/python-xue-xi-bi-ji-san/</url>
      
        <content type="html"><![CDATA[<h3 id="python学习笔记三">python学习笔记三</h3><h4 id="类与对象">类与对象</h4><p>语法：</p><pre class=" language-language-python"><code class="language-language-python">class Student:  native_plcae = '江苏'  def __int__(self,name,age)  self.name = name    self.age = age  del info(self):    print(self.name)  @classmethod  def cm(cls)  print('类方法')  @staticmethod  def sm():    print('静态方法')</code></pre><pre class=" language-language-python"><code class="language-language-python">stu = Student('aim',20)Student.info(stu) #对象直接调用方法，参数为类的对象实例</code></pre><p>在python中非静态类，可以直接通过类调用属性和方法。</p><pre class=" language-language-python"><code class="language-language-python">stu.gender = '' #运行时动态绑定属性</code></pre><p>动态绑定方法</p><pre class=" language-language-python"><code class="language-language-python">def show():  print('')stu.show = showstu.show()</code></pre><h4 id="三大特征">三大特征</h4><p>封装、继承与多态。</p><p>封装</p><pre class=" language-language-python"><code class="language-language-python">self.__age = age #通过__来标识属性只允许类内使用stu._Student_age #可以通过_类名_属性调用</code></pre><p>继承</p><pre class=" language-language-python"><code class="language-language-python">class Student(Person) #父类当作参数传递super().__init__(name,age) #通过super()调用父类的构造器class C(A,B) #python允许多父类</code></pre><pre class=" language-language-python"><code class="language-language-python">def __str__(self): #通过重写__str__修改类的默认输出内容(内存地址)</code></pre><p>多态</p><pre class=" language-language-python"><code class="language-language-python">def fun(obj): # 通过传入的参数设置类型进行多态  obj.eat()fun(Cat())fun(Dog())</code></pre><p>只关心对象的行为</p><h4 id="特殊属性和方法">特殊属性和方法</h4><table><thead><tr><th></th><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>特殊属性</td><td>_<em>dict</em>_</td><td>获得类对象或实例对象所绑定的所有属性和方法的字典</td></tr><tr><td>特殊方法</td><td>_<em>len</em>_()</td><td>通过重写_<em>len</em>_()方法，让内置函数len()的参数可以是自定义类型</td></tr><tr><td></td><td>_<em>add</em>_()</td><td>通过重写_<em>add</em>_()方法，可使用自定义对象具有+功能</td></tr><tr><td></td><td>_<em>new</em>_()</td><td>用于创建对象</td></tr><tr><td></td><td>_<em>init</em>_()</td><td>对创建的对象进行初始化</td></tr></tbody></table><pre class=" language-language-python"><code class="language-language-python">class Person(object):  def __new__(cls, *args, **kwargs):    print('__new__被调用执行了，cls的id值为{0}'.format(id(cls))) # 9360    obj = super().__new__(cls)    print('创建的对象的id为:{0}'.format(id(obj))) # 7104    return obj    def __init__(self,name,age):    print('__init__被调用了，self的id值为:{0}'.format(id(self))) # 7104    self.name = name    self.age = age    print('object这个类对象的id为:{0}'.format(id(object))) # 3232print('Person这个类对象的id为:{0}').format(id(Person)) # 9360# 创建Person类的实例对象p1 = Person('aim',20)print('p1这个Person类的实例对象的id:{0}'.format(id(p1)) # 7104</code></pre><h4 id="类的浅拷贝与深拷贝">类的浅拷贝与深拷贝</h4><p>浅拷贝：引用同一个子对象地址</p><pre class=" language-language-python"><code class="language-language-python">import copyobj2 = copy.copy(obj1)obj1.cpuobj2.cpu # obj2.cpu的内存地址等于obj1.cpu的内存地址</code></pre><p>深拷贝：引用的子对象地址也是新的</p><pre class=" language-language-python"><code class="language-language-python">import copyobj2 = copy.deepcopy(obj1)obj1.cpuobj2.cpu # obj2.cpu的内存地址和obj1.cpu的内存地址不同</code></pre><h4 id="模块">模块</h4><p>py单文件中既可以包括函数，还可以有类和语句。</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvfrom math import pow</code></pre><pre class=" language-language-python"><code class="language-language-python">if __name__ == '__main__': # 当作模块引入时通过判断不运行if体中的语句  print()</code></pre><p>内置模块</p><table><thead><tr><th>模块名</th><th>描述</th></tr></thead><tbody><tr><td>sys</td><td>与python解释器及其环境操作相关的标准库</td></tr><tr><td>time</td><td>提供与时间相关的各种函数的标准库</td></tr><tr><td>os</td><td>提供了访问操作系统服务功能的标准库</td></tr><tr><td>calendar</td><td>提供与日期相关的各种函数的标准库</td></tr><tr><td>urllib</td><td>用于读取来自网上的数据标准库</td></tr><tr><td>json</td><td>用于使用JSON序列化和反序列化对象</td></tr><tr><td>re</td><td>用于在字符串中执行正则表达式匹配和替换</td></tr><tr><td>math</td><td>提供标准算术运算函数的标准库</td></tr><tr><td>decimal</td><td>用于进行精确控制运算精度、有效数位和四舍五入操作的十进制运算</td></tr><tr><td>logging</td><td>提供了灵活的记录时错警告和调试信息等日志信息的功能</td></tr></tbody></table><h4 id="文件读写">文件读写</h4><table><thead><tr><th>打开模式</th><th>描述</th></tr></thead><tbody><tr><td>r</td><td>以只读模式打开文件，文件的指针将会放在文件的开头</td></tr><tr><td>w</td><td>以只写模式打开文件，如果文件不存在则创建，如果文件存在，则覆盖原有内容，文件指针在文件的开头</td></tr><tr><td>a</td><td>以追加模式打开文件，如果文件不存在则创建，文件指针在文件开头，如果文件存在，则在文件末尾追加内容，文件指针在原文件末尾</td></tr><tr><td>b</td><td>以二进制方式打开文件，不能单独使用，需要与其他模式一起使用，rb，或者wb</td></tr><tr><td>+</td><td>以读写方式打开文件，不能单独使用，需要与其他模式一起使用，a+</td></tr></tbody></table><p>API：</p><table><thead><tr><th>方法名</th><th>说明</th></tr></thead><tbody><tr><td>read([size])</td><td>从文件读取size个字节或字符的内容返回。若省略[size]，则读取到文件末尾，即一次读取文件所有内容。</td></tr><tr><td>readline()</td><td>从文本文件中读取一行内容</td></tr><tr><td>readlines()</td><td>把文本文件中每一行都作为独立的字符串对象，并将这些对象放入列表返回</td></tr><tr><td>write(str)</td><td>将字符串str内容写入文件</td></tr><tr><td>writelines(s_list)</td><td>将字符串列表s_list写入文本文件，不添加换行符</td></tr><tr><td>seek(offset,[whence])</td><td>把文件指针移动到新的位置，offset表示相对于whence的位置：offset为正往结束方向移动，为负往开始方向移动。whence不同的值代表不同含义：0代表从文件头开始计算，1表示从当前位置开始计算，2表示从文件尾开始计算</td></tr><tr><td>tell()</td><td>返回文件指针的当前位置</td></tr><tr><td>flush()</td><td>把缓冲区的内容写入文件，但不关闭文件</td></tr><tr><td>close()</td><td>把缓冲区的内容写入文件，同时关闭文件，释放文件对象相关资源</td></tr></tbody></table><p>with语句可以自动管理上下文资源，不论什么原因跳出with块，都能确保文件正确的关闭，以此达到释放资源的目的。<br>实现了特殊方法_<em>enter</em>_(),_<em>exit</em>_()的类遵守了上下文管理器协议，该类对象的实例对象，称为上下文管理器。(作用类似于代理和finally)</p><pre class=" language-language-python"><code class="language-language-python">with open('a.txt',r) as file:  print(file.read())</code></pre><p>os.path模块操作目录函数</p><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>abspath(path)</td><td>用于获取文件或目录的绝对路径</td></tr><tr><td>exists(path)</td><td>用于判断文件或目录是否存在，如果存在返回true，否则返回false</td></tr><tr><td>join(path,name)</td><td>将目录与目录或者文件名拼接起来</td></tr><tr><td>splitext()</td><td>分离文件名和扩展名</td></tr><tr><td>basename(path)</td><td>从一个目录中提取文件名</td></tr><tr><td>dirname(path)</td><td>从一个路径中提取文件路径，不包括文件名</td></tr><tr><td>isdir(path)</td><td>用于判断是否为路径</td></tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV学习笔记三</title>
      <link href="/2021/04/20/opencv-xue-xi-bi-ji-san/"/>
      <url>/2021/04/20/opencv-xue-xi-bi-ji-san/</url>
      
        <content type="html"><![CDATA[<h3 id="OpenCV学习笔记三">OpenCV学习笔记三</h3><h4 id="角点特征">角点特征</h4><p>角点是图像很重要的特征，对图像图形的理解和分析有着很重要的作用。角点在三维场景重建运动估计，目标跟踪、目标识别、图像配准与匹配等计算机视觉领域起着非常重要的作用。在现实世界中，角点对应于物体的拐角，道路的十字路口或丁字路口等。</p><h5 id="Harris和Shi-Tomas算法">Harris和Shi-Tomas算法</h5><p>Harris角点检测</p><p>原理：通过图像局部的小窗口观察图像，角点的特征是窗口沿任意方向移动都会导致图像灰度的明显变化。</p><p>将局部窗口各个方向移动(u,v)并计算所有灰度差异的总和，表达式如下。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/harr.PNG" alt=""></p><p>其中I(x,y)是局部窗口的图像灰度，I(x+u,y+v)是平移后的图像灰度，w(x,y)是窗口函数，可以是矩形窗口，也可以是对每一个像素赋予不同权重的高斯窗口。</p><p>角点检测中使E(u,v)的值最大，利用一阶泰勒展开式有：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tail.PNG" alt=""></p><p>其中Ix和Iy是沿x和y方向的导数，可用Sobel算子计算。</p><p>推导如下：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tuid.PNG" alt=""></p><p>M矩阵决定了E(u,v)的取值，利用M来求角点，M是Ix和Iy的二次项函数，可以表示成椭圆的形状，椭圆的长短半轴由M的特征值决定，方向由特征矢量决定。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/ty.PNG" alt=""></p><p>椭圆函数特征值与图像中的角点、直线和平面之间的关系如下所示。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/guax.PNG" alt=""></p><p>共可分为三种情况：</p><p>图像中的直线。一个特征值大，另一个特征值小，λ1&gt;&gt;λ2或λ2&gt;&gt;λ1.椭圆函数值在某一方向上大，在其他方向上小。<br>图像中的平面。两个特征值都小，且近似相等；椭圆函数数值在各个方向上都小。<br>图像中的角点。两个特征值都大，且近似相等，椭圆函数在所有方向都增大。</p><p>Harris给出的角点计算方法并不需要计算具体的特征值，而是计算一个角点响应值R来判断角点，R的计算公式为：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/R.PNG" alt=""></p><p>式中detM为矩阵M的行列式，即λ1λ2；traceM为矩阵的迹，即λ1+λ2。阿尔法为常数，取值范围为0.04-0.06。</p><p>R小于0为边缘，R大于0但是很小为平面，当R大于0且较大时为角点。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">dst = cv.cornerHarris(src,blockSize,ksize,k)</code></pre><p>img：数据类型为float32的输入图像<br>blockSize：角点检测中要考虑的领域大小<br>ksize：sobel求导使用的核大小<br>k：角点检测方程中的自由参数，取值参数为[0.04-0.06]</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport numpy as npimport  matplotlib.pyplot as plt# 读取图像，并转换为灰度图像img = cv.imread('/Users/dinggongcheng/Downloads/lou.jpeg')gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)# 角点检测# 输入图像必须是float32gray = np.float32(gray)# 最后一个参数在0.04到0.05之间dst = cv.cornerHarris(gray, 2, 3, 0.04)# 设置阈值，将角点绘制出来，阈值根据图像进行选择img[dst > 0.001 * dst.max()] = [0, 0, 255]# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.imshow(img[:, :, ::-1]), plt.title('harris')plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lou.png" alt=""></p><h5 id="Shi-Tomasi角点检测">Shi-Tomasi角点检测</h5><p>原理：</p><p>Shi-Tomasi算法是对角点检测算法的改进，一般会比Harris算法得到更好的角点。Harris算法的角点响应函数是将矩阵M的行列式值与M的迹相减，利用差值判断是否为角点。后来Shi和Tomasi提出改进的方法是，若矩阵M的两个特征值中较小的一个大于阈值，则认为他是角点，即R = min(λ1,λ2)</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">corners = cv2.goodFeaturesToTrack(image,maxcorners,qualityLevel,minDistance)</code></pre><p>image：输入灰度图像<br>maxCorners：获取角点数的数目<br>qualityLevel：该参数指出最低可接受的角点质量水平，在0-1之间<br>minDistance：角点之间最小的欧式距离，避免得到相邻特征点<br>corners：搜索到的角点，在这里所有低于质量水平的角点被排除掉，然后把合格的角点按质量排序，然后将质量较好的角点附近的角点删掉，最后找到maxCorners个角点返回</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport numpy as npimport  matplotlib.pyplot as plt# 读取图像，并转换为灰度图像img = cv.imread('/Users/dinggongcheng/Downloads/lou.jpeg')gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)# 角点检测corners = cv.goodFeaturesToTrack(gray, 1000, 0.01, 10)# 绘制角点for i in corners:    x, y = i.ravel()    cv.circle(img, (x, y), 2, (0, 0, 255), -1)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.imshow(img[:, :, ::-1]), plt.title('shi-tomasi')plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tomasi.png" alt=""></p><h5 id="SIFT-SURF算法">SIFT/SURF算法</h5><p>Harris和Shi-Tomasi角点检测算法，这两种算法具有旋转不变性，但不具有尺度不变性，即当角点被放大时，原先的角点无法检测。尺度不变特征转换即SIFT。它用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法有David Lowe在1999年所发表，2004年完善总结。应用范围包含物体辨识、机器人地图感知与导航、影像缝合、3D模型建立、手势辨识、影像追踪和动作对比等领域。</p><p>SIFT算法的实质是在不同的尺度空间上查找关键点，并计算出关键点的方向。SIFT所查找的关键点是一些十分突出，不会因光照，放射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。</p><p>基本流程：</p><p>Lowe将SIFT算法分解为如下四步：</p><p>1.尺度空间极值检测：搜索所有尺度上的图像位置。通过高斯差分函数来识别潜在的对于尺度和旋转不变的关键点。<br>2.关键点定位：在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。<br>3.关键点方向确定：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而保证了对于这些变换的不变性。<br>4.关键点描述：在每个关键点周围的领域内，在选定的尺度上测量图像局部的梯度。这些梯度作为关键点的描述符，它允许比较大的局部形状的变换或光照变化。</p><h6 id="尺度空间极值检测">尺度空间极值检测</h6><p>在不同的尺度空间是不能使用相同的窗口检测极值点，对小的关键点使用小的窗口，对大的关键点使用大的窗口，为了达到上述目的，我们使用尺度空间滤波器。</p><p>一个图像的尺度空间$L(x,y,\sigma)$，定义为原始图像$l(x,y)$与一个可变尺度的二维高斯函数，即</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sift.PNG" alt=""></p><p>σ是尺度空间因子，它决定了图像的模糊的程度。在大尺度下表现的是图像的概貌信息，在小尺度下表现的是图像的细节信息。</p><p>在计算高斯函数的离散近似值时，在大概3σ距离之外的像素都可以看作不起作用，这些像素的计算也就可以忽略。所以，在实际应用中，只计算(6σ+1)*(6σ+1)的高斯卷积核就可以保证相关像素影像。</p><p>下面我们构建图像的高斯金字塔，它采用高斯函数对图像进行模糊以及降采样处理得到的，高斯金字塔构建过程中，首先将图像扩大一倍，在扩大的图像的基础上构建高斯金字塔，然后对该尺寸下图像进行高斯模糊，几幅模糊之后的图像集合构成了一个Octave，然后对该Octave下选择一幅图像进行下采样，长和宽分别缩短一倍，图像面积变为原来四分之一。这幅图像就是下一个Octave的初始图像，在初始图像的基础上完成属于这个Octave的高斯模糊处理，以此类推完成整个算法所需要的所有八度构建，这样这个高斯金字塔就构建出来了。</p><p>利用LoG，即图像的二阶导数，可以在不同的尺度下检测图像的关键点信息，从而确定图像的特征点。但LoG的计算量大，效率低。所以我们通过两个相邻高斯尺度空间的图像的相减，得到DoG来近似LoG。</p><p>为了计算DoG我们构建高斯差分金字塔，该金字塔是在上述的高斯金字塔的基础上构建的，建立过程是：在高斯金字塔中每个Octave中相邻两层相减就构成了高斯差分金字塔。</p><p>高斯差分金字塔的第一组第一层是由高斯金字塔的第一组的第二层减去第一组的第一层得到的。以此类推，逐渐逐层生成每一个差分图像，所有差分图像构成差分金字塔。概括为DoG金字塔的第o组第i层图像是由高斯金字塔的第o组第i+1层减去第o组第i层得到的。后续sift特征点的提取都是在DoG金字塔上进行的。</p><p>在DoG搞定后，就可以在不同的尺度空间中搜索最大值了。对于图像中的每一个像素点而言，它需要与自己周围的8邻域，以及尺度空间中上下两层中的相邻的18个点相比。如果是局部最大值，它就可能是一个关键点。基本上来说关键点是图像在相应尺度空间中的最好代表。</p><p>搜索过程从每组的第二层开始，以第二层为当前层，对第二层的DoG图像中的每个点取一个3*3的立方体，立方体上下层为第一层与第三层。这样，搜索得到的极值点既有位置坐标，又有空间尺度坐标。当第二层搜索完成后，再以第三层作为当前层，其过程与第二层的搜索类似。当S=3时，每组里面要搜索三层，所以DoG中就有S+2层，在初始构建的金字塔中每组有S+3层。</p><h6 id="关键点定位">关键点定位</h6><p>由于DoG对噪声和边缘比较敏感，因此在上面高斯差分金字塔中检测到的局部极值点需经过进一步的检验才能精确定位为特征点。</p><p>使用尺度空间的泰勒级数展开来获得极值的准确位置，如果极值点的灰度值小于阈值就会被忽略掉，在OpenCV中这种阈值被称为contrastThreshold。</p><p>DoG算法对边界非常敏感，所以我们必须要把边界去掉。Harris算法除了可以用于角点检测之外还可以用于检测边界，从Harris角点检测的算法中，当一个特征值远远大于另外一个特征值时检测到的是边界。那在DoG算法中欠佳的关键点在平行边缘的方向有较大的主曲率，而在垂直于边缘的方向有较小的曲率，两者的比值如果高于某个阈值，就认为该关键点是边界，将被忽略，一般将该阈值设置为10.</p><p>将低对比度和边界的关键点去除，得到的就是我们感兴趣的关键点。</p><h6 id="关键点方向确定">关键点方向确定</h6><p>经过上述两个步骤，图像的关键点就完全知道了，这些关键点具有尺度不变性。为了实现旋转不变性，还需要为每个关键点分配一个方向角度，也就是根据检测到的关键点所在高斯尺度图像的邻域结构中求的一个方向基准。</p><p>对于任一关键点，我们采集其所在高斯金字塔图像以r为半径的区域内所有像素的梯度特征，半径r为：r = 3 * 1.5σ。其中σ是关键点所在层的图像的尺度，可以得到对应的尺度图像。</p><p>梯度的增值和方向的计算公式为：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tid.PNG" alt=""></p><p>完成关键点梯度计算后，使用直方图统计关键点邻域内像素的梯度幅值和方向。具体做法为将360度分为36柱，每10度为一柱，然后在以r为半径的区域内，将梯度方向在某一个柱内的像素找出来，然后将他们的幅值相加在一起作为柱的高度，因为在r为半径的区域内像素的梯度幅值对中心像素的贡献是不同的，因此还需要对幅值进行加权处理，采用高斯加权，方差为1.5σ。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/zhi.PNG" alt=""></p><p>每个特征点必须分配一个主方向，还需要一个或多个辅方向，增加辅方向的目的是为了增强图像匹配的鲁棒性。辅方向的定义是，当一个柱体的高度大于主方向柱体高度的百分之八十时，则该柱体所代表的方向就是给特征点的辅方向。</p><p>直方图的峰值，即最高的柱代表的方向是特征点邻域范围内图像梯度的主方向，但该柱体代表的角度是一个范围，所以我们还要对离散的直方图进行插值拟合，以得到更精确的方向角度值。利用抛物线对离散的直方图进行拟合。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/cha.PNG" alt=""></p><p>获取图像关键点主方向后，每个关键点有三个信息(x,y,σ,θ):位置、尺度、方向。由此我们可以确定一个SIFT特征区域。通常使用一个带箭头的圆或直接使用箭头表示SIFT区域的三个值，中心表示特征点位置，半径表示关键点尺度，箭头表示方向。</p><h6 id="关键点描述">关键点描述</h6><p>通过以上步骤，每个关键点就被分配了位置，尺度和方向信息。接下来我们为每个关键点建立一个描述符，该描述符既具有可区分性，又具有对某些变量的不变性，如光照，视角等。而且描述符不仅仅包含关键点，也包括关键点周围对其有贡献的像素点，主要思路就是通过将关键点周围图像区域分块，计算块内的梯度直方图，生成具有特征向量，对图像进行抽象。</p><p>描述符与特征点所在的尺度有关，所以我们在关键点所在的高斯尺度图像上生成对应的描述符。以特征点为中心，将其邻域划分为d*d个子区域(一般d=4)，每个子区域都是一个正方形，边长为3σ，考虑到实际计算时，需要进行三次线性插值，所以特征点邻域的为3σ(d+1)*3σ(d+1)的范围。为了保证特征点的旋转不变性，以特征点为中心，将坐标轴旋转为关键点的主方向。</p><p>计算子区域内的像素梯度，并按照σ=0.5d进行高斯加权，然后插值计算得到每个种子点的八个方向的梯度，插值方法如下所示。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/5d.PNG" alt=""></p><p>每个种子点的梯度都是由覆盖其的四个子区域插值而得到的。如图中的红色点，落在第0行和第1行之间，对这两行都有贡献。对第0行和第3列种子点的贡献因子为dr，对第一行和第三列的贡献因此为1-dr，同理，对邻近两列的贡献因子为dc和1-dc，对邻近两个方向的贡献因子为do和1-do。则最终结果累加在每个方向上的梯度大小为</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/qyab.PNG" alt=""></p><p>其中k，m，n为0或为1，如上统计128个梯度信息即为该关键点的特征向量，按照特征点对每个关键点的特征向量进行排序，就得到了SIFT特征描述向量。</p><h6 id="SURF原理">SURF原理</h6><p>使用SIFT算法进行关键字检测和描述的执行速度比较慢，需要速度更快的算法。2006年Bay提出了SURF算法，是SIFT算法的增强版，它的计算量小，运算速度快，提取的特征值与SIFT几乎相同，将其与SIFT算法对比。</p><table><thead><tr><th></th><th>SIFT</th><th>SURF</th></tr></thead><tbody><tr><td>特征点检测</td><td>使用不同尺度的图片与高斯函数进行卷积</td><td>使用不同大小的盒滤波器与原始图像做卷积，易于并行</td></tr><tr><td>方向</td><td>关键点邻接矩形区域内，利用梯度直方图计算</td><td>关键点邻接圆域内，计算x，y方向的haar小波</td></tr><tr><td>描述符生成</td><td>关键点邻域内划分d*d子区域，每个子区域内计算8个方向的直方图</td><td>关键点邻域内划分d*d个子区域，每个子区域计算采样点的haar小波响应，即dx，dy及对应的绝对值的累加</td></tr></tbody></table><p>API：</p><pre class=" language-language-python"><code class="language-language-python"># 1.实例化siftsift = cv.xfeatures2d.SIFT_create()# 2.利用sift.detectAndCompute()检测关键点并计算kp, des = sift.detectAndCompute(gray,None)# 3.将关键点检测结果绘制在图像上cv.drawKeypoints(images,keypoints,outputimages,color,flags)</code></pre><p>gray：进行关键点检测的图像，注意是灰度图像<br>kp：关键点信息，包括位置、尺度和方向信息<br>des：关键点描述符，每个关键点对应128个梯度信息的特征向量<br>image：原始图像<br>keypoints：关键点信息，将其绘制在图像上<br>outputimage：输出图片，可以是原始图像<br>color：颜色设置，通过修改bgr的值更改画笔的颜色<br>flags：绘图功能的标识设置<br>1.cv2.DRAW_MATCHES_FLAGS_DEFAULT：创建输出图像矩阵，使用现存的输出图像绘制匹配对和特征点，对每一个关键点只绘制中间点<br>2.cv2.DRAW_MATCHES_FLAGS_OVER_OUTIMG：不创建输出图像矩阵，而是在输出图像上绘制匹配对<br>3.cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS：对每一个特征点绘制带大小和方向的关键点图形<br>4.cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS：单点的特征点不被绘制</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport numpy as npimport matplotlib.pyplot as plt# 读取图像，并转换为灰度图像img = cv.imread('/Users/dinggongcheng/Downloads/lou.jpeg')gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)# SIFT关键点检测# 实例化sift对象sift = cv.xfeatures2d.SIFT_create()# 关键点检测：kp关键点信息包括方向、尺度、位置信息，des是关键点的描述符kp, des = sift.detectAndCompute(gray, None)# 在图像桑绘制关键点的检测结果cv.drawKeypoints(img, kp, img, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.imshow(img[:, :, ::-1]), plt.title('sift')plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/feature.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV学习笔记二</title>
      <link href="/2021/04/16/opencv-xue-xi-bi-ji-er/"/>
      <url>/2021/04/16/opencv-xue-xi-bi-ji-er/</url>
      
        <content type="html"><![CDATA[<h3 id="OpenCV学习笔记二">OpenCV学习笔记二</h3><h4 id="图像处理">图像处理</h4><h5 id="直方图">直方图</h5><p>直方图是对数据进行统计的一种方法，并且将统计值组织到一系列实现定义好的bin中。其中，bin为直方图中经常用到的一个概念，可以译为直条或组距，其数值是从数据中计算出的特征统计量，这些数据可以是诸如梯度、方向、色彩或任何其他特征。</p><p>图像直方图适用以表示数字图像中亮度分布的直方图，标绘了图像中每个亮度值的像素个数。这种直方图中，横坐标的左侧为较暗的区域，而右侧为较亮的区域。因此一张较暗图片的直方图中的数据多集中于左侧和中间部分，而整体明亮，只有少量阴影的图像则相反。</p><p>注意：直方图是根据灰度图进行绘制的，而不是彩色图像。</p><p>绘制原理：将图像的信息(灰度值 0-255)，已知数字的范围包含256个值，于是可以按一定规律将这个范围分割成子区域(bins)。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/huidu.PNG" alt=""></p><p>dims：需要统计的特征数目。图中dims = 1，因为仅仅统计了灰度值。<br>bins：每个特征空间子区段的数目，可译为直条或组距，在图中，bins = 16。<br>range：要统计特征的取值范围。图中range = [0，255]。</p><p>直方图是图像中像素强度分布的图形表达方式。它统计了每一个强度值所具有的像素个数，不同的图像的直方图可能是相同的。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">cv2.calcHist(images,channels,mask,histSize,ranges[,hist[,accumlate]])</code></pre><p>images：原图像，当传入函数时应该用中括号[]括起来，如[img]<br>channels：如果输入图像是灰度图，他的值就是[0]；如果是彩色图像的话，传入的参数可以是[0],[1],[2]，它们分别对应着通道B，G，R。<br>mask：掩膜图像，要统计整幅图像的直方图就把它设为None，但是如果想统计图像的某一部分的直方图的话，你就需要制作一个掩膜图像并使用它。<br>hitSize：bin的数目。也应该用中括号括起来，即[256]。<br>ranges：像素值范围，通常为[0，256]</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/pythondemo.jpeg" alt=""></p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvfrom matplotlib import pyplot as plt# 直接以灰度图的方式读入img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# 统计灰度图histr = cv.calcHist([img], [0], None, [256], [0,256])# 绘制灰度图plt.figure(figsize=(10, 6) ,dpi=100)plt.plot(histr)plt.grid()plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4161.png" alt=""></p><h5 id="掩膜的应用">掩膜的应用</h5><p>掩膜是用选定的图像，图形或物体，对要处理的图像进行遮挡，来控制图像处理的区域。</p><p>在数字图像处理中，我们通常使用二维矩阵数组进行掩膜，掩膜是由0和1组成一个二进制图像，利用该掩膜图像要处理的图像进行掩膜，其中1值的区域被处理，0值区域被屏蔽，不会处理。</p><p>掩膜的主要用途是：<br>提取感兴趣区域：用预先制作的感兴趣区掩膜与待处理图像进行与操作，得到感兴趣区图像，感兴趣区内图像值保持不变，而区外图像值都为0。<br>屏蔽作用：用掩膜对图像上某些区域作屏蔽，使其不参加处理或不参加处理参数的计算，或仅对屏蔽区作处理或统计。<br>结构特征提取：用相似性变量或图像匹配方法检测和提取图像中与掩膜相似的结构特征。<br>特殊形状图像制作</p><p>掩膜在遥感影像处理中使用较多，当提取道路或者河流，或者房屋时，通常一个掩膜矩阵来对图像进行像素过滤，然后将我们需要的地物或者标志突出显示出来。</p><p>我们使用cv.calcHist()来查找完整图像的直方图。如果要查找图像的某些区域的直方图，只需在要查找直方图的区域上创建一个白色的掩膜图像，否则创建黑色，然后将其作为掩码mask传递即可。</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvfrom matplotlib import pyplot as plt# 直接以灰度图的方式读入img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# 创建蒙板mask = np.zeros(img.shape[:2], np.uint8)mask[400:650, 200:500] = 255# 掩膜masked_img = cv.bitwise_and(img, img, mask = mask)# 统计掩膜后图像的灰度图mask_histr = cv.calcHist([img], [0], mask, [256], [0, 256])# 图像演示fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))axes[0, 0].imshow(img, cmap=plt.cm.gray)axes[0, 0].set_title("from")axes[0, 1].imshow(mask, cmap=plt.cm.gray)axes[0, 1].set_title("mask")axes[1, 0].imshow(masked_img, cmap=plt.cm.gray)axes[1, 0].set_title("mask_data")axes[1, 1].plot(mask_histr)axes[1, 1].grid()axes[1, 1].set_title("gray")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4162.png" alt=""></p><h5 id="直方图均衡化">直方图均衡化</h5><p>如果一幅图像中的大多数像素点的像素值都集中在某一个小的灰度值范围之内，一幅图像整体很亮，那所有的像素值的取值个数应该都会很高。所以应该把它的直方图做一个横向拉伸，就可以扩大图像像素值的分布范围，提高图像的对比度，这就是直方图均衡化要做的事情。</p><p>直方图均衡化是把原始图像的灰度直方图从比较密集的某个灰度区间变成在更广泛灰度范围内的分布。直方图均衡化就是对图像进行非线形拉伸，重新分配图像像素值，使一定灰度范围内的像素数量大致相同。</p><p>这种方法提高图像整体的对比度，特别是有用数据的像素值分布比较接近时，在x光图像中使用广泛，可以提高骨架结构的显示，另外在曝光过度或不足的图像中可以更好的突出细节。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">dst = cv.equalizeHist(img)</code></pre><p>img：灰度图像<br>dst：均衡化后的结果</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvfrom matplotlib import pyplot as plt# 直接以灰度图的方式读入img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# 均衡化处理dst = cv.equalizeHist(img)# 结果显示fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), dpi=100)axes[0, 0].imshow(img, cmap=plt.cm.gray)axes[0, 0].set_title("from")axes[0, 1].imshow(dst, cmap=plt.cm.gray)axes[0, 1].set_title("equalize")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4163.png" alt=""></p><h5 id="自适应的直方图均衡化">自适应的直方图均衡化</h5><p>上述的直方图均衡，我们考虑的是图像的全局对比度。的确在进行完直方图均衡化后，图片背景的对比度被改变了，但在某个局部太暗，丢失了很多信息，所以在很多情况下，这样做的效果并不好。为了解决这个问题，需要使用自适应的直方图均衡化。此时，整幅图像会被分成很多小块，这些小块被称为“tiles”(在OpenCV中tiles的大小默认为8*8)。如果有噪声的话，噪声会被放大。为了避免这种情况的出现要使用对比度限制。对于每个小块来说，如果直方图中的bin超过对比度的上限的话，就把其中的像素点均匀分散到其他bins中，然后在进行直方图均衡化。</p><p>最后为了去除每一个小块之间的边界，在使用双线性差值，对每一小块进行拼接。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">cv.createCLAHE(clipLimit, tileGridSize)</code></pre><p>clipLimit：对比度限制，默认为40<br>tileGridsize：分块的大小，默认为8*8</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvfrom matplotlib import pyplot as plt# 直接以灰度图的方式读入img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# 创建一个自适应均衡化的对象，并应用于图像clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))cl1 = clahe.apply(img)# 结果显示fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), dpi=100)axes[0, 0].imshow(img, cmap=plt.cm.gray)axes[0, 0].set_title("from")axes[0, 1].imshow(cl1, cmap=plt.cm.gray)axes[0, 1].set_title("clahe")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4164.png" alt=""></p><h5 id="边缘检测">边缘检测</h5><p>原理：</p><p>边缘检测是图像处理和计算机视觉中的基本问题，边缘检测的目的是标识数字图像中亮度变化明显的点。图像属性中的显著变化通常反映了属性的重要事件和变化。</p><p>图像边缘检测大幅度的减少了数据量，并且剔除了可以认为不相关的信息，保留了图像重要的结构属性。有许多方法用于边缘检测，它们的绝大部分可以划分为两类：基于搜索和基于零穿越。</p><p>基于搜索：通过寻找图像一阶导数中的最大值来检测边界，然后利用计算结果估计边缘的局部方向，通常采用梯度的方向，并利用此方向找到局部最大梯度模的最大值，代表算法是Sobel算子和Scharr算子。<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sousuo.PNG" alt=""></p><p>基于零穿越：通过寻找图像二阶导数零穿越来寻找边界，代表算法是Laplacian算子<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lapu.PNG" alt=""></p><h5 id="Sobel检测算子">Sobel检测算子</h5><p>Sobel检测算法比较简单，实际应用中效率比canny边缘检测效率要高，但是边缘不如Canny检测的准确，但是很多实际应用的场合，sobel边缘却是首选，Sobel算子是高斯平滑与微分操作的结合体，所以其抗噪声能力很强，用途较多。尤其是效率要求较高，而对纹理不太关心的时候。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sobel.PNG" alt=""></p><p>注意：当内核大小为3时，以上Sobel内核可能产生比较明显的误差，为解决这一问题，我们使用Scharr函数，但该函数仅作用于大小为3的内核。该函数的运算与Sobel函数一样快，但结果却更加精确。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/schar.PNG" alt=""></p><p>Sobel API：</p><pre class=" language-language-python"><code class="language-language-python">Sobel_x_or_y = cv2.Sobel(src,ddepth,dx,dy,dst,ksize,scale,delta,borderType)</code></pre><p>src：传入的图像<br>ddepth：图像的深度<br>dx和dy：指求导的阶数，0表示这个方向上没有求导，取值为0，1。<br>ksize：是Sobel算子的大小，即卷积核的大小，必须为奇数1、3、5、7，默认为3。注意：如果ksize为-1，就演变为3*3的Scharr算子。<br>scale：缩放导数的比例常数，默认情况为没有伸缩系数。<br>borderType：图像边界的模式，默认值为cv2.BORDER_DEFAULT。</p><p>Sobel函数求完导数后会有负值，还会有大于255的值。而原图像是uint8，即8位无符号数，所以Sobel建立的图像位数不够，会有截断。因此要使用16位有符号的数据类型，即cv2.CV_16S。处理完图像后，在使用cv2.convertScaleAbs()函数将其转回原来的uint8格式，否则图像无法显示。</p><p>Sobel算子是在两个方向计算的，最后还需要用cv2.addWieghted()函数将其组合起来。</p><pre class=" language-language-python"><code class="language-language-python">Scale_abs = cv2.convertScaleAbs(x)result = cv2.addWeighted(src1, alpha, src2, beta)</code></pre><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport  numpy as npfrom matplotlib import pyplot as plt# 读取图像img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# 计算Sobel卷积结果x = cv.Sobel(img, cv.CV_16S, 1, 0)y = cv.Sobel(img, cv.CV_16S, 0, 1)# 将数据进行转换Scale_absX = cv.convertScaleAbs(x)Scale_absY = cv.convertScaleAbs(y)# 结果合成result = cv.addWeighted(Scale_absX, 0.5, Scale_absY, 0.5, 0)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.subplot(121), plt.imshow(img, cmap=plt.cm.gray), plt.title("from")plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(result, cmap=plt.cm.gray), plt.title("result")plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4165.png" alt=""></p><h5 id="Laplacian算子">Laplacian算子</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/lp.PNG" alt=""></p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">laplacian = cv2.Laplacian(src,ddepth,dst,ksize,scale,delta,borderType)</code></pre><p>src：需要处理的图像。<br>ddepth：图像的深度，-1表示采用的是原图像相同的深度，目标图像的深度必须大于等于原图像的深度。<br>ksize：算子的大小，即卷积核的大小，必须为1、3、5、7。</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport  numpy as npfrom matplotlib import pyplot as plt# 读取图像img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# laplacian转换result = cv.Laplacian(img, cv.CV_16S)Scale_abs = cv.convertScaleAbs(result)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.subplot(121), plt.imshow(img, cmap=plt.cm.gray), plt.title("from")plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(Scale_abs, cmap=plt.cm.gray), plt.title("laplacian")plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4166.png" alt=""></p><h5 id="Canny边缘检测">Canny边缘检测</h5><p>canny边缘检测算法是一种非常流行的边缘检测算法，被认为是最优的边缘检测算法。</p><p>原理：</p><p>第一步：噪声去除<br>由于边缘检测很容易受到噪声的影响，所以首先用5*5的高斯滤波器去除噪声。<br>第二步：计算图像梯度<br>对平滑后的图像使用Sobel算子计算水平方向和竖直方向的一阶导数，根据得到的这两幅梯度图找到边界的梯度和方向。如果某个像素点是边缘，则其梯度方向总是与边缘垂直。梯度方向被归为四类：垂直、水平、和两个对角线方向。<br>第三步：非极大值抑制<br>在获得梯度的方向和大小之后，对整幅图像进行扫描，去除非边界上的点。对每一个像素进行检查，看这个点的梯度是不是周围具有相同梯度方向的点中最大的。<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/cann.PNG" alt=""></p><p>A点位于图像的边缘，在其梯度变化方向，选择像素点B和C，用来检验A点的梯度是否为极大值，若为极大值，则进行保留，否则A点被抑制，最终的结果是具有细边的二进制图像。<br>第四步：滞后阈值<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/yu.PNG" alt=""></p><p>现在要确定真正的边界。我们设置两个阈值：minVal和maxVal。当图像的灰度梯度高于maxVal时被认为是真的边界，低于minVal的边界会被抛弃。如果介于两者之间的话，就要看这个点是否与某个被确定为真正的边界点相连，如果是就认为它也是边界点，如果不是就抛弃。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">canny = cv2.Canny(image,threshold1,threshold2)</code></pre><p>image：灰度图。<br>threshold1：minVal，较小的阈值将间断的边缘连接起来<br>threshold2：maxVal，较大的阈值将检测图像中明显的边缘</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport  numpy as npfrom matplotlib import pyplot as plt# 读取图像img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg', 0)# canny边缘检测lowThreshold = 0max_lowThreshold = 100canny = cv.Canny(img, lowThreshold, max_lowThreshold)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.subplot(121), plt.imshow(img, cmap=plt.cm.gray), plt.title("from")plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(canny, cmap=plt.cm.gray), plt.title("canny")plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4167.png" alt=""></p><h5 id="模板匹配和霍夫变换">模板匹配和霍夫变换</h5><p>模板匹配</p><p>原理：</p><p>所谓模板匹配，就是给定的图片中查找和模板最相似的区域，该算法的输入包括模板和图片，整个任务的思路就是按照滑窗的思路不断的移动模板图片，计算其与图像中对应区域的匹配度，最终将匹配度最好的区域选择为最终的结果。</p><p>实现流程：<br>准备两幅图像：<br>1.原图像：在这幅图中，找到与模板相匹配的区域<br>2.模板：与原图像进行对比的图像块</p><p>滑动模板图像和原图像进行比对：<br>将模板块每次移动一个像素(从左往右，从上往下)，在每一个位置，都计算与模板图像的相似程度。<br>对于每一个位置将计算的相似结果保存在结果矩阵R中，如果输入图像的大小为W*H，而模板图像的大小为w*h，则输出矩阵R的大小为(W-w+ 1，H-h+1)将R显示为图像。<br>获取上述图像后，查找最大值所在的位置，那么该位置对应的区域就被认定是最匹配的。对应的区域就是以该点为顶点，长宽和模板图像一样大小的矩阵。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">res = cv2.matchTemplate(img,template,method)</code></pre><p>img：要进行模板匹配的对象<br>template：模板<br>method：实现模板匹配的算法，主要有：<br>1.平方差匹配(CV_TM_SQDIFF)：利用模板和图像之间的平方差进行匹配，最好的匹配是0，匹配越差，匹配的值越大。<br>2.相关匹配(CV_TM_CCORR)：利用模板与图像间的乘法进行匹配，数值越大表示匹配程度越高，越小表示匹配效果越差。<br>3.利用相关系数匹配(CV_TM_CCOEFF)：利用模板与图像间的相关系数匹配，1表示完美的匹配，-1表示最差的匹配。</p><p>完成匹配后，使用cv.minMaxLoc()方法查找最大值所在的位置即可。如果使用平方差作为比较方法，则最小值位置是最佳匹配位置。</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport  numpy as npfrom matplotlib import pyplot as plt# 图像和模板读取img = cv.imread('/Users/dinggongcheng/Downloads/pythondemo.jpeg')template = cv.imread('/Users/dinggongcheng/Downloads/template.PNG')h, w, l = template.shape# 模板匹配res = cv.matchTemplate(img, template, cv.TM_CCOEFF)# 返回图像中最匹配的位置，确定左上角的坐标，并将匹配位置绘制在图像上min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)# 使用平方差时最小值为最佳匹配位置top_left = max_locbottom_right = (top_left[0] + w, top_left[1] + h)cv.rectangle(img, top_left, bottom_right, (0, 255, 0), 2)# 图像显示plt.imshow(img[:, :, ::-1])plt.title("result"), plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4168.png" alt=""></p><p>扩展：模板匹配不适用尺度变换，视角变幻后的图像，这时我们就要使用关键点匹配算法，比较经典的关键点检测算法包括SIFT和SURF等，主要的思路是首先通过关键点检测算法获取模板和测试图片中的关键点；然后使用关键点匹配算法处理即可，这些关键点可以很好的处理尺度变化、视角变换、旋转变换、光照变换等，具有很好的不变性。</p><h5 id="霍夫变换">霍夫变换</h5><p>原理：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/hu.PNG" alt=""></p><p>变换后的空间我们叫做霍夫空间。即：笛卡尔坐标系中的一条直线，对应于霍夫空间中的一个点。反过来，同样成立，霍夫空间中的一条线，对应笛卡尔坐标中的一个点。</p><p>如果在笛卡尔坐标系的点共线，那么这些点在霍夫空间中对应直线交于一点。</p><p>如果不止存在一条直线时，我们选择尽可能多的直线汇成的点。当点的横坐标相同时，将笛卡尔坐标系转换为极坐标，在极坐标下是一样的，极坐标中的点对应于霍夫空间的线，这时的霍夫空间不再是参数(k,q)的空间，而是极坐标的空间，分别为原点到直线的垂直距离和直线的垂线与横轴顺时针方向的夹角，垂直线的角度为0度，水平线的角度是180度。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/ji.PNG" alt=""></p><p>我们只要求得霍夫空间中的交点的位置，即可得到原坐标系下的直线。</p><p>实现流程：</p><p>假设有一个大小为100*100的图片，使用霍夫变换检测图中的直线，则步骤如下：</p><p>直线都可以使用极坐标系表示，首先创建一个2D数组，我们叫做累加器，初始化所有值为0，行表示ρ，列表示θ。该数组的大小决定了结果的准确性，若希望角度的精度为1度，那就需要180列。对于ρ，最大值为图片对角线的距离，如果希望精度达到像素级别，行数应该与图像的对角线的距离相等。<br>取直线上的第一个点(x,y)，将其带入直线在极坐标中的公式，然后遍历θ的取值：0、1、2、…、180，分别求出对应的ρ值，如果这个数值在你上述累加器中存在相应的位置，则在该位置上加1。<br>取直线上的第二个点，重复上述步骤，更新累加器中的值。对图像中的直线上的每个点执行以上步骤，每次更新累加器中的值。<br>搜索累加器中的最大值，并找到其对应的(ρ,θ)，就可将图像中的直线表示出来。</p><h5 id="霍夫线检测">霍夫线检测</h5><pre class=" language-language-python"><code class="language-language-python">cv.Houghlines(img,rho,theta,threshold)</code></pre><p>img：检测的图像，要求是二值化的图像，所以在调用霍夫变换之前首先要进行二值化，或者进行canny边缘检测。<br>rho，theta：ρ和θ的精确度。<br>threshold：阈值，只有累加器中的值高于该阈值时才被认定是直线。</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport  numpy as npfrom matplotlib import pyplot as plt# 加载图片，转为二值图img = cv.imread('/Users/dinggongcheng/Downloads/lineandcircle.PNG')gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)edges = cv.Canny(gray, 50, 150)# 霍夫曼直线变换lines = cv.HoughLines(edges, 0.8, np.pi / 180, 150)# 将检测的线绘制在图像上for line in lines:    rho, theta = line[0]    a = np.cos(theta)    b = np.sin(theta)    x0 = a * rho    y0 = b * rho    x1 = int(x0 + 1000 * (-b))    y1 = int(y0 + 1000 * a)    x2 = int(x0 - 1000 * (-b))    y2 = int(y0 - 1000 * a)    cv.line(img, (x1, y1), (x2,y2), (0, 255, 0))# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.imshow(img[:, :, ::-1])plt.title("houghlines"), plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/4169.png" alt=""></p><h5 id="霍夫圆检测">霍夫圆检测</h5><p>原理：</p><p>圆的表示是(x-a)²+(y-b)²=r²，其中a和b表示圆心坐标，r表示圆半径，因此标准的霍夫圆检测就是在这三个参数组成的三位空间累加器上进行圆形检测，此时效率就会很低，所以OpenCV中使用霍夫梯度法进行圆形的检测。</p><p>霍夫梯度法将霍夫圆检测范围分为两个阶段，第一阶段检测圆心，第二阶段利用圆心推导出圆半径。<br>圆心检测的原理：圆心是圆周法线的交汇处，设置一个阈值，在某店的相交的直线的条数大于这个阈值就认为该交汇点为圆心。<br>圆半径确定原理：圆心到圆周上的距离是相同的，确定一个阈值，只要相同距离的数量大于该阈值，就认为该距离是该圆心的半径。</p><p>原则上霍夫变换可以检测任何形状，但复杂的形状需要的参数就多，霍夫空间的维数就多，因此在程序实现上所需要的内存空间以及运行效率上都不利于把标准霍夫变换应用于实际复杂图形的检测中。霍夫梯度法是霍夫变换的改进，它的目的是减小霍夫空间的维度，提高效率。</p><p>API：</p><pre class=" language-language-python"><code class="language-language-python">circles = cv.HoughCircles(images,method,dp,minDist,param1=100,param2=100,minRadius=0,maxRadius=0)</code></pre><p>images：输入图像，应输入灰度图像<br>method：使用霍夫变换圆检测的算法，它的参数是CV_HOUGH_GRADIENT<br>dp：霍夫空间的分辨率，dp=1时表示霍夫空间与输入图像空间的大小一致，dp=2时霍夫空间时输入图像空间的一半<br>minDist：为圆心之间的最小距离，如果检测到的两个圆心之间距离小于该值，则认为它们是同一个圆心<br>param1：边缘检测时使用canny算子的高阈值，低阈值是高阈值的一半<br>param2：检测圆心和确定半径时所共用的阈值<br>minRadius和maxRadius：所检测到的圆半径的最小值和最大值</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport  numpy as npfrom matplotlib import pyplot as plt# 加载图片，转为灰度图plants = cv.imread('/Users/dinggongcheng/Downloads/lineandcircle.PNG')gray_img = cv.cvtColor(plants, cv.COLOR_BGR2GRAY)# 进行中值模糊，去燥点img = cv.medianBlur(gray_img, 7)# 霍夫圆检测circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 20, param1=100, param2=30, minRadius=0, maxRadius=100)# 将检测结果绘制在图像上for i in circles[0, :]:    # 绘制圆形    cv.circle(plants, (i[0], i[1]), int(i[2]), (0, 255, 0), 2)    # 绘制圆心    cv.circle(plants, (i[0], i[1]), 2, (0, 255, 0), -1)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.imshow(plants[:, :, ::-1])plt.title("houghCircle"), plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/41610.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习笔记一</title>
      <link href="/2021/04/15/shen-du-xue-xi-bi-ji-yi/"/>
      <url>/2021/04/15/shen-du-xue-xi-bi-ji-yi/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习笔记一">深度学习笔记一</h3><h4 id="名词">名词</h4><p>函数的Loss：深度学习的错误率<br>Regression：回归算法，找到一个函数 function，通过输入特征 x ，输出一个数值 Scalar 。与其他任务不同的是输出一个数值<br>Classification：分类，输入参数，通过某种模型计算后，输出所属类别<br>Meta Learning：让机器学会学习，第一个任务是语音识别，第二个任务是图像识别，第一百个任务是文本分类，机器会因为之前所学到得任务，所以在后面得任务学习得更好。<br>Unsupervised Learning：无监督学习，数据没有label，需要根据数据自身结构特性来将数据分类<br>Life-long Learning：终身学习，对多个任务使用一个模型进行学习，期望机器学完第一个任务后，再学习学第二个任务后，不会遗忘第一个任务，继续学习第三个任务后，不会忘记第一个和第二个任务。<br>Reinforcement Learning：强化学习，估计未来的收益，并根据这些收益对<strong>当前动作进行强化</strong>的算法设计思想是<strong>强化学习</strong><br>Domain Adversarial Learning：对抗训练，在对抗训练的过程中，样本会被混合一些微小的扰动（改变很小，但是很可能造成误分类），然后使神经网络适应这种改变，从而对对抗样本具有鲁棒性。<br>Network Compression：模型压缩<br>Anomaly Detection：<strong>异常检测</strong>，目的在于让机器知道我所不知道的事情<br>Explainable AI：可解释AI</p><h4 id="Regression">Regression</h4><p>Step1:Model<br>视频通过宝可梦的例子进行讲解，这里我通过新闻推荐的方式对相关内容进行理解。<br>函数，即通过线性模型构造函数。y = b + w*x(similar)；<br>这里的y为推荐新闻的推荐指数，而b可以为新闻分类的相似度下时间发表的前后或者新闻的阅读量等因素构成的推荐数值。而w为用户行为(点赞，评论，收藏，分享)分析的推荐数值。x(similar)则为被推荐新闻与用户阅读过新闻关键字的匹配值。<br>其中x中的各种属性，即关键字匹配度或者发表机构的匹配度等为输入特征。w为权重，x为偏差。</p><p>Step2:Goodness of Function<br>获取多个数据通过表格，折线等多种形式进行分析。<br>定义Loss function L：衡量参数的好坏<br>通过对应的估测误差的函数来计算所有已有样本根据不同function计算的Loss值。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/ss.png" alt=""></p><p>Step3:Best Function<br>根据Loss值获取最好的function。<br>gradient descent：处理function找到最优值<br>1.穷举<br>2.Gradient Descent<br>单参<br>随机选取或者其他方法找到一个初始点w0；<br>计算w0位置的微分。为负的话应该增加w的值。为正的话应该减少w的值。<br>增量选取：根据微分的大小(陡峭程度)；常数项learning rate(事先定义好的值)。<br>重复之前的步骤。<br>经过t次的更新，找到local optimal微分为0的位置。局部最优解。<br>双参<br>和单参数相同，不过是计算双参数的微分。使用同一个常数项learning rate。<br>对于三维Loss Function函数的计算是根据选取点的，而对线性模型得到的是同一组参数。<br>根据原图像与得到的最优的function进行误差估测。<br>获取新的模型用于测试得到的最优函数，在进行误差估测。</p><p>Step4：新的模型误差较大或者某个局部预测不准<br>引入二次式进行再次预测或继续引入三次方再次预测。<br>由于现有模型的不足和分布不均，使用较高的次方进行预测的原数据模型对当前数据模型误差可能增大。产生overfitting。<br>高次方程的范围是低次方程的超集，在找出最优function的情况下，根据最优function在找到最优的误差。</p><p>Step5：根据较大的数据模型和分布重新设计function<br>显然根据不同的新闻的分类或者不同的新闻分类下的再次细分设计不同的function。<br>使用switch区分分类来构成线性函数。线性函数则由x的多个特征来设计。</p><p>Step6：若仍想取最优<br>将所有x的特征都加入到function中，并采取多次函数进行预测。可能会产生overfitting。<br>Regularization：<br>第二步中的估计误差的loss function选取更好的误差估计函数。<br>在进行regularization的时候，进行图像的平滑程度调整不需要考虑偏差，偏差只是让图像上下移动而已。<br>误差函数新增的项的值应该较小，使加上的项产生的函数较平滑。输入对输出的影响较小。<br>根据结果得出，新增的变量的值越大，图像越平滑。变量的值越大，越依赖于w，则error越大。<br>而对于新的测试数据，平滑图像的最优也可能导致error更大。</p><h4 id="Error来源">Error来源</h4><p>f*来自于训练得出的最优function，而程序使用的function为fhat<br>其中f*距fhat的‘距离’来自于bias和variance。<br>计算f*的数学期望 E[f*] 。bias即E[f*] 与f hat的误差，而variance即E[f*] 与f*的方差。<br>对于简单的模型函数拥有较低的variance，拥有较大的bias。而对于复杂的模型函数拥有较大的variance，拥有较小的bias。<br>对于简单的模型函数可能不包含你的fhat，而较复杂的模型函数可能包含你的fhat，但是variance较大。<br>对于模型函数来讲，当准度高即bias越来越小的时候，你的精度低即variance会越来越大。<br>对于训练数据得到的f*没有fit的情况下，bias较大。而对于测试数据测试f*时，产生的error较大，则应该考虑variance较大。<br>当bias较大时，应输入更多特征，优化模型函数。而variance较大时，应增大训练数据或者regularization(可能平滑之后无法包含fhat)。<br><strong>训练数据应分为两组，一组为Training set，另一组为Validation set。通过训练得到的f*在Validation set上进行验证，或者使用全部的训练数据。</strong><br>N-fold Cross Validation：将训练数据分为三份，以组合的形式(两份训练，一份验证)训练模型函数得到f*，再在全部的训练数据中进行测试。</p><h4 id="Gradient-Descent">Gradient Descent</h4><p>表达式：</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/grad.png" alt=""></p><h4 id="tuning-your-learning-rate">tuning your learning rate</h4><p>可视化参数的变化对loss的影响，当折线平稳下降时，rate较小，而折现下降平稳之后，则rate较大，当折线开始上升则rate错误。<br>learning rate会随着参数的更新越来越小。每一个不同的参数都应该有不同的learning rate。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/desct.png" alt=""></p><p>Adagrad：选取一个数值为参数之前的均方根进行更新</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/Adagrad.png" alt=""></p><p>g为偏微分。</p><p>推荐使用Adam。</p><p>Adagrad会使用除值来弥补反差。当进行跨参数时，分子和一次微分成正比，分母和二次微分成反比。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/er.png" alt=""></p><p>对于跨参数而言，参考如图即可得知，在a位置的一次微分较小，二次微分也较小。而对于c的位置一次微分较大，同时二次微分也较大。Adagrad就是通过分母预估二次微分的值。</p><h4 id="Stochastic-Gradient-Descent">Stochastic Gradient Descent</h4><p>计算单个样本的Loss和单个Gradient</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sss.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sgrad.png" alt=""></p><p>每个样本更新一次参数，计算较快。</p><h4 id="Feature-Scaling">Feature Scaling</h4><p>当输入时考虑多个特征，且多个特征的规模相差较大时，可以rescaling将多个特征的规模调整到基本一致。<br>当多特征的选值相差较大时，会发现较大的特征值变更时对于loss的变化是较大的，而特征值小变化较小。所以小的特征值方向上比较平滑，而特征值大方向上的比较陡峭。这个时候参数的update寻找f*时是比较难的，消耗的时间较长。<br>而对特征进行rescaling之后，正圆形的参数更新是比较方便的。</p><p>rescaling的方法</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/resc.png" alt=""></p><h4 id="Gradient-Descent背后原理">Gradient Descent背后原理</h4><p>在使用gradient descent的时候，随着每次参数的更新，loss的值可能增大也可能也减小。</p><p>泰勒公式：有</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tai.png" alt="img"></p><p>这是在对函数进行局部线性化处理时常用的公式之一。从几何上看，它是用切线近似代替曲线。<br><strong>让近似多项式函数在x=x0处的y值, 一阶导, 二阶导 …n阶导的值 = 原始函数在x=x0处的y值, 一阶导, 二阶导 …n阶导</strong><br>泰勒公式几何意义，一看就懂。<a href="https://www.zhihu.com/question/21149770" target="_blank" rel="noopener">https://www.zhihu.com/question/21149770</a><br>多维也是如此。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/sasa.png" alt=""></p><p>而对于gradient descent来说，给定一个初始点和范围，可以使用泰勒公式计算最小的loss；而计算最小值的计算公式即为gradient。而learning rate设置时即要保证使用到的泰勒公式要成立，即x-x0的高阶导为无穷小量时。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tr.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次查漏补缺一</title>
      <link href="/2021/04/14/ji-yi-ci-cha-lou-bu-que-yi/"/>
      <url>/2021/04/14/ji-yi-ci-cha-lou-bu-que-yi/</url>
      
        <content type="html"><![CDATA[<h3 id="记一次查漏补缺一">记一次查漏补缺一</h3><h4 id="业务功能：不使用mysql或oracle实现持续签到领奖">业务功能：不使用mysql或oracle实现持续签到领奖</h4><p>技术实现：使用redis的bitmap实现</p><p>bitmap基于最小的单位bit进行存储，设置时候时间复杂度O(1)、读取时候时间复杂度O(n)，方便扩容。</p><p>签到信息以用户id和年数月数为key，所在天数为偏移值进行签到，1为签到，0为未签到。</p><p>伪代码实现</p><pre class=" language-language-java"><code class="language-language-java">//签到jedis.setbit(getKey(userId,keyDate), dayDate, true);//获取签到情况//获取位于偏移量 0 上的 monthDayLength 位长无符号数List<Long> list = jedis.bitfield(getKey(userId,keyDate), "GET", monthDayLength, "0");//业务逻辑处理断签...//统计签到次数jedis.bitcount(getKey(userId,keyDate))</code></pre><p>redis的接口文档地址：<a href="http://redisdoc.com/bitmap/index.html" target="_blank" rel="noopener">http://redisdoc.com/bitmap/index.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习笔记二</title>
      <link href="/2021/04/14/python-xue-xi-bi-ji-er/"/>
      <url>/2021/04/14/python-xue-xi-bi-ji-er/</url>
      
        <content type="html"><![CDATA[<h3 id="python学习笔记二">python学习笔记二</h3><h4 id="循环结构">循环结构</h4><p>语法：<br>while a &gt; 0:<br>print(a)<br>for a in list:<br>print(a)<br>对于for in循环，不需要使用到自定义变量，可将自定义变量写为_</p><h4 id="列表">列表</h4><p>语法：<br>list = [“string”,int,float]<br>其中list()为内置函数<br>lst2 = list([“string”,int,float])<br>混淆点：python中列表中任意数据类型混存，列表的索引存在负值的索引</p><p>API：<br>.index(var) var：列表中的元素 列表中有相同元素，只返回列表中相同元素的第一个元素的索引<br>.index(var,start,end) start,end：指定查找的列表索引范围，前闭后开。<br>list[start:stop:step] step：步长 获取列表中的多个元素。获取到的对象为新对象，占用新的内存空间，而非对原数据的操作。步长为负时，将截取的列表逆序输出。相应的start与stop也逆序设置。<br>var in list：判断元素在列表中是否存在<br>.append(var)：在列表尾追加一个元素。对原数据进行操作，不产生新的对象。元素的类型可以为列表。<br>.extend(var)：追加元素时，若添加的元素为列表，则解析列表，以单元素的方式追加。<br>.insert(index,var)：在索引为index的位置上插入var元素<br>list[start:] = lst：将切掉的部分进行替换<br>.remove(var)：移除列表中的var元素，存在相同元素时，只移除第一个<br>.pop(index)：根据列表索引index移除元素。参数列表为空移除列表的最后一个元素<br>list[start:stop] = []：使用空元素替代<br>.clear()：清除列表中的所有元素<br>del var：删除列表，回收列表占用的内存空间<br>.sort(reverse=True)：对当前列表进行排序，默认由小至大排序。reverse设置为true进行降序排序。<br>sorted(var,reverse=True)：使用内置函数对列表进行排序，默认生序排序，排序生成新的列表对象。使用reverse参数值进行降序排序。</p><p>列表生成式：<br>[i for i in range(1,10)] 将1到9产生的整数作为列表，for之前的元素可以作为表达式进行对列表元素进行设置</p><h4 id="字典">字典</h4><p>语法：<br>map = {key:value, key:value,key:value}<br>dict()为内置函数<br>dict(name=‘var’,age=20)<br>面向对象，json序列化<br>字典中的key无法重复，同时必须是不可变对象。根据hash值存放元素，因此字典是无序的。</p><p>API：<br>map[key]：根据key获取value值<br>get(key,default)：根据key获取value值。不存在key时返回None，不报错。可以使用default指定查找失败时的默认值。<br>del map[key]：删除指定key的键值对<br>clear()：清空字典的元素<br>keys()：获取字典中的所有key，类型为dict_keys。可以通过list()进行转换<br>values()：获取字典中的所有values，类型为dict_values。<br>items()：获取字典中的所有键值对。</p><p>字典生成式：<br>zip(var1,var2)：var1与var2为可遍历对象。遍历时根据两个列表的索引动态生成字典。<br>{item:price for item, price in zip(items,prices)} 打包时以短的列表为基准，for之前为表达式</p><h4 id="元组">元组</h4><p>语法：<br>(‘var’,‘str’,int) 可以省略括号进行简写<br>使用内置函数tuple()<br>tuple((‘str’,int,float))<br>元组中只包含一个元素时，需要添加逗号。否则会被认定为String<br>元组的不可变类比于java中的final。</p><h4 id="集合">集合</h4><p>集合是没有value的字典，对比于java的set集合<br>语法：<br>{2,3,3,4,5,6,7} 集合中的value值不允许重复<br>使用内置函数set()<br>set(range(6))</p><p>API：<br>add(var)：向集合中添加元素<br>update({var1,var2,var3})：向集合中添加多个元素。同时可以向集合中添加列表和元组，列表和元组会被解析为单个元素添加。<br>remove(var)：删除集合中的元素<br>discard(var)：删除集合中的元素，不会报错值不存在异常<br>pop()：无参api，删除任意元素<br>clear()：清空集合元素<br>==和!=：判断时判断集合中的数据，而非地址<br>var1.issubset(var2)：var1是var2的子集判断<br>var2.issuperset(var1)：var2是var1的超集判断<br>var1.isdisjoint(var2)：var1和var2的交集判断</p><p>集合的数学操作：<br>交集：var1.intersection(var2)     var1 &amp; var2     集合不变化<br>并集：var1.union(var2)     var1 | var2     集合不变化<br>差集：var1.difference(var2)   var1-var2    集合不变化<br>对称差集：var1.symmetric_difference(var2)</p><p>集合生成式：<br>{i for i in range(1,10)} 和列表生成式一致，区别在于{}和[]</p><h4 id="字符串的操作">字符串的操作</h4><p>字符串驻留，与java相同。<br>驻留机制：<br>1.字符串长度为0或者1<br>2.符合标识符的字符串<br>3.只在编译时进行驻留，在程序运行时会开辟新的空间。<br>4.[-5,256]间的整数数字</p><p>API：<br>index(var)：查找子串第一次出现的位置，未出现子串报异常<br>find(var)：查找子串第一次出现的位置，未出现子串返回-1<br>rindex(var)：查找子串最后一次出现的位置<br>rfind(var)：查找子串最后一次出现的位置<br>upper()：转换字符为大写字母，产生一个新的字符串对象<br>lower()：转换字符为小写字母，产生一个新的字符串对象<br>swapcase()：大小写互相转换<br>capitalize()：第一个字符转换为大写，其余字符转换为小写<br>title()：把每个单词的第一个字符转换为大写，把每个单词的剩余字符转换为小写。<br>center(var1,var2)：居中对齐，var1指定宽度，var2指定填充符，var2可选，默认为空格，如果设置宽度小于实际宽度则返回原字符串<br>ljust(var1,var2)：左对齐，var1指定宽度，var2指定填充符，var2可选，默认为空格，如果设置宽度小于实际宽度则返回原字符串<br>rjust(var1,var2)：右对齐，var1指定宽度，var2指定填充符，var2可选，默认为空格，如果设置宽度小于实际宽度则返回原字符串<br>zfill(var)：右对齐，左边用0填充，该方法只接收一个参数，用于指定字符串的宽度，如果设置宽度小于实际宽度则返回原字符串<br>split(var1,var2)：从字符串的左侧开始劈分，默认为空格，返回一个列表。通过var1参数设置劈分的字符，通过var2设置劈分的最大次数，剩余的子串会单独做为一部分保存。<br>rsplit(var1,var2)：从字符串的右侧开始劈分，默认为空格，返回一个列表。通过var1参数设置劈分的字符，通过var2设置劈分的最大次数，剩余的子串会单独做为一部分保存。<br>isidentifier()：判断指定的字符串是不是合法的标识符<br>isspace()：判断指定的字符串是否全部由空白字符组成<br>isaplha()：判断指定的字符串是否全部由字母组成<br>isdecimal()：判断指定字符串是否全部由十进制的数字组成<br>isnumeric()：判断指定的字符串是否全部由数字组成<br>isalnum()：判断指定字符串是否全部由字母和数字组成<br>replace(var1,var2,var3)：var1指定被替换的子串，var2指定替换子串的字符串，该方法返回替换后得到的字符串，替换前的字符串不发生变化，调用该方法可以通过var3指定最大替换次数<br>join()：将列表或元组中的字符串合并成一个字符串</p><p>格式化字符串</p><p>%s %d % (name,age)<br>{0} {1} .format(name,age)<br>f {name} {age}<br>%10d 10表示宽度<br>%.3f % 3.1415926 .3保留小数<br>{0:.3}.format(3.1415926) .3一共表示三位数<br>{0:.3f}.format(3.1415926) .3f表示三位小数<br>{0:10.3f}.format(3.1415926) 宽度为10</p><p>字符串的编码转换</p><p>即序列化<br>encode(var) var即编码格式 GBK中文两个字节 UTF-8中文三个字节<br>decode(var) var即解码格式</p><h4 id="函数">函数</h4><p>语法：<br>def main(var1,var2):<br>业务逻辑<br>return var3<br>Python函数为值传递 具体的详细业务与java一样<br>函数返回值为多个时，返回的数据类型为元组</p><p>参数：<br>可变的位置参数：*args，传入的参数为一个元组。可变的位置参数只能为一个<br>个数可变的关键字形参：**args，传入的参数为一个字典。个数可变的关键字形参只能为一个。<br>在一个函数的定义过程中，既有个数可变的关键字形参，也有个数可变的位置形参，要求个数可变的位置参数在个数可变的关键字形参之前。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring boot相关知识点</title>
      <link href="/2021/03/30/springboot-xiang-guan-zhi-shi-dian/"/>
      <url>/2021/03/30/springboot-xiang-guan-zhi-shi-dian/</url>
      
        <content type="html"><![CDATA[<h3 id="Springboot中如何解决跨域问题？">Springboot中如何解决跨域问题？</h3><p>在脚本进行http请求的时候要满足同源协议，即url的协议，域名和端口要相同才能才能发起请求。浏览器在脚本发出跨域请求后，会拦截返回的结果，所以需要配置跨域。</p><p>当用户退出或者token过期时，拦截器和跨域的顺序配置会导致跨域问题，一个http请求，先走filter，到达servlet后才进行拦截器的处理。一般在前后端分离部署时要解决跨域问题，通过cors来实现跨域问题。</p><p>如果不是前后端分离部署的项目。可以通过实现WebMvcConfigurer然后重写addCorsMappings方法来实现跨域配置。</p><pre class=" language-language-java"><code class="language-language-java">@Configurationpublic class CorsConfig implements WebMvcConfigurer {    @Override    public void addCorsMappings(CorsRegistry registry) {        registry.addMapping("/**")                .allowedOrigins("*")                .allowCredentials(true)                .allowedMethods("GET", "POST", "PUT", "DELETE", "OPTIONS")                .maxAge(3600);    }}</code></pre><p>把cors放在filter里面，就可以在拦截器之前进行。</p><pre class=" language-language-java"><code class="language-language-java">@Configurationpublic class CorsConfig {    @Bean    public CorsFilter corsFilter() {        CorsConfiguration corsConfiguration = new CorsConfiguration();        corsConfiguration.addAllowedOrigin("*");        corsConfiguration.addAllowedHeader("*");        corsConfiguration.addAllowedMethod("*");        corsConfiguration.setAllowCredentials(true);        UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource();        urlBasedCorsConfigurationSource.registerCorsConfiguration("/**", corsConfiguration);        return new CorsFilter(urlBasedCorsConfigurationSource);    }}</code></pre><h3 id="SpringBoot的核心配置文件有哪几个？他们的区别是什么？">SpringBoot的核心配置文件有哪几个？他们的区别是什么？</h3><p>Spring boot的核心配置文件有两个，application和bootstrap。application主要用于spring boot的自动化配置。spring boot使用@EnableConfigurationProperties注解映射application中的参数和POJO的关系。</p><p>bootstrap配置文件的加载要优于application，且里面配置的属性不能被覆盖。使用spring Cloud config 配置中心时，这时需要在bootstrap 配置文件中添加连接到配置中心的配置属性来加载外部配置中心的配置信息。</p><h3 id="Spring-boot的异常处理">Spring boot的异常处理</h3><h4 id="Spring-Boot类BasicErrorController">Spring Boot类BasicErrorController</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/20190608165028699.png" alt=""></p><p>这是一个基础全局错误controller,Spring Boot自带的，看到这个RequestMapping地址，这是一个相当于三元写法，如果你在配置文件配置了server.error.path的话，就会使用你配置的异常处理地址，如果没有就会使用你配置的error.path路径地址，如果还是没有，默认使用/error来作为发生异常的处理地址。</p><p>默认的类在你的请求返回文本或者json数据的时候，当发生错误的话，如果你在配置文件里面配置了模板引擎并且有error的页面，这个类就会返回你的error页面。返回的数据通过getErrorAttributes方法封装你的路径，状态，信息，时间戳等信息。</p><p>可以通过自定义一个bean来实现ErrorController来屏蔽默认的异常处理。如果不想全部屏蔽，可以自定义一个bean来继承BasicErrorController接口来使用部分功能，并自定义自己的错误映射地址。</p><h4 id="统一异常处理">统一异常处理</h4><pre class=" language-language-java"><code class="language-language-java">package com.riemann.springbootdemo.exception;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.HttpServletRequest;@ControllerAdvicepublic class GlobalExceptionHandle extends RuntimeException{    private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandle.class);    @ExceptionHandler(value = Exception.class)    public ModelAndView exceptionHandle(HttpServletRequest request, Exception exception) {        String url = request.getRequestURL().toString();        logger.error("URL: " + url);        logger.error("Excption: ", exception);        ModelAndView mv = new ModelAndView();        mv.addObject("exception", exception);        mv.addObject("url", url);        mv.setViewName("error");        return mv;    }}</code></pre><p>统一处理Exception异常，只需要在类上标注ControllerAdvice这个注解，然后在类方法上标注好对应的ExceptionHandler及异常类。ControllerAdvice可以用来进行全局异常处理，全局数据绑定和全局数据预处理。</p><h3 id="Spring-Boot与Spring的区别">Spring Boot与Spring的区别</h3><h4 id="Spring">Spring</h4><p>spring框架为开发Java应用程序提供了全面的基础架构支持，它包含一些很好的功能，如依赖注入和开箱即用的模块。</p><p><strong>Spring JDBC</strong></p><p>spring中的JDBC主要为数据库资源管理和错误处理，简化开发人员对数据库的操作。其中，JDBC Template是JDBC的核心类，使用核心类提供的方法来操作数据库。</p><p><strong>Spring MVC</strong></p><p>Spring MVC将传统的模型层拆分为了业务层(Service)和数据访问层(DAO)。在 Service 下可以通过 Spring 的声明式事务操作数据访问层，而在业务层上还允许我们访问 NoSQL。其中，M即模型，数据，V代表视图，C为控制器，将不同的数据显示在不同的视图上。</p><p><strong>Spring Security</strong></p><p>Spring Security的核心功能为认证，授权和攻击防护。核心为使用Basic Authentication Filter过滤器认证用户身份。</p><p><strong>Spring AOP</strong></p><p>不想多说</p><p><strong>Spring ORM</strong></p><p>对象关系映射，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系数据库中。</p><p><strong>Spring Test</strong></p><p>并不重要</p><h4 id="Spring-Boot">Spring Boot</h4><p>Spring Boot基本上是Spring框架的扩展，它消除了设置Spring应用程序所需的复杂例行配置。</p><ul><li>通过starter这一个依赖，以简化构建和复杂的应用程序配置</li><li>可以直接main函数启动，嵌入式web服务器，避免了应用程序部署的复杂性</li><li>Metrics度量，Helth check健康检查和外部化配置</li><li>自动化配置Spring功能</li></ul><h4 id="两者的区别">两者的区别</h4><h5 id="maven依赖">maven依赖</h5><p>Spring创建Web应用程序所需的最小依赖项：</p><pre class=" language-language-xml"><code class="language-language-xml">org.springframeworkspring-web5.1.0.RELEASEorg.springframeworkspring-webmvc5.1.0.RELEASE</code></pre><p>与Spring不同，Spring Boot只需要一个依赖项来启动和运行Web应用程序：</p><pre class=" language-language-xml"><code class="language-language-xml">org.springframework.bootspring-boot-starter-web2.0.5.RELEASE</code></pre><p>在构建期间，所有其他依赖项将自动添加到最终归档中。spring-boot-starter-web包自动帮我们引入了web模块开发需要的相关jar包。spring-boot-starter-web会自动引入spring-webmvc，spring-boot-starter-validation，spring-boot-starter，spring-boot-starter-json，spring-boot-starter-tomcat5个基础依赖。</p><h5 id="MVC配置">MVC配置</h5><p>通过实现WebApplicationInitializer，在其中可以添加servlet，listener等，在加载Web项目的时候会加载这个接口实现类，从而起到web.xml相同的作用</p><pre class=" language-language-java"><code class="language-language-java">package org.springframework.web; import java.lang.reflect.Modifier;import java.util.Iterator;import java.util.LinkedList;import java.util.List;import java.util.Set;import javax.servlet.ServletContainerInitializer;import javax.servlet.ServletContext;import javax.servlet.ServletException;import javax.servlet.annotation.HandlesTypes;import org.springframework.core.annotation.AnnotationAwareOrderComparator; @HandlesTypes({WebApplicationInitializer.class})public class SpringServletContainerInitializer implements ServletContainerInitializer {    public SpringServletContainerInitializer() {    }     public void onStartup(Set<Class<?>> webAppInitializerClasses, ServletContext servletContext) throws ServletException {        List<WebApplicationInitializer> initializers = new LinkedList();        Iterator var4;        if(webAppInitializerClasses != null) {            var4 = webAppInitializerClasses.iterator();             while(var4.hasNext()) {                Class<?> waiClass = (Class)var4.next();                if(!waiClass.isInterface() && !Modifier.isAbstract(waiClass.getModifiers()) && WebApplicationInitializer.class.isAssignableFrom(waiClass)) {                    try {                        initializers.add((WebApplicationInitializer)waiClass.newInstance());                    } catch (Throwable var7) {                        throw new ServletException("Failed to instantiate WebApplicationInitializer class", var7);                    }                }            }        }         if(initializers.isEmpty()) {            servletContext.log("No Spring WebApplicationInitializer types detected on classpath");        } else {            servletContext.log(initializers.size() + " Spring WebApplicationInitializers detected on classpath");            AnnotationAwareOrderComparator.sort(initializers);            var4 = initializers.iterator();             while(var4.hasNext()) {                WebApplicationInitializer initializer = (WebApplicationInitializer)var4.next();                initializer.onStartup(servletContext);            }         }    }}</code></pre><p>判断webAppInitializerClasses这个Set是否为空。如果不为空的话，找到这个set中不是接口，不是抽象类，并且是WebApplicationInitializer接口实现类的类，将它们保存到list中。当这个list为空的时候，抛出异常。不为空的话就按照一定的顺序排序，并将它们按照一定的顺序实例化。调用其onStartup方法执行。</p><pre class=" language-language-java"><code class="language-language-java">public class MyWebAppInitializer implements WebApplicationInitializer {    @Override    public void onStartup(ServletContext container) {        AnnotationConfigWebApplicationContext context          = new AnnotationConfigWebApplicationContext();        context.setConfigLocation("com.test.package");        container.addListener(new ContextLoaderListener(context));        ServletRegistration.Dynamic dispatcher = container          .addServlet("dispatcher", new DispatcherServlet(context));        dispatcher.setLoadOnStartup(1);        dispatcher.addMapping("/");    }}</code></pre><p>我们还需要将@EnableWebMvc注解添加到@Configuration注解类，并定义一个视图解析器来解析从控制器返回的视图。在使用该注解后配置一个继承于WebMvcConfigurerAdapter的配置类即可配置好Spring WebMVC。</p><pre class=" language-language-java"><code class="language-language-java">@EnableWebMvc@Configurationpublic class ClientWebConfig implements WebMvcConfigurer {   @Bean   public ViewResolver viewResolver() {      InternalResourceViewResolver bean        = new InternalResourceViewResolver();      bean.setViewClass(JstlView.class);      bean.setPrefix("/WEB-INF/view/");      bean.setSuffix(".jsp");      return bean;   }}</code></pre><p>与所有这些相比，一旦我们添加了Spring boot web starter，Spring Boot只需要一些属性来使上面的事情正常工作。</p><pre class=" language-language-yml"><code class="language-language-yml">spring.mvc.view.prefix=/WEB-INF/jsp/spring.mvc.view.suffix=.jsp</code></pre><p>上面的所有Spring配置都是通过一个名为auto-configuration的进程添加Boot web starter来自动包含的。这意味着Spring Boot将自动扫描应用程序中存在的依赖项，属性和bean，并根据这些内容启用相应的配置。</p><p>SpringBoot 自动配置主要通过 <code>@EnableAutoConfiguration</code>, <code>@Conditional</code>, <code>@EnableConfigurationProperties</code> 或者 <code>@ConfigurationProperties</code> 等几个注解来进行自动配置完成的。</p><p><code>@EnableAutoConfiguration</code> 开启自动配置，主要作用就是调用 <code>Spring-Core</code> 包里的 <code>loadFactoryNames()</code>，将 <code>autoconfig</code> 包里的已经写好的自动配置加载进来。</p><p><code>@Conditional</code> 条件注解，通过判断类路径下有没有相应配置的 <code>jar</code> 包来确定是否加载和自动配置这个类。</p><p><code>@EnableConfigurationProperties</code> 的作用就是，给自动配置提供具体的配置参数，只需要写在 <code>application.properties</code> 中，就可以通过映射写入配置类的 <code>POJO</code> 属性中</p><h5 id="模板引擎">模板引擎</h5><p>在Spring中，我们需要为视图解析器添加  thymeleaf-spring5依赖项和一些配置</p><pre class=" language-language-java"><code class="language-language-java">@Configuration@EnableWebMvcpublic class MvcWebConfig implements WebMvcConfigurer {    @Autowired    private ApplicationContext applicationContext;    @Bean    public SpringResourceTemplateResolver templateResolver() {        SpringResourceTemplateResolver templateResolver = new SpringResourceTemplateResolver();        templateResolver.setApplicationContext(applicationContext);        templateResolver.setPrefix("/WEB-INF/views/");        templateResolver.setSuffix(".html");        return templateResolver;    }    @Bean    public SpringTemplateEngine templateEngine() {        SpringTemplateEngine templateEngine = new SpringTemplateEngine();        templateEngine.setTemplateResolver(templateResolver());        templateEngine.setEnableSpringELCompiler(true);        return templateEngine;    }    @Override    public void configureViewResolvers(ViewResolverRegistry registry) {        ThymeleafViewResolver resolver = new ThymeleafViewResolver();        resolver.setTemplateEngine(templateEngine());        registry.viewResolver(resolver);    }</code></pre><p>ApplicationContext是spring继BeanFactory之外的另一个核心接口或容器，允许容器通过应用程序上下文环境创建、获取、管理bean。为应用程序提供配置的中央接口。</p><p>Spring Boot 只需要spring-boot-starter-thymeleaf的依赖项 来启用Web应用程序中的Thymeleaf支持。一旦依赖关系添加成功后，我们就可以将模板添加到src / main / resources / templates文件夹中，Spring Boot将自动显示它们。具体过程也参考上方的自动配置。</p><h5 id="安全配置">安全配置</h5><p>Spring需要标准的  spring-security-web和spring-security-config  依赖项来在应用程序中设置Security。接下来，我们需要添加一个扩展WebSecurityConfigurerAdapter的类，并使用@EnableWebSecurity注解</p><pre class=" language-language-java"><code class="language-language-java">@Configuration@EnableWebSecuritypublic class CustomWebSecurityConfigurerAdapter extends WebSecurityConfigurerAdapter {    @Autowired    public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception {        auth.inMemoryAuthentication()          .withUser("user1")            .password(passwordEncoder()            .encode("user1Pass"))          .authorities("ROLE_USER");    }    @Override    protected void configure(HttpSecurity http) throws Exception {        http.authorizeRequests()          .anyRequest().authenticated()          .and()          .httpBasic();    }    @Bean    public PasswordEncoder passwordEncoder() {        return new BCryptPasswordEncoder();    }}</code></pre><p>WebSecurityConfigurerAdapter 类是个适配器, 在配置的时候,需要我们自己写个配置类去继承他,然后编写自己所特殊需要的配置。</p><p>Spring Boot也需要这些依赖项才能使其工作。但是我们只需要定义spring-boot-starter-security的依赖关系，它会自动将所有相关的依赖项添加到类路径中。</p><h5 id="应用引导Application-Bootstrap">应用引导Application Bootstrap</h5><p><strong>web.xml引导方法</strong></p><ol><li>Servlet容器（服务器）读取web.xml</li><li>web.xml中定义的DispatcherServlet由容器实例化</li><li>DispatcherServlet通过读取WEB-INF / {servletName} -servlet.xml来创建WebApplicationContext</li><li>最后，DispatcherServlet注册在应用程序上下文中定义的bean</li></ol><p><strong>servlet 3+引导方法</strong></p><p><strong>在web容器启动时为提供给第三方组件机会做一些初始化的工作，例如注册servlet或者filtes等，servlet规范中通过ServletContainerInitializer实现此功能。每个框架要使用ServletContainerInitializer就必须在对应的jar包的META-INF/services 目录创建一个名为javax.servlet.ServletContainerInitializer的文件，文件内容指定具体的ServletContainerInitializer实现类，那么，当web容器启动时就会运行这个初始化器做一些组件内的初始化工作。</strong></p><ol><li>容器搜索实现ServletContainerInitializer的 类并执行</li><li>SpringServletContainerInitializer找到实现类WebApplicationInitializer的子类</li><li>WebApplicationInitializer创建会话使用XML或上下文@Configuration类</li><li>WebApplicationInitializer创建DispatcherServlet，使用先前创建的上下文。</li></ol><p><strong>Spring Boot引导</strong></p><p>详情参考Spring Boot启动过程的文章。</p><p><strong>参考文章：<a href="https://mp.weixin.qq.com/s/0qk2kaCKLdAViVzsw401sg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/0qk2kaCKLdAViVzsw401sg</a></strong></p><h3 id="Shiro知识点">Shiro知识点</h3><p>shiro和核心概念为subject，securityManager和realm。subject即为与当前应用交互的主体，主体将交互请求委托给securityManager。securityManager通过realm判断当前用户认证信息，并授权用户的身份和角色信息，判断是否具有相关权限。</p><h4 id="自定义relam">自定义relam</h4><p>AuthorizingRealm中的doGetAuthenticationInfo用于身份验证，doGetAuthorizationInfo用于授权。</p><h4 id="身份认证过程">身份认证过程</h4><p>主体subject调用subject.login(token)，将请求委托给securityManager。</p><p>Authenticator获取token信息并传入realm，从realm获取身份验证信息。</p><p>realm根据加密方式查询数据库进行验证，验证成功返回成功信息。可以单个realm验证，也可以多realm验证，根据不同策略判断是否需要多个realm同时验证成功。</p><p>doGetAuthenticationInfo获取身份验证相关信息：首先根据传入的用户名获取User信息；如果user为空，那么抛出没找到账号异常UnknownAccountExecption；如果user找到但却被锁定了抛出锁定异常LockedAccountException；最后生成AuthenticationInfo信息，交给间接父类AuthenticatingRealm使用CredentialsMatcher进行判断密码是否匹配，如果不匹配将抛出密码错误异常信息IncorrectCredentialsException；如果密码重试次数太多将抛出超出重试次数异常ExcessiveAttemptsException；在组装SimpleAuthenticationInfo信息时，需要传入：身份信息（用户名）、凭据（密文密码）、盐（username+salt），CredentialsMatcher使用盐加密传入的明文密码和此处的密文密码进行匹配。</p><h4 id="授权过程">授权过程</h4><p>身份验证成功，通过Authorization进行授权。可以在后台使用逻辑代码判断是否具有权限和角色，也可以使用注解给接口授权，也可以使用前台标签的形式进行授权。</p><p>同时可以在shiro的配置文件中放开登录接口和静态资源的访问权限。</p><p>doGetAuthorizationInfo获取授权信息：PrincipalCollection是一个身份集合，因为只用到了一个Realm，所以直接调用getPrimaryPrincipal得到之前传入的用户名即可；然后根据用户名调用UserService接口获取角色及权限信息。</p><h4 id="Session-Manager">Session Manager</h4><p>Session Manager是securityManager的父类，维护subject主体的相关信息。Session manager一般将会话委托也就是继承的方式给securityManager进行处理。</p><h3 id="JWT知识点">JWT知识点</h3><h4 id="jwt和security的对比">jwt和security的对比</h4><p>jwt使用token验证，jwt无需在服务器端存储用户数据，减轻服务端压力。security是将用户数据保存在服务器端。</p><p>jwt是可以跨语言的，不想security针对某个语言进行开发一个适用的框架。</p><p>随之而来的就是对token的控制问题。</p><h4 id="如何解决token注销问题">如何解决token注销问题</h4><p>减少token的有效期，使token尽快失效。</p><p>使用redis，前端发起请求时，携带的jwt数据信息，把信息中的唯一的id存到redis里面，验证的时候查询redis中是否有这个id，在token注销的时候删除redis里面的id即可。</p><h4 id="怎么解决token失效后的续签问题">怎么解决token失效后的续签问题</h4><p>在用户登录验证的时候根据相关标准判断当前的token是否要过期。如果token快要过期，重新生成token就可以。</p><h4 id="解决cookie被盗用问题">解决cookie被盗用问题</h4><p>如果收到XSS攻击的话，设置一下cookie的httponly为true，防止脚本攻击。</p><p>对于抓包的话使用https协议即可。</p><p>对于CSRF攻击，在请求头中加个随机码。</p><h4 id="cookie被禁用怎么办">cookie被禁用怎么办</h4><p>判断一下用户的cookie是否禁用，如果禁用提示用户打开即可。</p><h4 id="解决cookie被篡改问题">解决cookie被篡改问题</h4><p>cookie被篡改，但是jwt验证是通过签名进行验证的，签名不对，后台的jwt验证就通过不了。</p><h4 id="服务端微服务地址不小心暴露了，用户就可以绕过网关，直接访问微服务，怎么办">服务端微服务地址不小心暴露了，用户就可以绕过网关，直接访问微服务，怎么办</h4><p>一般来说微服务都是使用nginx代理的，一般暴露的都是nginx的地址。</p><p>如果担心接口暴露，那也可以在微服务之间通信也使用jwt验证即可。</p><h4 id="jwt流程分析">jwt流程分析</h4><p>在前端，jwt头部记录信息和加密方式，然后进行编码。载荷记录token的对象和时间等信息，也进行编码。以句号拼接两部分的编码内容使用密钥和算法进行加密形成签名。服务器端以相同的方式即相同的密码和算法进行签名得到一直的结果验证成功返回客户端jwt信息，客户端请求接口时携带jwt信息就可以了。</p><h3 id="redis知识点">redis知识点</h3><h4 id="redis的特点">redis的特点</h4><p>redis的数据存在于内存之中，因此redis的读写速度很快，所以常用作缓存。</p><p>支持AOF和RDB两种数据持久化方式。</p><p>支持事务，redis的所有操作都是原子性的。</p><p>支持主从复制，主机可以将数据同步到从机。</p><p>redis数据存在于内存中，所以不适合海量数据的读写。</p><p>redis对扩容的兼容性较差。</p><h4 id="为什么要使用redis">为什么要使用redis</h4><p>redis是运行在内存的，当一个项目多户发起多次请求访问数据库时，访问速度很慢，如果将数据库的数据放在redis中用作缓存，那么用户访问redis中的数据即可。当数据库的数据变更时，同步一下redis中的数据即可。加快访问速度，提供系统的性能。</p><p>当使用用户较多时，多用户同时访问数据库，数据库的压力较大，可能承受不住，而运行在内存中的redis支持高并发，因此使用redis可以减轻数据库的压力，也可以提升系统性能。</p><p>redis属于分布式缓存，多实例公用一份缓存数据，缓存具有一致性。</p><h4 id="redis为什么访问速度快">redis为什么访问速度快</h4><p>redis运行在内存之中所以访问速度快。</p><p>redis使用的是单线程，不存在线程切换带来的性能消耗。</p><p>数据结构简单，有string，list，set，zset，hash五种数据结构。io采用多路复用方式。</p><h4 id="redis的应用场景">redis的应用场景</h4><p>用作计数器，在新闻中，对于热门新闻的点击，可以使用redis的string进行记录，速度快。</p><p>缓存，对于经常访问到的热点数据，可以同步到redis中，减轻数据库的压力。</p><p>存储会话信息，使用jwt的时候，可以用来存储jwt的唯一id，在登录逻辑中进行判断，注销token时删除redis中的id就行。</p><p>对于交集并集数据，查找共同好友或者笔记共同的分类，可以使用redis的set集合实现。</p><h4 id="redis的持久化">redis的持久化</h4><p>持久化就是将数据持久化到硬盘之中，防止宕机之后的数据丢失。redis提供了AOF和RDB两种数据持久化方式。</p><p>RDB是redis的默认的持久化方式，按照时间持久化数据，时间可在配置中自己定义，效率比较高。且持久化时主进程处理命令，子进程负责io操作，保证了redis的高性能。但是因为是按照时间进行持久化，会发生数据丢失。</p><p>AOF将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。AOF通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。数据安全，每进行一次 命令操作就记录到 aof 文件中一次。AOF的文件较大，且效率较低。</p><p>通常应该同时使用AOF和RDB两种持久化方式，因为AOF的持久化方式数据较为完整，但是使用RDB的持久化文件可以作为数据库的备份来使用，因为他恢复较快。</p><h4 id="redis过期键的删除策略">redis过期键的删除策略</h4><p>一般来说内存较大或者对内存的占用性能影响不大的情况下可以使用惰性过期，在访问key的时候在判断当前的key是否过期，如果过期的话就删除。</p><p>如果数据较多，会严重占用内存，考虑定期过期，每隔一段时间扫描key，然后清楚过期的key即可。</p><p>一般不使用定时过期，因为会浪费cpu资源，影响redis性能。</p><h4 id="redis的内存淘汰">redis的内存淘汰</h4><p>当redis中的数据满，而数据库的数据仍然在持续增加时。</p><p>如果系统的需求是当前redis中的数据已经满足使用，则不进行淘汰，不插入新的数据到redis中。</p><p>如果像新闻需要实时更新热门新闻的话，可以将当前内存中不常使用的key进行删除。</p><p>而类似于普通的用户信息，不存在优先级问题，也可以随机删除key。</p><h4 id="redis的内存优化">redis的内存优化</h4><p>对于redis，不应按照平常面向对象的习惯使用key-value的形式存储数据，占用的空间会很大。在redis中，hash占用的空间较小，所以应将信息存储到hash中。</p><h4 id="redis事务">redis事务</h4><p>事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。redis的事务也是如此。</p><p>redis使用multi事务开始，然后命令入队，然后exec执行。事务执行时服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排。</p><p>一般事务具有原子性，隔离性，一致性和持久性。redis的事务具有一致性和隔离性，使用持久化时，具有持久性。在redis中，命令的执行是原子性的，但是事务不是，也无法进行回滚。</p><h3 id="rabbitmq知识点">rabbitmq知识点</h3><h4 id="为什么使用MQ">为什么使用MQ</h4><p>可以进行异步处理，用户的请求需要多个系统写入数据，如果延迟过高，用户体验较差。使用MQ，我把请求放在mq里，让延迟快的优先反应并返回信息给用户，让其他系统订阅mq，取消息在保存到数据库即可。</p><p>解耦，当前系统产生的数据可能被其他多个系统使用，让其他系统请求，当前系统分发的话，有可能产生不一致问题，那么使用MQ把数据放到MQ里，谁使用谁订阅并且取数据即可。</p><p>流量削峰，对于那些某时刻会有大量请求的系统，使用MQ的话，可以将请求先放在MQ中，先由MQ处理，之后在慢慢交给后台，防止数据量过大，服务器直接奔溃。</p><p>对于电商，用户下单，没有付款的话，使用死信队列，使当前的订单超过半个小时自动关闭。</p><p>使用MQ的时候除了他优点，要非常注意数据一致性问题，即把MQ当成中间件，要确保后面的数据写入要同步完成。</p><h4 id="为什么使用rabbitMQ">为什么使用rabbitMQ</h4><p>rabbitmq对高并发的处理和适配比较好，所以一般使用这个，而且使用的公司比较多，社区的更新频繁，bug少。</p><h4 id="保证消息的顺序">保证消息的顺序</h4><p>通过MQ自身的实现的话可能会影响MQ的性能，所以可以在业务层面保证消息消费的顺序，或者使无序的消息消费顺序不影响系统的使用。</p><h4 id="保证消息的重复性">保证消息的重复性</h4><p>保证接收端的幂等性即可。可以使用日志文件记录消息的唯一id，消费信息后更新日志id，再此消费时判断一下当前的日志中是否存在该消息的id即可。</p><h4 id="消息的路由方式">消息的路由方式</h4><p>通过队列的路由键将队列绑定到对应的交换器上，消息到达交换器时，根据消息创建时携带的路由和交换器中队列的路由进行匹配即可。</p><h4 id="消息基于什么传输">消息基于什么传输</h4><p>由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。</p><h4 id="确保消息被消费">确保消息被消费</h4><p>将信道设置为confirm方式，发送方发送的消息携带唯一的id，接收方消费完成之后需要返回确认给发送方，发送方才可以删除消息。这个过程是异步的，不会影响发送方继续发送消息。如果没有接收到确认方的信息，接收方断开或者取消订阅，就把消息送给下一个接收方。如果没有断开也没有返回确认信息，发送方判断接收方忙且不在发送数据。</p><h4 id="如何确保消息的可靠传输">如何确保消息的可靠传输</h4><p>对于发送方来说除了信道设置confirm方式采用唯一id等待确认方式之外，还可以使用事务，如果出错，事务回滚确保数据的安全。</p><p>队列的话可以设置一下持久化，将数据保存到磁盘，数据丢失可以进行恢复。</p><p>接收方通过业务逻辑手动回复确认即可。</p><h4 id="如何解决MQ中的消息积压问题">如何解决MQ中的消息积压问题</h4><p>新建一个 topic模式(通配符的方式)，分区是原来的 10 倍，临时建立好原先 10 倍的队列 数量。然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 队列中。接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。</p><h3 id="mysql面试题">mysql面试题</h3><h4 id="数据库的三大范式">数据库的三大范式</h4><p>第一范式保证原子性，即当前字段不可在分割，当前字段不能由多条信息组成。</p><p>第二范式是不应存在联合主键，每条记录要有唯一性。</p><p>第三范式保证数据库的单挑数据的字段中不存在依赖关系，字段之间应保证独立。</p><h4 id="MySQL的binlog有有几种录入格式？分别有什么区别？">MySQL的binlog有有几种录入格式？分别有什么区别？</h4><p>binlog为mysql的二进制日志文件，记录数据库的操作语句，方便数据库进行主从复制时保证数据的一致性和产生宕机后的数据恢复问题。</p><p>binlog使用statement方式录入，会记录修改数据的sql语句，减少日志文件的大小。使用row方式录入，会记录表中每一行的改动，产生的日志文件较大，记录较全。mixed混合录入方式对于普通的操作语句使用statement方式录入，若无法使用statement，使用row方式录入。</p><h4 id="什么是索引？">什么是索引？</h4><p>索引相当于目录，建立索引可以方便数据的查找。索引是一种文件系统，占用物理空间，一般通过B树或者B+树来实现。</p><h4 id="索引的使用场景">索引的使用场景</h4><p>一般使用最多的是主键索引，通过主键id进行查询。新建的字段一般查询时都没有索引，当数据量较大时，可以考虑使用add index为字段添加索引来提高查询效率。</p><p>使用order by对数据库某个字段进行排序时，当字段没有索引的时候，系统会将所有的数据读入到内存中，再利用内部排序进行排序最后将每个部分进行合并，很大的浪费了性能。通过对排序字段添加索引，可以根据索引的顺序进行逐条读取数据。</p><p>建立所以可以提高连接查询的效率。</p><p>select和select *的区别，如果要查询的字段已经建立了索引，那么查询时就会根据索引查询，而不是访问原数据。所以推荐使用select加字段的方式，而select * 则会全表扫描。</p><h4 id="索引有哪几种类型">索引有哪几种类型</h4><p>使用最多的就是主键索引，唯一id，数据项不能重复且不能为空。</p><p>数据项不能重复且可以为空的为唯一索引。</p><p>数据项可以重复可以为空的为普通索引。</p><p>对于搜索引擎还有一种全文索引。</p><h4 id="索引的实现原理">索引的实现原理</h4><p>对字段添加索引，数据库会对添加索引的字段进行排序生成排序表，在排序表上记录数据的地址。对字段进行查询的时候会先找到排序表，在通过排序表中的数据地址获取数据。</p><h4 id="索引设计的原则">索引设计的原则</h4><p>如果你的查询条件中有&gt;,&lt;,between等条件，对于这些条件以后的字段不应设置索引，会失效，或者将带有这些条件的字段放在最后，因为mysql是默认向右匹配到这些条件就停止的。</p><p>建立索引用于提高查询效率，因此被频繁查询的字段可以添加索引，但是频繁更新的字段不应该建立索引。</p><p>对于已经有索引的表，添加新的字段索引的时候应有限扩展索引，即在当前的基础上扩展。</p><p>如果该列作为外键使用不应建立索引。</p><p>由于建立索引要进行排序，对于字段中的数据大量重复的情形或者字段较长的也不应该进行设置索引。设置短索引。</p><h4 id="创建索引的方式">创建索引的方式</h4><p>在创建表时添加索引。</p><p>已经表的基础上通过add index或者create index的方式创建索引。</p><h4 id="使用索引查询一定能提高查询的性能吗？为什么">使用索引查询一定能提高查询的性能吗？为什么</h4><p>索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能。</p><p>一般来说当你根据字段查询的数据为当前表的三分之一的时候性能提升较大。或者非唯一性索引进行查询性能提升也较大。</p><h4 id="对于百万级的数据的删除">对于百万级的数据的删除</h4><p>对于有着百万数据的表中的字段索引在删除数据前，应该先删除索引，删除索引后进行数据的删除。数据删除完毕之后重新创建索引较为安全。</p><h4 id="什么是聚簇索引？何时使用聚簇索引与非聚簇索引">什么是聚簇索引？何时使用聚簇索引与非聚簇索引</h4><p>聚簇索引就是将数据和索引放到一起，找到索引即找到了数据。</p><p>非聚簇索引数据和索引不在一起，优先将索引缓存到内存，查询数据先找到索引在读取磁盘找到数据。</p><h4 id="联合索引是什么？为什么需要注意联合索引中的顺序？">联合索引是什么？为什么需要注意联合索引中的顺序？</h4><p>联合索引是将多个字段的索引组合，联合索引是严格按照索引的排序顺序进行查询的，即先以第一个索引为基础排序，再以第二个排序。所以要严格按照联合索引的顺序使用。</p><h4 id="什么是脏读？幻读？不可重复读？">什么是脏读？幻读？不可重复读？</h4><p>脏读就是读到不该读的数据，A事务进行回滚前的数据被B事务读到了。</p><p>幻读就是没读全，A事务在插入数据前已经被B事务读了，A事务插入数据完成，B事务在读发现有一部分没读到。</p><p>不可重复读就是数据被别人动了，A事务进行逻辑处理的时候，B事务更新了数据，A事务使用的数据不是原来的。</p><h4 id="什么是事务的隔离级别？MySQL的默认隔离级别是什么？">什么是事务的隔离级别？MySQL的默认隔离级别是什么？</h4><p>数据库定义的隔离级别阻止脏读幻读和不可重复读的发生。</p><p>read-uncommited读取未提交，最低的隔离级别，容易导致脏读幻读和不可重复读。</p><p>read-commited读取已提交，读取事务已经提交的数据，防止脏读。</p><p>repeatable-read可重复读，对同一个字段的读取结果是一致的，除非自身的事务更新了数据，可以防止脏读和不可重复读。</p><p>serializable：可串行化，满足事务的四个性质，最高隔离级别，可以防止脏读，幻读和不可重复读。</p><p>mysql的默认级别是可重复读，在使用innoDB引擎的时候分布式事务使用可串行化。</p><h4 id="隔离级别与锁的关系">隔离级别与锁的关系</h4><p>read-uncommited读取未提交读操作不使用共享锁。</p><p>read-commited读取已提交，读操作是使用共享锁，读操作完成之后释放锁。</p><p>repeatable-read可重复读，读操作使用共享锁，读操作完成之后不释放锁，事务完成之后在释放锁。</p><p>serializable对于键一直保持锁，直到事务结束。</p><h4 id="按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法">按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法</h4><p>数据库的锁分为行级锁，表级锁和页级锁。</p><p>其中mysql的MyISAM引擎使用表级锁。</p><p>InnoDB使用行级锁和表级锁，默认使用行级锁。</p><p>行级锁是粒度最细的锁，对行进行加锁，减少数据库操作的冲突，但是因为是对每行加锁，所以开销较大，也较为安全。</p><p>表级锁是对整张表加锁，粒度最大，发生的概率也最大，消耗较低。</p><p>页级锁兼容上面两种，做到中消耗，中冲突，每次锁定相邻的一组记录。</p><h4 id="从锁的类别上分MySQL都有哪些锁呢？">从锁的类别上分MySQL都有哪些锁呢？</h4><p>锁的类别区分mysql有共享锁和排他锁。</p><p>共享锁即读操作时使用的锁，数据可以被多个语句读，可以被加多个共享锁。</p><p>排他锁是写操作时使用的锁，只能加一个，和其他的所有锁互斥。</p><h4 id="MySQL中InnoDB引擎的行锁是怎么实现的？">MySQL中InnoDB引擎的行锁是怎么实现的？</h4><p>使用索引来实现行锁。使用for update</p><p>select * from tab_with_index where id = 1 for update;</p><p>且for update查询的字段要有索引，否则会使用表级锁。</p><h4 id="数据库的乐观锁和悲观锁是什么？">数据库的乐观锁和悲观锁是什么？</h4><p>在数据库操作并发时使用悲观锁和乐观锁两种手段确保数据安全。</p><p>悲观锁是在事务查询数据的时候就加锁，而乐观锁是在事务最后修改数据的时候在加锁。</p><p>当读操作角度的时候使用乐观锁可以提高系统性能。</p><p>而在写操作多的时候保证数据的安全使用悲观锁。</p><h4 id="为什么要使用视图？什么是视图？">为什么要使用视图？什么是视图？</h4><p>视图和表相似但是是一种虚拟表，在进行查询的时候根据从基本表即实表中查询的数据动态生成一个含有这些数据的视图。</p><p>在使用数据库时，只是用某几张表中的部分数据，可以使用视图，确保其他数据的安全性。</p><h4 id="视图有哪些特点？">视图有哪些特点？</h4><p>视图是一种虚表。</p><p>视图中的内容可以由来自不同的实表组合生成。</p><p>对于查询视图的操作不会影响的原表。</p><p>对于数据更新的操作则会更新原表，如果视图是由多个实表中的数据组合，不允许进行数据更改。</p><h4 id="视图的使用场景有哪些？">视图的使用场景有哪些？</h4><p>所使用到的数据来自多个表，且是多个表中的部分字段。</p><p>用户权限细分的时候，只给有具体权限的用户展示具体的数据。</p><p>重用sql语句时，多个业务逻辑都用到当前的sql语句查询的结果。</p><p>由于要生成视图，所以性能上会有所消耗。</p><h4 id="什么是存储过程？有哪些优缺点？">什么是存储过程？有哪些优缺点？</h4><p>存储过程是预编译的sql语句，可以进行模块化设计，使用的时候直接调用存储过程即可。</p><p>存储过程是预编译的且存在于数据库中，所以速度较快。同时重用性可以减少开发人员的工作量。可以进行权限的划分。</p><p>使用存储过程之后大量的存储过程维护较为麻烦。</p><h4 id="什么是触发器？触发器的使用场景有哪些？">什么是触发器？触发器的使用场景有哪些？</h4><p>触发器即触发某个操作时自动执行的代码。</p><p>可以进行级联更改。</p><p>由当前数据生成其他数据的相关信息。</p><h4 id="关联查询">关联查询</h4><p>内连接inner join</p><p>外连接，left join，左连接以左表为基础，右连接以右表为基础，没有的字段信息使用null。</p><h4 id="mysql中-in-和-exists-区别">mysql中 in 和 exists 区别</h4><p>in语句将外表和内表做hash连接，exists对外表进行loop循环。每次循环过程中在对内表进行查询。</p><p>在查询的表大小差不多时，in和exists的效率相同。</p><p>当查询的表大小差别大时，子查询的表为大表使用exists,小表使用in</p><p>使用not语句，not exists一定比not in效率高，因为not exists可以使用索引。</p><h4 id="varchar与char的区别">varchar与char的区别</h4><p>char的长度是固定的，为255，空白用空格填充，查询的速度较快。</p><p>varchar的长度是可变的，数据多长，varchar可以设置多长，不会浪费空间。</p><h4 id="如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到-或者说怎么才可以知道这条语句运行很慢的原因？">如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？</h4><p>使用explain命令查看语句的执行计划，通过type判断当前运行的语句是否使用了索引。eq_ref是在join中使用了索引关联，ref使用了非唯一索引，index_subquery在子查询中使用非唯一索引，index为遍历索引，ALL为全表扫描。</p><p>如果possible_keys为null应该考虑sql优化，因为该字段显示查询时所查询字段的索引。以及key字段显示当前使用的索引。</p><h4 id="SQL的生命周期？">SQL的生命周期？</h4><p>应用服务器和数据库服务器建立连接，数据库服务器获取应用服务器的sql语句，生成执行计划，查询数据并读取到内存进行逻辑处理，最后将数据返回给应用服务器，断开连接，释放资源。</p><h4 id="大表数据查询，怎么优化">大表数据查询，怎么优化</h4><p>使用索引或者使用缓存技术。</p><h4 id="超大分页怎么处理？">超大分页怎么处理？</h4><p>使用缓存或者使用带索引的字段进行子查询。</p><h4 id="为什么要尽量设定一个主键？">为什么要尽量设定一个主键？</h4><p>主键是一张表中一行数据唯一性的保证。在增删改查的时候，使用主键更快且能保证数据的安全和隐蔽。</p><h4 id="主键使用自增ID还是UUID？">主键使用自增ID还是UUID？</h4><p>推荐使用自增id，因为mysql使用的算法是B+树，叶子节点上按照顺序存放主键索引，若是自增id，插入数据进行往下排列即可。如果是uuid，若新的id与已有的id大小不一致会导致数据的移动，产生内存碎片消耗性能。</p><h4 id="优化查询过程中的数据访问">优化查询过程中的数据访问</h4><p>避免查询过多的不必要的数据，可以使用limit字段。尽量避免使用select *而应该使用select具体字段，尤其是在多表关联查询的时候更应该指定列名。对于热点数据的查询，应优先考虑使用缓存。</p><h4 id="优化特定类型的查询语句">优化特定类型的查询语句</h4><p>使用count(*)会直接统计列树，不建议使用count(列名)。当对count没有确切的需求是可以用explain近似值代替count。使用limit可以记录上一次查询的最大id。</p><h4 id="优化WHERE子句">优化WHERE子句</h4><p>在where子句中应避免使用null，!=和大于小于操作符。避免使用in，not in，避免使用函数，参数和表达式求值等。都会导致全表扫描。</p><h4 id="MySQL数据库cpu飙升到500-的话他怎么处理？">MySQL数据库cpu飙升到500%的话他怎么处理？</h4><p>查看当前的进程判断是否是mysqld的进程导致的cpu增加。如果是使用show processlist查看sql语句的性能消耗，找到消耗大的sql语句查看执行计划，进行sql优化。</p><h3 id="springboot知识点">springboot知识点</h3><h4 id="Spring-Boot-的核心注解是哪个？它主要由哪几个注解组成的？">Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？</h4><p>springboot的核心注解是springbootApplication，它包含三个注解分别是springbootConfiguration，EnableAutoConfiguration和ComponentScan。其中SpringbootConfiguration用于实现配置文件，EnableAutoConfiguration用于实现自动化配置，ComponentScan实现组件扫描。</p><h4 id="什么是-JavaConfig？">什么是 JavaConfig？</h4><p>javaconfig提供配置容器的纯java方法，从而避免使用繁杂的xml配置。可以直接使用类的方式来进行容器配置。通过泛型按照类型的方式来配置更加安全，便于重构。</p><h4 id="Spring-Boot-是否可以使用-XML-配置">Spring Boot 是否可以使用 XML 配置 ?</h4><p>可以使用@importResource注解来使用xml配置</p><h3 id="mybatis知识">mybatis知识</h3><h4 id="为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？">为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？</h4><p>全自动ORM hibernate可以根据对象关系映射获取数据，而mybatis半自动映射需要自己书写sql语句。</p><h4 id="mybatis的优势">mybatis的优势</h4><p>mybatis使用线程池，使用的时候去线程池中取即可，避免了创建释放导致的资源消耗。</p><p>mybatis将java代码和sql语句分离开来，配置在mapper.xml中，对系统进行解耦。</p><p>mybatis可以将类对象直接映射到sql语句，反之根据返回类型的设置可以将数据映射到类。</p><h4 id="MyBatis的工作原理">MyBatis的工作原理</h4><p>框架启动加载mybatis的配置文件，读取项目中的对应的mapper.xml中的sql语句。构造会话工厂sqlsessionfactory，创建sqlsession会话信息。使用exec执行器创建mappedstatement对象，mapped statement读取传入的参数，并将返回的数据映射到类对象。</p><h4 id="为什么需要预编译">为什么需要预编译</h4><p>预编译的sql语句在数据库拿到之后可以直接执行，减少数据库对复杂sql语句编译的消耗。</p><p>对于预编译产生的statement对象可以重复利用从而减少开销。</p><p>使用预编译可以防止sql注入。</p><h4 id="Mybatis都有哪些Executor执行器？它们之间的区别是什么？">Mybatis都有哪些Executor执行器？它们之间的区别是什么？</h4><p>mybatis有三种执行器，simple，reuse和batch</p><p>simple每执行一次查询或者更新创建一个statement对象，使用完后关闭。</p><p>reuse创建statement对象后，使用完不关闭，放入Map中存储进行重复使用。</p><p>batch执行更新时使用批处理，将所有sql添加到批处理之后统一执行。</p><h4 id="Mybatis中如何指定使用哪一种Executor执行器？">Mybatis中如何指定使用哪一种Executor执行器？</h4><p>可以在mybatis的配置文件中设置执行器。也可以在创建sqlsessionfactory工厂时将执行器的类型作为参数指定。</p><h4 id="Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？">Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？</h4><p>在mybatis中对于association和collection是支持延迟加载的，可以在配置文件中通过lazyLoadingEnable开启延迟加载。</p><p>使用cglib创建代理对象，当进行方法调用的时候发现值为空时，根据实现保存的sql查询到当前为空的值并进行赋值，然后在进行方法调用。</p><h4 id="和-的区别">#{}和${}的区别</h4><p>#{}会对sql语句进行预处理可以防止sql注入。而${}是直接将字符串进行替换。</p><h4 id="模糊查询like语句该怎么写">模糊查询like语句该怎么写</h4><p>一般用concat(’%’,#{},’%’)的形式拼接查询。</p><p>也可以在sql中使用bind标签，指定name和value，其中value进行组合查询字段，sql语句中使用bind标签定义的name。</p><h4 id="在mapper中如何传递多个参数">在mapper中如何传递多个参数</h4><p>可以使用占位符的方式，即0，1</p><p>可以在dao层的接口中使用Param注解，指定传入参数的名称</p><p>可以封装map，将map中的key与字段进行直接映射。</p><p>使用javabean，通过parameterType传入指定的类的地址进行类和字段的映射。</p><h4 id="Mybatis如何执行批量操作">Mybatis如何执行批量操作</h4><p>使用foreach进行迭代处理。</p><h4 id="如何获取生成的主键">如何获取生成的主键</h4><p>开启useGeneratedKeys标签，使用keyProperty指定返回的主键字段名称。</p><h4 id="当实体类中的属性名和表中的字段名不一样-，怎么办">当实体类中的属性名和表中的字段名不一样 ，怎么办</h4><p>在sql语句中可以定义别名。</p><p>使用resultMap定义映射关系，在sql标签中指定定义的resultMap进行映射。</p><h4 id="什么是MyBatis的接口绑定？有哪些实现方式？">什么是MyBatis的接口绑定？有哪些实现方式？</h4><p>将接口和sql语句绑定，使用接口即可调用sql进行数据查询。</p><p>使用注解的方式来绑定，但是耦合度太高。</p><p>一般使用namespace指定dao类的方式进行接口绑定。</p><h4 id="使用MyBatis的mapper接口调用时有哪些要求？">使用MyBatis的mapper接口调用时有哪些要求？</h4><p>方法名和sqlid要相同。</p><p>传入参数和返回参数的类型要在sql中通过parameterType和resultType进行定义。</p><h4 id="Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？">Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？</h4><p>顺序没有关系，当a标签调用b标签时，若a标签解析时发现b不存在，则会暂停a标签解析，解析需要的标签，需要的标签解析完毕之后，在解析a标签。</p><h4 id="MyBatis实现一对一，一对多有几种方式，怎么操作的？">MyBatis实现一对一，一对多有几种方式，怎么操作的？</h4><p>通过association标签实现一对一，通过collection标签实现一对多。</p><h3 id="Spring知识点">Spring知识点</h3><h4 id="版本迭代">版本迭代</h4><p>4.x 增加了lamda表达式，同时为了适配restfui接口格式，添加了restController接口。</p><p>5.x支持函数和响应式编程</p><h4 id="spring核心">spring核心</h4><p>spring核心包括IOC，AOP和DI</p><p>其中IOC是控制反转，spring框架起到了中介的作用，程序员只需要编程自己的需求，相应的提供由spring框架来完成。实现了可插拔式接口，降低了代码的耦合度。</p><p>为了提升组件重用的概率，使用DI，容器在系统运行期间进行依赖注入。通过DI来实现spring的IOC的功能。</p><p>常见的注入模式一般为setter注入和构造方法注入和注解注入。</p><p>使用setter方式注入一般使用xml的形式，使用bean标签进行类的注入。</p><p>使用注解注入一般使用autowired，service等注解实现。</p><p>aop是切面编程，一般使用注解的方式after，before，pointcut等注解实现，使用springboot之后进行日志记录或者接口防刷等，自定义注解后，在具体的方法上使用自己的注解即可。</p><h4 id="Component-和-Bean-有什么区别？">@Component 和 @Bean 有什么区别？</h4><p>@Component注解主要运用于类上面，使用SomponentScan进行扫描并注册到spring容器之中，而@bean主要运用于方法，标志某个类的实例，需要时在使用。</p><h4 id="Spring-中-bean-的作用域有几种类型？">Spring 中 bean 的作用域有几种类型？</h4><p>单例模式整个应用程序只创建一个bean。原型模式，每次注入都会创建一个bean。会话模式，每次会话创建一个bean，但只在web系统中有效。请求模式，每次请求创建一个bean，只在web系统中有效。</p><h4 id="spring实现事务的方式">spring实现事务的方式</h4><p>使用编程式或者注解的方法。一般使用主机的方法使用事务。声明式事务，底层是建立在 Spring AOP 的基础上，在方式执行前后进行拦截，并在目标方法开始执行前创建新事务或加入一个已存在事务，最后在目标方法执行完后根据情况提交或者回滚事务。<br>声明式事务的优点：不需要编程，减少了代码的耦合，在配置文件中配置并在目标方法上添加 @Transactional 注解来实现。</p><h4 id="Spring-声明式事务无效可能的原因有哪些？">Spring 声明式事务无效可能的原因有哪些？</h4><p>mysql使用MyISAM引擎。</p><p>注解只支持public方法。</p><h4 id="Spring-中的-Bean-是线程安全的吗？">Spring 中的 Bean 是线程安全的吗？</h4><p>bean默认是单例模式，所以是非线程安全的。可以设置为原型模式，每次注入创建一个bean可以保证线程安全。</p><h4 id="Spring-中都是用了哪些设计模式？">Spring 中都是用了哪些设计模式？</h4><p>在listen中进行监听，动作触发进行通知使用了观察者模式。</p><p>通过beanFactory或者ApplicationContext创建bean使用的是工厂模式。</p><p>bean的单例或者原型使用单例原型模式。</p><p>在创建代理类时，根据代理的是不是接口选择不同的代理方式使用的是策略模式。</p><h3 id="SpringMVC知识点">SpringMVC知识点</h3><h4 id="Spring-MVC的主要组件？">Spring MVC的主要组件？</h4><p>DispatcherServlet前端控制器，主要用于接收请求和响应结果。</p><p>HandlerMapping映射器，根据对应的映射路径找到handler。</p><p>HandlerAdapter适配器，用于执行handler。</p><p>handler程序员开发，相当于方法。</p><p>ViewResolver视图解析器，用于解析视图。</p><p>View视图，即程序员编写的前端页面。</p><h4 id="请描述Spring-MVC的工作流程？描述一下-DispatcherServlet-的工作流程？">请描述Spring MVC的工作流程？描述一下 DispatcherServlet 的工作流程？</h4><p>用户发起请求至DispatcherServlet，DispatcherServlet拿到请求之后调用handlerMapping映射器请求获取handler。HandlerMapping根据url映射地址找到handler后生成处理器对象返回给DispatcherServlet。DispatcherServlet拿到对象之后调用adapter适配器，适配器调用具体的handler处理逻辑，最终返回modelandView。DispatcherServlet将ModelAndView传给ViewResolver视图解析器解析。解析完成后送给view页面。DispatcherServlet渲染页面，将数据显示到视图中，最终响应给用户。</p><h4 id="MVC是什么？MVC设计模式的好处有哪些">MVC是什么？MVC设计模式的好处有哪些</h4><p>MVC是一种设计模式，采用三层架构的方式。将系统进行解耦，提高开发效率。</p><h4 id="Spring-MVC常用的注解有哪些？">Spring MVC常用的注解有哪些？</h4><p>@RequestMapping用于映射url和类中的方法。</p><p>@RequestBody将http请求中接收到的json数据转换为系统中的类。</p><p>@ResponseBody将返回的类数据转换为json返回给前端。</p><h4 id="Controller注解的作用">@Controller注解的作用</h4><p>@Controller负责接收DispatcherServlet的请求，然后交由对应的方法处理，最终再把返回的ModelAndView交给视图。在类上标注注解并在配置文件中加载对应的controller类的bean对象即可。</p><h4 id="Spring-MVC怎么样设定重定向和转发的？">Spring MVC怎么样设定重定向和转发的？</h4><p>转发使用forward，不改变页面路径。而重定向使用redirect，改变页面url。</p><h4 id="如何解决POST请求中文乱码问题，GET的又如何处理呢？">如何解决POST请求中文乱码问题，GET的又如何处理呢？</h4><p>POST请求在web.xml文件中配置一下CharacterEncodeingFilter的属性为utf-8即可。</p><p>GET请求修改一下tomcat服务器的配置文件。</p><h3 id="javaSE">javaSE</h3><h4 id="JVM、JRE和JDK的关系">JVM、JRE和JDK的关系</h4><p>jvm是虚拟机，java程序是运行在虚拟机上的，不同的平台有不同的虚拟机，因此java是可以跨平台的。</p><p>jre是虚拟机和java程序运行的必不可少的类库和各种包。</p><p>jdk是程序员使用的开发工具，包括jre和编译工具，打包工具等。</p><h4 id="什么是字节码？采用字节码的最大好处是什么">什么是字节码？采用字节码的最大好处是什么</h4><p>字节码是java代码通过虚拟机编译后产生的只面向虚拟机的.class文件。</p><p>使用字节码，由于字节码是面向虚拟机的，因此移植的时候安装具体的虚拟机即可，不用考虑平台。使用字节码，虚拟机解释语言的效率也比较高。</p><h4 id="Java有哪些数据类型">Java有哪些数据类型</h4><p>byte 1字节，int 4字节，boolean 一字节，double 8字节，long 8字节，char 2字节，short 2字节。float 4字节。</p><p>对应的包装类，Byte，Integer，Boolean，Double，Long，Character，Short，Float</p><h4 id="switch-是否能作用在-byte-上，是否能作用在-long-上，是否能作用在-String-上">switch 是否能作用在 byte 上，是否能作用在 long 上，是否能作用在 String 上</h4><p>java7之后switch是可以支持String的，byte是一只支持的。long是不支持的。</p><h4 id="用最有效率的方法计算-2-乘以-8">用最有效率的方法计算 2 乘以 8</h4><p>a&lt;&lt;b，&lt;&lt;相当于乘，&gt;&gt;相当于除，a为底数，b为次方。</p><h4 id="Math-round-11-5-等于多少？Math-round-11-5-等于多少">Math.round(11.5) 等于多少？Math.round(-11.5)等于多少</h4><p>round函数对于正数来讲向上取整，对于负数来说也是向上取整。</p><h4 id="short-s1-1-s1-s1-1-有错吗-short-s1-1-s1-1-有错吗">short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗</h4><p>+=符号会自动进行转型，不会发生错误。</p><h4 id="访问修饰符-public-private-protected-以及不写（默认）时的区别">访问修饰符 public,private,protected,以及不写（默认）时的区别</h4><p>public是同包下和其他包下，private是本类，protect是本包，子类。default是本包。</p><h4 id="和-的区别-2">&amp;和&amp;&amp;的区别</h4><p>和| ||相同，&amp;只是前一个成功，后一个就忽略。而&amp;&amp;是双方必须都成功。</p><h4 id="final-有什么用？">final 有什么用？</h4><p>final修饰变量无法更改。修饰方法无法重写。修饰类无法继承。</p><p>final修饰变量时，若变量是引用变量，地址无法更改，而内容可以更改。</p><h4 id="final-finally-finalize区别">final finally finalize区别</h4><p>finally用于异常捕获，无论异常是否发生都会执行，且在方法return之前执行。finalize是Object的方法，所有的类都继承，如果实现了该方法，在第一次垃圾回收该对象时，先调用该方法。第二次则进行标记直接回收。</p><h4 id="static存在的主要意义">static存在的主要意义</h4><p>即便没有创建对象，也可以使用类名调用变量和方法。</p><p>使用静态代码块来优化性能，在类中而不是方法中是无法进行变量运算的，可以使用静态代码块。</p><h4 id="static的独特之处">static的独特之处</h4><p>不属于创建的实例对象，而是被类的实例对象共享。</p><p>共享对象值的变化是通用的。</p><h4 id="break-continue-return-的区别及作用">break ,continue ,return 的区别及作用</h4><p>break是跳出当前的循环体。continue是跳出当前循环，进行下一轮。return是返回，屏蔽return以后的语句。</p><h4 id="抽象类和接口的对比">抽象类和接口的对比</h4><p>抽象类和接口都不能被实例化。其中包括的方法子类必须实现。</p><p>接口没有构造函数。抽象类的方法没有方法体。</p><p>接口只能被定义为public，抽象类只能继承一个。</p><p>接口的字段默认都是static和final的。</p><h4 id="在Java中定义一个不做事且没有参数的构造方法的作用">在Java中定义一个不做事且没有参数的构造方法的作用</h4><p>当调用子类的时候，子类没有使用super来调用父类的构造方法，就会默认调用父类的无参构造器。而如果父类没有无参构造器，则编译错误。</p><h4 id="内部类">内部类</h4><p>内部类为静态内部类，成员内部类，局部内部类和匿名内部类</p><p>静态内部类是类中的静态类，静态内部类无法访问外部的非静态变量。成员内部类是类内部定义在成员位置上的非静态类，成员内部类可以访问外部的静态和非静态变量，私有和非私有变量。局部内部类是定义在方法中的内部类，可以访问外部的所有方法和所有变量。匿名内部类是没有名字的内部类，首先匿名内部类实现接口或者继承一个类，匿名内部类中不能定义静态方法和静态变量，匿名内部类用到的外部参数需要设置为final类型。</p><h4 id="匿名内部类使用参数为什么要设置为final">匿名内部类使用参数为什么要设置为final</h4><p>在类中定义的成员变量存储在栈中，当方法销毁时会销毁成员变量，而匿名内部类仍然保有对成员变量的引用，因此如果不把变量设置为final，则会被销毁，匿名内部类会出错。</p><h4 id="重写和重载">重写和重载</h4><p>父类的构造函数无法继承因此无法被重写，可以被重载。重载意味着方法名相同，其他可以不同。而重写方法名相同，参数列表相同。返回值小于父类，可见性大于父类。</p><h4 id="和-equals-的区别是什么">== 和 equals 的区别是什么</h4><p>==对于对象的判断是判断引用地址，对于基本变量的判断是判断值。equals判断的是变量的值。</p><p>对于String类型的数据使用equals进行比较，由于String的equals方法被重写过，因此比较的是String的值。在创建String时，若在常量池中存在当前的字符串，则直接引用，否则重新创建对象。</p><h4 id="hashCode-与-equals">hashCode 与 equals</h4><p>hashcode是数据存储的索引，存储数据时根据hashcode的值进行存储，若存在相同的值，则不存，若hashcode计算相同，值不相同则发生冲突在进行散列。</p><p>两个对象相等，则hashcode一定相等。若hashcode相等，则两个对象不一定相等。</p><p>hashcode是对堆上的对象产生独特值，若不重写，则两个class类永远不会相等。</p><h4 id="对象的相等与指向他们的引用相等，两者有什么不同？">对象的相等与指向他们的引用相等，两者有什么不同？</h4><p>对象的相等指值的相等。而引用相等，意味着所在的内存地址相等。</p><h4 id="值传递">值传递</h4><p>java中只存在值传递。当一个基本数据变量通过参数使用时，参数只是一份原先数据的拷贝，对于其他方法中对变量的操作不会影响到原变量。当对象引用作为参数传递时，方法中对参数的更改会更改原对象的相关数据。</p><h4 id="java-中-IO-流分为几种">java 中 IO 流分为几种?</h4><p>输入输出流，字节字符流</p><h4 id="什么是反射机制？">什么是反射机制？</h4><p>反射即在程序运行期间动态的获取当前类的属性和方法。可以使用指定位置或者forClassName指定类名的方式获取对应的属性和方法。</p><h4 id="String真的是不可变的吗？">String真的是不可变的吗？</h4><p>不可变，String是用final修饰的无法被继承，底层使用的是char型的数组。</p><h4 id="在使用-HashMap-的时候，用-String-做-key-有什么好处？">在使用 HashMap 的时候，用 String 做 key 有什么好处？</h4><p>String是不可变的，因此使用String作为key得到的hashcode被缓存后不需要再此计算。</p><h4 id="Integer-a-127-与-Integer-b-127相等吗">Integer a= 127 与 Integer b = 127相等吗</h4><p>当int型变量在-128到127之间时，会在常量池中调用已有的变量值。当超过这个范围，会创建新的对象。</p><h4 id="String和StringBuffer、StringBuilder的区别是什么？">String和StringBuffer、StringBuilder的区别是什么？</h4><p>当字符串使用较多且不存在多线程的时候建议使用StringBuilder，存在缓冲区，减少了String频繁的性能消耗。使用多线程下使用StringBuffer，StringBuffer是线程安全的。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring Boot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>opencv学习笔记一</title>
      <link href="/2021/03/29/opencv-xue-xi-bi-ji-yi/"/>
      <url>/2021/03/29/opencv-xue-xi-bi-ji-yi/</url>
      
        <content type="html"><![CDATA[<h3 id="opencv学习笔记一">opencv学习笔记一</h3><h4 id="opencv模块">opencv模块</h4><p>core：实现了最核心的数据结构及其基本运算，如绘图函数，数组操作相关函数等。</p><p>highgui：实现了视频与图像的读取，显示，存储等接口</p><p>imgproc：实现了图像处理的基础方法，包括图像滤波，图像的几何变换，平滑，阈值分割，形态学处理，边缘检测，目标检测，运动分析和对象跟踪等等。</p><p>features2d：用于提取图像特征以及特征匹配，nonfree模块实现了一些专利算法，如sift特征。<br>objectect：实现了一些目标检测的功能，经典的基于haar，LBP特征的人脸检测，基于HOG的行人，汽车等目标检测，分类器使用Cascade Classification(级联分类)和Latent SVM等。<br>stitching：实现了图像拼接功能。<br>FLANN：包含快速近似最近邻搜索FLANN和聚类Clustering算法<br>ml：机器学习模块(SVM，决策树，Boosting等等)<br>photo：包含图像修复和图像去噪两部分<br>video：针对视频处理，如背景分离，前景检测，对象跟踪等。<br>calib3d：即Calibration3D，这个模块主要是相机校准和三维重建相关的内容。包含了基本的多视角几何算法，单个立体摄像头标定，物体姿态估计，立体相似性算法，3D信息的重建等等。<br>G-API：包含超高效的图像处理pipline引擎</p><h4 id="图像的基本操作">图像的基本操作</h4><h5 id="读取图像">读取图像</h5><p>cv.imread(var1,var2)<br>var1为读取图像的路径<br>var2：cv.IMREAD*COLOR：以彩色模式加载图像，任何图像的透明度都将被忽略。这是默认参数。var2为1<br>cv.IMREAD*GRAYSCALE：以灰度模式加载图像。var2为0<br>cv.IMREAD_UNCHANGED：包括alpha通道的加载图像模式。var2为-1</p><h5 id="显示图像">显示图像</h5><p>cv.imshow(var1,var2)<br>var1：显示图像的窗口名称，以字符串类型表示<br>var2：要加载的图像的代理对象<br>注意：在调用显示图像的API后，要调用cv.waitKey(var)给图像绘制留下时间，否则窗口会出现无响应情况，并且图像无法显示出来。</p><p>plt.imshow(img,<a href="http://camp=plt.cm" target="_blank" rel="noopener">camp=plt.cm</a>.) cmap即colormaps,图谱 plt.cm是matplotlib库中内置的色彩映射函数 <a href="http://plt.cm" target="_blank" rel="noopener">plt.cm</a>.[色彩](’[数据集]’)即对[数据集]应用[色彩]<br>plt.show()<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8A%E5%8D%8810.36.35.png" alt=""><br>及其他色彩表</p><h5 id="保存图像">保存图像</h5><p>cv.imwrite(var1,var2)<br>var1：图片保存的路径<br>var2：保存图像的代理对象</p><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread('/Users/dinggongcheng/Downloads/me.JPG', 0)#cv.imshow('image', img)#cv.waitKey(0)#cv.destroyAllWindows()#cv.waitKey(1)plt.imshow(img, cmap=plt.cm.gray)plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8A%E5%8D%8810.42.37.png" alt=""></p><h4 id="绘制几何图形">绘制几何图形</h4><h5 id="绘制直线">绘制直线</h5><p>cv.line(img,start,end,color,thickness)<br>img：要绘制直线的图像<br>start，end：直线的起点和终点<br>color：线条的颜色<br>thickness：线条宽度</p><h5 id="绘制圆形">绘制圆形</h5><p>cv.circle(img,centerpoint,r,color,thickness)<br>img：要绘制图形的图像<br>centerpoint，r：圆心和半径<br>color：线条的颜色<br>thickness：线条宽度，为-1时生成闭合图案并填充颜色</p><h5 id="绘制矩形">绘制矩形</h5><p>cv.rectangle(img,leftupper,rightdown,color,thickness)<br>img：要绘制矩形的图像<br>leftupper，rightdown：矩形的左上角和右下角坐标<br>color：线条的颜色<br>thickness：线条宽度</p><h5 id="向图像中添加文字">向图像中添加文字</h5><p>cv.putText(img,text,station,font,fontsize,color,thickness,cv.LINE_AA)<br>img：图像<br>text：要写入的文本数据<br>station：文本的放置位置<br>font：字体<br>fontsize：字体大小</p><pre class=" language-language-python"><code class="language-language-python"># author:dinggc# date:2021/3/29 上午10:54import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 创建一个空白的图像img = np.zeros((512, 512, 3), np.uint8)# 绘制图形cv.line(img, (0, 0), (511, 511), (255, 0, 0), 5)cv.rectangle(img, (384, 0), (510, 128), (0, 255, 0), 3)cv.circle(img, (447, 63), 63, (0, 0, 255), -1)font = cv.FONT_HERSHEY_SIMPLEXcv.putText(img, 'OpenCV', (10, 500), font, 4, (255, 255, 255), 2, cv.LINE_AA)# 图像展示plt.imshow(img[:, :, ::-1])plt.title('匹配结果'), plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8A%E5%8D%8811.02.51.png" alt=""></p><h4 id="获取并修改图像中的像素点">获取并修改图像中的像素点</h4><p>可以通过行和列的坐标值获取该像素点的像素值。对于BGR图像，它返回一个蓝，绿，红值的数组。对于灰度图像，仅返回相应的强度值。使用相同的方法对像素值进行修改。</p><pre class=" language-language-python"><code class="language-language-python">img = np.zeros((256, 256, 3), np.uint8)# 获取某个像素点的值px = img[100, 100]print(px)# 仅获取蓝色通道的强度值blue = img[100, 100, 0]print(blue)# 修改某个位置的像素值img[100, 100] = [255, 255, 255]plt.imshow(img[:, :, ::-1])plt.show()</code></pre><h4 id="获取图像的属性">获取图像的属性</h4><p>图像属性包括行数，列数和通道数，图像数据类型，像素数等。</p><table><thead><tr><th>属性</th><th>API</th></tr></thead><tbody><tr><td>形状</td><td>img.shape</td></tr><tr><td>图像大小</td><td>img.size</td></tr><tr><td>数据类型</td><td>img.dtype</td></tr></tbody></table><h4 id="图像通道的拆分与合并">图像通道的拆分与合并</h4><p>有时需要在B，G，R通道图像上单独工作。在这种情况下，需要将BGR图像分割为单个通道。或者在其他情况下，可能需要将这些单独的通道合并到BGR图像。</p><pre class=" language-language-python"><code class="language-language-python"># 通道拆分b,g,r = cv.split(img)# 通道合并img = cv.merge((b,g,r))</code></pre><h4 id="色彩空间的改变">色彩空间的改变</h4><p>cv.cvtColor(input_image, flag)<br>input_images：进行颜色空间转换的图像<br>flag：转换类型<br>cv.COLOR_BGR2GRAY：BGR-GRAY<br>cv.COLOR_BGR2HSV：BGR-HSV</p><h4 id="算数操作">算数操作</h4><h5 id="图像的加法">图像的加法</h5><p>可以使用OpenCV的cv.add()函数把两幅图像相加，或者可以简单的通过numpy操作添加两个图像，如res = img1 + img2.两个图像应该具有相同的大小和类型，或者第二个图像可以是标量值。</p><p>注意：OpenCV加法和Numpy加法之间存在差异。OpenCv的加法是饱和操作，而Numpy添加是模运算。</p><pre class=" language-language-python"><code class="language-language-python">>>> x = np.uint8([250])>>> y = np.uint8([10])>>> print(cv.add(x,y)) # 250+10 = 260 => 255[[255]]>>> print(x+y)  # 250+10 = 260 % 256 = 4[4]</code></pre><p>这种差别在你对两幅图像进行加法时会更加明显。OpenCV的结果会更好一点。所以尽量使用OpenCV中的函数。</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 读取图像img1 = cv.imread('/Users/dinggongcheng/Downloads/me.JPG')img2 = cv.imread('/Users/dinggongcheng/Downloads/me.JPG')# 加法操作img3 = cv.add(img1, img2)img4 = img1 + img2# 图像显示plt.imshow(img3[:, :, ::-1])plt.imshow(img4[:, :, ::-1])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%881.23.37.png" alt="cv相加"></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%881.24.06.png" alt="直接相加"></p><h5 id="图像的混合">图像的混合</h5><p>通过加法实现，但是不同的是两幅图像的权重不同，这就会给人一种混合或者透明的感觉。图像混合的计算公式如下：<br>g(x) = (1-a)f0(x) + af1(x)<br>通过修改a的值(0-&gt;1)，可以实现非常炫酷的混合。<br>现在把两幅图混合在一起。第一幅图的权重是0.7，第二幅图的权重是0.3。函数cv2.addWeighted()可以按下面的公式对图片进行混合操作。<br>dst = a*img1 + b*img2 + c</p><pre class=" language-language-python"><code class="language-language-python"># 读取图像img1 = cv.imread('/Users/dinggongcheng/Downloads/me.JPG')img2 = cv.imread('/Users/dinggongcheng/Downloads/me.JPG')# 图像混合img3 = cv.addWeighted(img1, 0.8, img2, 0.6, 0)# 图像显示plt.figure(figsize=(8, 8))plt.imshow(img3[:, :, ::-1])plt.show()</code></pre><h4 id="几何变换">几何变换</h4><h5 id="图像缩放">图像缩放</h5><p>缩放是对图像的大小进行调整<br>cv.resize(src,dsize,fx=0,fy=0,interpolation=cv.INTER_LINEAR)<br>src：输入图像<br>dsize：绝对尺寸，直接指定调整后图像的大小<br>fx，fy：相对尺寸，将dsize设置为None，然后将fx和fy设置为比例因子即可<br>interpolation：插值方法</p><table><thead><tr><th>插值</th><th>含义</th></tr></thead><tbody><tr><td>cv.INTER_LINEAR</td><td>双线性插值法</td></tr><tr><td>cv.INTER_NEAREST</td><td>最近邻插值</td></tr><tr><td>cv.INTER_AREA</td><td>像素区域重采样</td></tr><tr><td>cv.INTER_CUBIC</td><td>双三次插值</td></tr></tbody></table><pre class=" language-language-python"><code class="language-language-python">import cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 绝对尺寸缩放rows, cols = img.shape[:2]res = cv.resize(img, (2*cols, 2*rows), interpolation=cv.INTER_CUBIC)# 相对尺寸缩放res1 = cv.resize(img, None, fx=0.5, fy=0.5)# 图像显示fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 8), dpi=100)axes[0].imshow(res[:, :, ::-1])axes[0].set_title("dsize big")axes[1].imshow(img[:, :, ::-1])axes[1].set_title("from")axes[2].imshow(res1[:, :, ::-1])axes[2].set_title("fx small")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%881.53.24.png" alt=""></p><h5 id="图像平移">图像平移</h5><p>图像平移将图像按照指定方向和距离，移动到相应的位置。<br>cv.warpAffine(img,M,dsize)<br>img：输入图像<br>M：2*3移动矩阵，对于(x,y)处的像素点，要把它移动到(x+tx,y+ty)处时，M矩阵应如下设置：<br>M = 1 0 tx<br>0 1 ty<br>注意：将M设置为np.float32类型的Numpy数组<br>dsize：输出图像的大小<br>注意：输出图像的大小，它应该是(宽度，高度)的形式。请记住，width=列数，height=行数。</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 图像平移rows, cols = img.shape[:2]M = np.float32([[1, 0, 100], [0, 1, 50]]) #平移矩阵dst = cv.warpAffine(img, M, (cols, rows))# 图像显示fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 8), dpi=100)axes[0].imshow(dst[:, :, ::-1])axes[0].set_title("move")axes[1].imshow(img[:, :, ::-1])axes[1].set_title("from")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%882.07.55.png" alt=""></p><h5 id="图像旋转">图像旋转</h5><p>图像旋转是指图像按照某个位置转动一定角度的过程，旋转中图像仍保持原始尺寸。图像旋转后图像的水平对称轴，垂直对称轴及中心坐标原点都可能发生变换。因此需要对图像旋转中的坐标进行相应转换。<br>在OpenCV中图像旋转首先根据旋转角度和旋转中心获取旋转矩阵，然后根据旋转矩阵进行变换，即可实现任意角度和任意中心的旋转效果。<br>cv.getRotationMatrix2D(center, angle,scale)<br>center：旋转中心<br>angle：旋转角度<br>scale：缩放比例<br>返回：<br>M：旋转矩阵<br>调用cv.warpAffine完成图像的旋转</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 图像旋转rows, cols = img.shape[:2]# 生成旋转矩阵M = cv.getRotationMatrix2D((cols/2, rows/2), 90, 1)# 进行旋转变换dst = cv.warpAffine(img, M, (cols, rows))# 图像显示fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 8), dpi=100)axes[0].imshow(dst[:, :, ::-1])axes[0].set_title("rotate")axes[1].imshow(img[:, :, ::-1])axes[1].set_title("from")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%882.25.20.png" alt=""></p><h5 id="仿射变换">仿射变换</h5><p>图像的仿射变换涉及到图像的形状位置角度的变化，是深度学习预处理中常用到的功能，仿射变换主要是对图像的缩放，旋转，翻转和平移等操作的组合。<br>需要注意的是，对于图像而言，宽度方向是x，高度方向是y，坐标的顺序和图像像素对应下标一致。所以原点的位置不是左下角而是右上角，y的方向也不是向上，而是向下。<br>在仿射变换中，原图中所有的平行线在结果图像中同样平行。为了创建这个矩阵我们需要从原图像中找到三个点以及他们在输出图像中的位置。然后cv.getAffineTransform会创建一个2*3的矩阵，最后这个矩阵会被传给函数cv.warpAffine</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 仿射变换rows, cols = img.shape[:2]# 创建变换矩阵pts1 = np.float32([[50, 50], [200, 50], [50, 200]])pts2 = np.float32([[100, 100], [200, 50], [100, 250]])M = cv.getAffineTransform(pts1, pts2)# 完成仿射变换dst = cv.warpAffine(img, M, (cols, rows))# 图像显示fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 8), dpi=100)axes[0].imshow(dst[:, :, ::-1])axes[0].set_title("affine")axes[1].imshow(img[:, :, ::-1])axes[1].set_title("from")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%882.38.35.png" alt=""></p><h5 id="透射变换">透射变换</h5><p>透射变换是视角变换的结果，是指利用透视中心，像点，目标点三点共线的条件，按透视旋转定律使承影面绕迹线旋转某一角度，破坏原有的投影光线束，仍能保持承影面上投影几何图形不变的变换。<br>在OpenCV中，我们要找到四个点，其中任意三个不共线，然后获取变换矩阵T，在进行透射变换。通过函数cv.getPerspectiveTransform找到其变换矩阵，将cv.warpPerspective应用于此3*3变换矩阵。</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 透射变换rows, cols = img.shape[:2]# 创建变换矩阵pts1 = np.float32([[56, 65], [368, 52], [28, 387], [389,390]])pts2 = np.float32([[100, 145], [300, 100], [80, 290], [310, 300]])T = cv.getPerspectiveTransform(pts1,pts2)# 进行变换dst = cv.warpPerspective(img, T, (cols, rows))# 图像显示fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 8), dpi=100)axes[0].imshow(dst[:, :, ::-1])axes[0].set_title("perspective")axes[1].imshow(img[:, :, ::-1])axes[1].set_title("from")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%882.53.23.png" alt=""></p><h5 id="图像金字塔">图像金字塔</h5><p>图像金字塔是图像多尺度表达的一种，最主要用于图像的分割，是一种以多分辨率来解释图像的有效但概念简单的结构。<br>图像金字塔用于机器视觉和图像压缩，一幅图像的金字塔是一系列以金字塔形状排列的分辨率逐步降低且来源于同一张原始图的图像集合。其通过梯次向下采样获得，直到达到某个终止条件才停止采样。<br>金字塔的底部是待处理图像的高分辨率表示，而顶部是低分辨率的近似，层级越高，图像越小，分辨率越低。<br>cv.pyrUp(img) 对图像进行上采样<br>cv.pyrDown(img) 对图像进行下采样</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 进行图像采样up_img = cv.pyrUp(img)down_img = cv.pyrDown(img)# 图像显示fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 8), dpi=100)axes[0].imshow(up_img[:, :, ::-1])axes[0].set_title("Up")axes[1].imshow(down_img[:, :, ::-1])axes[1].set_title("Down")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%883.04.56.png" alt=""></p><h4 id="形态学操作">形态学操作</h4><h5 id="连通性">连通性</h5><p>连通性是描述区域和边界的重要概念，两个像素联通的两个必要条件是：<br>1.两个像素的位置是否相邻<br>2.两个像素的灰度值是否满足特定的相似性准则</p><h5 id="腐蚀和膨胀">腐蚀和膨胀</h5><p>腐蚀和膨胀是最基本的形态学操作，腐蚀和膨胀都是针对白色部分(高亮部分)而言的。<br>膨胀就是使图像中高亮部分扩张，效果图拥有比原图更大的高亮区域；腐蚀是原图中的高亮区域被蚕食，效果图拥有比原图更小的高亮区域。膨胀是求局部最大值的操作，腐蚀是求局部最小值的操作。</p><p>腐蚀的具体操作是用一个结构元素扫描图像中的每一个元素，用结构元素中的每一个像素与其覆盖的像素做与操作，如果都为1，则该像素为1，否则为0.<br>腐蚀的作用是消除物体边界点，使目标缩小，可以消除小于结构元素的噪声点。<br>cv.erode(img,kernel,iterations)<br>img：要处理的图像<br>kernel：核结构<br>iterations：腐蚀的次数，默认是1</p><p>膨胀的具体操作是用一个结构元素扫描图像中的每一个像素，用结构元素中的每一个像素与其覆盖的像素做与操作，如果都为0，则该像素为0，否则为1.<br>膨胀的作用是将与物体接触的所有背景点合并到物体中，使目标增大，可添补目标中的孔洞。<br>cv.dilate(img,kernel,iterations)img：要处理的图像<br>kernel：核结构<br>iterations：腐蚀的次数，默认是1</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 读取图像img = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 创建核结构kernel = np.ones((5, 5), np.uint8)# 图像腐蚀和膨胀erosion = cv.erode(img, kernel)dilate = cv.dilate(img, kernel)# 图像显示fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 8), dpi=100)axes[0].imshow(img[:, :, ::-1])axes[0].set_title("from")axes[1].imshow(erosion[:, :, ::-1])axes[1].set_title("erosion")axes[1].imshow(dilate[:, :, ::-1])axes[1].set_title("dilate")plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%883.34.21.png" alt=""></p><h5 id="开闭运算">开闭运算</h5><p>开运算和闭运算是将腐蚀和膨胀按照一定的次序进行处理。但这两者并不是可逆的，即先开后闭并不难得到原来的图像。<br>开运算是先腐蚀后膨胀，其作用是分离物体，消除小区域。消除噪点，去除小的干扰块，而不影响原来的图像。<br>闭运算与开运算相反，是先膨胀后腐蚀，作用是消除/闭合物体里面的孔洞。可以填充闭合区域。</p><p>cv.morphologyEx(img,op,kernel)<br>img：要处理的图像<br>op：处理方式：若进行开运算，则设为cv.MORPH_OPEN，若进行闭运算，则设为cv.MORPH_CLOSE<br>kernel：核结构</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 读取图像img = cv.imread("/Users/dinggongcheng/Downloads/me.JPG")# 创建核结构kernel = np.ones((10, 10), np.uint8)# 图像的开闭运算cvOpen = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)cvClose = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel)# 图像显示fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 8), dpi=100)axes[0].imshow(img[:, :, ::-1])axes[0].set_title("from")axes[1].imshow(cvOpen[:, :, ::-1])axes[1].set_title("cvOpen")axes[1].imshow(cvClose[:, :, ::-1])axes[1].set_title("cvClose")plt.show()</code></pre><h5 id="礼帽和黑帽">礼帽和黑帽</h5><p>礼帽运算<br>原图像与开运算的结果图之差，如下式计算：<br>dst = tophat(src,element) = src - open(src,element)<br>因为开运算带来的结果是放大了裂缝或者局部低亮度的区域，因此，从原图中减去开运算后的图，得到的效果突出了比原图轮廓周围的区域更明亮的区域，且这一操作和选择的核的大小相关。<br>礼帽运算用来分离比邻近点亮一些的斑块。当一幅图像具有大幅的背景的时候，而微小物品比较有规律的情况下，可以使用礼帽运算进行背景提取。</p><p>黑帽运算<br>为闭运算的结果图与原图像之差，数学表达式为<br>dst = blackout(src,element) = close(src,element) - src<br>黑帽运算后的效果图突出了比原图轮廓周围的区域更暗的区域，且这一操作和选择的核的大小相关。黑帽运算用来分离比邻近点暗一些的斑块。</p><p>cv.morphologyEx(img,op,kernel)<br>img：要处理的图像<br>op：处理方式：若进行开运算，则设为cv.MORPH_OPEN，若进行闭运算，则设为cv.MORPH_CLOSE。若进行礼帽运算，则设为cv.MORPH_TOPHAT，若进行黑帽运算，则设为cv.MORPH_BLACKHAT。<br>kernel：核结构</p><h4 id="图像平滑">图像平滑</h4><h5 id="图像噪声">图像噪声</h5><p>椒盐噪声<br>椒盐噪声也称为脉冲噪声，是图像中经常见到的一种噪声，它是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素。</p><p>高斯噪声<br>噪声密度函数服从高斯分布的一类噪声。</p><h5 id="图像平滑-2">图像平滑</h5><h6 id="均值滤波">均值滤波</h6><p>cv.blur(src,ksize,anchor,borderType)<br>src：输入图像<br>ksize：卷积核的大小<br>anchor：默认值(-1,-1)，表示核中心<br>borderType：边界类型</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 图像读取img = cv.imread("/Users/dinggongcheng/Downloads/me.png")# 均值滤波blur = cv.blur(img, (5, 5))# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.subplot(121), plt.imshow(img[:, :, ::-1]), plt.title('from')plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(blur[:, :, ::-1]), plt.title('blur')plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%884.16.32.png" alt=""></p><h6 id="高斯滤波">高斯滤波</h6><p>cv.GaussianBlur(src,ksize,sigmaX,sigmaY,borderType)<br>src：输入图像<br>ksize：高斯卷积核大小，注意：卷积核的宽度和高度都应为奇数，且可以不同<br>sigmaX：水平方向的标准差<br>sigmaY：垂直方向的标准差，默认值为0，表示与sigmaX相同<br>borderType：填充边界类型</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 图像读取img = cv.imread("/Users/dinggongcheng/Downloads/me.png")# 高斯滤波gauss = cv.GaussianBlur(img, (3, 3), 1)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.subplot(121), plt.imshow(img[:, :, ::-1]), plt.title('from')plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(gauss[:, :, ::-1]), plt.title('gauss')plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%884.26.06.png" alt=""></p><h6 id="中值滤波">中值滤波</h6><p>中值滤波是一种典型的非线性滤波技术，基本思想适用像素点领域灰度值的中值来代替该像素点的灰度值。中值滤波对椒盐噪声来说尤其有用，因为它不依赖于领域内那些与典型值差别很大的值。<br>cv.medianBlur(src,ksize)<br>src：输入图像<br>ksize：卷积核的大小</p><pre class=" language-language-python"><code class="language-language-python">import numpy as npimport cv2 as cvimport matplotlib.pyplot as plt# 图像读取img = cv.imread("/Users/dinggongcheng/Downloads/me.png")# 中值滤波media = cv.medianBlur(img, 5)# 图像显示plt.figure(figsize=(10, 8), dpi=100)plt.subplot(121), plt.imshow(img[:, :, ::-1]), plt.title('from')plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(media[:, :, ::-1]), plt.title('media')plt.xticks([]), plt.yticks([])plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%884.30.18.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习笔记(一)</title>
      <link href="/2021/03/27/python-xue-xi-bi-ji/"/>
      <url>/2021/03/27/python-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="python学习笔记-一">python学习笔记(一)</h2><h3 id="python函数">python函数</h3><p>print输出函数<br>id(var) 获取对象的唯一id type(var) 获取对象的类型 var 存储的值</p><p>open函数 var3 = open(var1 var2) var1参数值代表输出文件路径 var2代表文件创建的形式 'a+'代表无则创建，有则追加。var3代表返回对象<br>print(var1 var2) var1代表输出内容 var2代表使用的返回参数，对于输入到文件来说，file=var3进行输入<br><strong>使用open进行输入到文件，需要关闭返回var3的对象(关闭流)</strong></p><p>chr(0b二进制) 输出对应的汉字编码对应的汉字<br>ord(字符串) 输出对应汉字的二进制编码 都需要借助print输出</p><p>var2 = input(var1) var1作为脚本提问，var2作为用户输入</p><p>bool() 获取对象的bool值。默认为False的数据结构默认值，False，0，0.0，None，’’，""，[]，list()，()，tuple()，{}，dict()，set()</p><p>var = range(stop) 默认从0开始，步长为1。给出结束<br>var = range(start,stop) 默认步长为1，给出开始与结束。<br>var = range(start,stop,step) 给出开始，结束与步长。左闭右开。<br>list® 列表输出 使用in或not in判断整数在列表中是否存在，返回bool</p><h3 id="转义字符">转义字符</h3><p>\n 换行 \t 制表符 制表符以首字母开始以四个占位为标准进行<br>\r 回车 \b 回退 撤销前一格<br>不希望转义，字符串前加r或R即可<br>三引号可以换行</p><h3 id="字符编码">字符编码</h3><p>50 2 70 F 100 d</p><h3 id="类型转换">类型转换</h3><p>直接使用数据类型，str(),int(),float()</p><h3 id="运算符">运算符</h3><p>//整除运算，取整数部分，正负整除运算，向下取整数<br>**幂运算，次方运算<br>正负取余运算时，公式为余数=被除数-除数✖️商<br>解包赋值，a,b,c = 20,30,40<br>is进行对象判断<br>python中直接使用英文and,or或not<br>字符串判断存在可以直接使用in或not in</p><h3 id="分支结构">分支结构</h3><p>if语法:<br>if 条件表达式:<br>业务逻辑<br>else:<br>业务逻辑</p><p>if 条件表达式:<br>if 条件表达式:<br>业务逻辑<br>else:<br>业务逻辑<br>else:<br>业务逻辑</p><p>(var1) if var1&gt;=var2 else (var2)  类似三元表达式，True前，False后</p><p>pass占位符，代替模糊的业务逻辑</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring boot接口防刷</title>
      <link href="/2021/02/17/springboot-jie-kou-fang-shua/"/>
      <url>/2021/02/17/springboot-jie-kou-fang-shua/</url>
      
        <content type="html"><![CDATA[<h3 id="Springboot接口防刷">Springboot接口防刷</h3><p>自定义注解</p><pre class=" language-language-java"><code class="language-language-java">import java.lang.annotation.Retention;import java.lang.annotation.Target; import static java.lang.annotation.ElementType.METHOD;import static java.lang.annotation.RetentionPolicy.RUNTIME; @Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface AccessLimit {     int seconds();    int maxCount();    boolean needLogin() default true;}</code></pre><p>拦截器</p><p>用户的 ID和请求的路径作为key,首先从redis中根据该key去取得用户的访问次数，如果为null,则根据该key重新设置key,value,value的值是１，能取到key对应的value，则判断vlaue是否小于５，是则处理请求，否则返回error。</p><pre class=" language-language-java"><code class="language-language-java">import com.alibaba.fastjson.JSON;import com.example.demo.action.AccessLimit;import com.example.demo.redis.RedisService;import com.example.demo.result.CodeMsg;import com.example.demo.result.Result;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.method.HandlerMethod;import org.springframework.web.servlet.handler.HandlerInterceptorAdapter; import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.OutputStream; @Componentpublic class AntiBrushInterceptor extends HandlerInterceptorAdapter {     @Autowired    private RedisService redisService;     @Override    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {         if(handler instanceof HandlerMethod){             HandlerMethod hm = (HandlerMethod) handler;             AccessLimit accessLimit = hm.getMethodAnnotation(AccessLimit.class);            if(accessLimit == null){                return true;            }            int seconds = accessLimit.seconds();            int maxCount = accessLimit.maxCount();            boolean login = accessLimit.needLogin();            String key = request.getRequestURI();            if(login){                //业务逻辑            }             AccessKey ak = AccessKey.withExpire(seconds);            Integer count = redisService.get(ak,key,Integer.class);            if(count == null){                redisService.set(ak,key,1);            }else if(count < maxCount){                redisService.incr(ak,key);            }else{                render(response,CodeMsg.ACCESS_LIMIT_REACHED);                 return false;            }        }         return true;     }    private void render(HttpServletResponse response, CodeMsg cm)throws Exception {        response.setContentType("application/json;charset=UTF-8");        OutputStream out = response.getOutputStream();        String str  = JSON.toJSONString(Result.error(cm));        out.write(str.getBytes("UTF-8"));        out.flush();        out.close();    }}</code></pre><p>拦截器注册和注解使用</p><pre class=" language-language-java"><code class="language-language-java">import com.example.demo.ExceptionHander.AntiBrushInterceptor;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.InterceptorRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;@Configurationpublic class WebConfig extends WebMvcConfigurerAdapter {     @Autowired    private AntiBrushInterceptor interceptor;      @Override    public void addInterceptors(InterceptorRegistry registry) {        registry.addInterceptor(interceptor);    }}</code></pre><pre class=" language-language-java"><code class="language-language-java">@PostMapping("getNewsModule")@AccessLimit(seconds=5, maxCount=10, needLogin=true)public Result<List<NewsModuleDTO>> getNewsModule(@ApiIgnore @RequestParam Map<String, Object> params){        PageData<NewsModuleDTO> page = newsModuleService.page(params);        return new Result<List<NewsModuleDTO>>().ok(page.getList());    }</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 日常随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring Boot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用docker创建项目镜像</title>
      <link href="/2021/02/05/shi-yong-docker-chuang-jian-xiang-mu-jing-xiang/"/>
      <url>/2021/02/05/shi-yong-docker-chuang-jian-xiang-mu-jing-xiang/</url>
      
        <content type="html"><![CDATA[<h3 id="使用docker创建项目镜像">使用docker创建项目镜像</h3><p>本文创建的镜像为hexo静态博客的镜像，防止自己的服务器到期之后进行迁移的麻烦。</p><h4 id="确定创建镜像所需的环境">确定创建镜像所需的环境</h4><ul><li>node.js</li><li>git</li><li>nginx</li></ul><h4 id="编写自己的镜像的Dockerfile文件">编写自己的镜像的Dockerfile文件</h4><p>docker指令详解</p><table><thead><tr><th>指令名称</th><th>用法</th><th>作用</th></tr></thead><tbody><tr><td>FROM</td><td>FROM image</td><td>FROM指定构建镜像的基础源镜像，如果本地没有指定的镜像，则会自动从 Docker 的公共库 pull 镜像下来；FROM必须是 Dockerfile 中非注释行的第一个指令，即一个 Dockerfile 从FROM语句开始；FROM可以在一个 Dockerfile 中出现多次，如果有需求在一个 Dockerfile 中创建多个镜像；如果FROM语句没有指定镜像标签，则默认使用latest标签。</td></tr><tr><td>MAINTAINER</td><td>MAINTAINER name</td><td>指定创建镜像的用户</td></tr><tr><td>RUN</td><td>RUN “executable”, “param1”, “param2”</td><td>每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像，后续的RUN都在之前RUN提交后的镜像为基础，镜像是分层的，可以通过一个镜像的任何一个历史提交点来创建，类似源码的版本控制；</td></tr><tr><td></td><td>RUN [ “echo”, “$HOME” ]</td><td></td></tr><tr><td></td><td>RUN [ “sh”, “-c”, “echo”, “$HOME” ]</td><td>RUN产生的缓存在下一次构建的时候是不会失效的，会被重用，可以使用–no-cache选项，即docker build –no-cache，如此便不会缓存。</td></tr><tr><td>CMD</td><td>CMD “executable”,“param1”,“param2”</td><td>CMD指定在 Dockerfile 中只能使用一次，如果有多个，则只有最后一个会生效。</td></tr><tr><td></td><td>CMD “param1”,“param2”</td><td>CMD的目的是为了在启动容器时提供一个默认的命令执行选项。如果用户启动容器时指定了运行的命令，则会覆盖掉CMD指定的命令。</td></tr><tr><td></td><td>CMD command param1 param2 (shell form)</td><td>CMD会在启动容器的时候执行，build 时不执行，而RUN只是在构建镜像的时候执行，后续镜像构建完成之后，启动容器就与RUN无关了</td></tr><tr><td>EXPOSE</td><td>EXPOSE port [port…]</td><td>告诉 Docker 服务端容器对外映射的本地端口，需要在 docker run 的时候使用-p或者-P选项生效。</td></tr><tr><td>ENV</td><td>ENV key value       <em># 只能设置一个变量</em></td><td></td></tr><tr><td></td><td>ENV key=value …   <em># 允许一次设置多个变量</em></td><td>指定一个环境变量，会被后续RUN指令使用，并在容器运行时保留。</td></tr><tr><td>ADD</td><td>ADD src… dest</td><td>ADD复制本地主机文件、目录或者远程文件 URLS 从 并且添加到容器指定路径中 。</td></tr><tr><td>COPY</td><td>COPY src… dest</td><td>COPY复制新文件或者目录从 并且添加到容器指定路径中 。用法同ADD，唯一的不同是不能指定远程文件 URLS。</td></tr><tr><td>ENTRYPOINT</td><td>ENTRYPOINT “executable”, “param1”, “param2”</td><td>每个 Dockerfile 中只能有一个ENTRYPOINT，当指定多个时，只有最后一个生效。</td></tr><tr><td></td><td>ENTRYPOINT command param1 param2 (shell form)</td><td>配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖，而CMD是可以被覆盖的。如果需要覆盖，则可以使用<code>docker run --entrypoint</code>选项。</td></tr><tr><td>VOLUME</td><td>VOLUME ["/data"]</td><td>将本地主机目录挂载到目标容器中，或者将其他容器挂载的挂载点 挂载到目标容器中</td></tr><tr><td>USER</td><td>USER daemon</td><td>指定运行容器时的用户名或 UID，后续的RUN、CMD、ENTRYPOINT也会使用指定用户。</td></tr><tr><td>WORKDIR</td><td>WORKDIR /path/to/workdir</td><td>为后续的RUN、CMD、ENTRYPOINT指令配置工作目录。可以使用多个WORKDIR指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。</td></tr><tr><td>ONBUILD</td><td>ONBUILD [INSTRUCTION]</td><td>使用该dockerfile生成的镜像A，并不执行ONBUILD中命令，如再来个dockerfile 基础镜像为镜像A时，生成的镜像B时就会执行ONBUILD中的命令</td></tr></tbody></table><p><strong>拉取ubuntu环境镜像</strong></p><pre class=" language-language-dockerfile"><code class="language-language-dockerfile">docker pull ubuntu:18.04</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%8B%89%E5%8F%96ubuntu.png" alt=""></p><p>如果拉取镜像慢的话，可以配置一下镜像加速。</p><p>使用<code>docker version</code>查看自己的docker版本，大于1.10可以进行加速配置，小于的更新一下即可。</p><pre><code>mkdir -p /etc/dockertee /etc/docker/daemon.json &lt;&lt;-'EOF'{  "registry-mirrors": ["https://xxxxx.mirror.aliyuncs.com"]  #这里也可以用网易的镜像加速器："http://hub-mirror.c.163.com"}EOFsystemctl daemon-reloadsystemctl restart docker</code></pre><p><strong>配置nginx环境</strong></p><p>安装步骤在后面的Dockerfile中，安装完nginx后，这里以ubuntu18.04为例，nginx的目录为/ect/nginx。我们修改配置文件只需要修改sites-available下的default即可。我这里在之前学习的时候都是不一样的路径，所以我这里以ubuntu为例。准备好一份default配置文件COPY即可。</p><pre><code>server {        listen 80 default_server;        listen [::]:80 default_server;        listen 443 ssl;        ssl off;        ssl_certificate cert/4287626_www.dingdm.club.pem;        ssl_certificate_key cert/4287626_www.dingdm.club.key;        ssl_session_timeout 5m;        #ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aN$        ssl_ciphers ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS;        ssl_protocols TLSv1 TLSv1.1 TLSv1.2;        ssl_prefer_server_ciphers on;        root /home/www/hexo;        server_name dingdm.club;}</code></pre><p>server_name为你的域名，root为你的静态网站所在的路径，可以自己创建，<strong>注意这里的路径不是你的hexo博客的路径，如果你在服务器上创建博客的话</strong>。</p><p>ssl的配置为开启https的配置，这里先不赘述，不需要的话可以将ssl相关的配置注释掉即可</p><p><strong>在你使用docker镜像服务器上配一下自己的git环境和仓库。(非docker容器)</strong></p><p>更新频繁因此不将git整合到容器内。</p><pre><code>apt-get update apt-get install gitgit config --global user.name "你的用户名"git config --global user.email "你的注册邮箱"</code></pre><p>添加git用户，使用root账户也可以。</p><pre><code>adduser git</code></pre><p>修改文件内容赋予权限</p><pre><code>chmod 740 /etc/sudoersnano /etc/sudoers</code></pre><p>找到root用户那，在下方添加一条</p><pre><code>git ALL=(ALL) ALL</code></pre><p>将文件权限恢复</p><pre><code>chmod 400 /etc/sudoers</code></pre><p>切换到git用户，并创建ssh文件夹</p><pre><code>su gitcd ~mkdir .sshcd .ssh</code></pre><p>生成密钥公钥并配置</p><pre><code>ssh-keygencp id_rsa.pub authorized_keyschmod 600 ~/.ssh/authorized_keyschmod 700 ~/.ssh</code></pre><p>配置完成之后可以使用<code>ssh -v git@ip</code>连接测试</p><p>新建仓库并设置钩子脚本</p><pre><code>cd ~git init --bare blog.gitnano ~/blog.git/hooks/post-receive</code></pre><p>输入以下内容</p><pre><code>git --work-tree=/home/www/hexo --git-dir=/home/git/blog.git checkout -f</code></pre><p>/home/www/hexo为你刚刚在nginx的配置文件里的root路径，/home/git/blog.git为你的仓库地址</p><p><strong>制作Dockerfile文件</strong></p><p>使用自己刚刚拉取的ubuntu镜像并安装nginx和配置环境</p><pre class=" language-language-dockerfile"><code class="language-language-dockerfile">FROM c090eaba6b94RUN apt-get update && apt-get install -y nginxCOPY default /etc/nginx/sites-available/COPY www/ /home/wwwCMD ["/usr/sbin/nginx","-g","daemon off;"]</code></pre><p><strong>制作镜像</strong></p><pre><code>docker build -t dingdm:1.0.0 .</code></pre><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E8%BF%90%E8%A1%8C%E9%95%9C%E5%83%8F.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E8%BF%90%E8%A1%8C%E6%88%90%E5%8A%9F.png" alt=""></p><p><strong>镜像为nginx加静态博客页面。服务器到期后在考虑本地部署，使用ADD加载git仓库里新部署的文件信息并发布新版镜像。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dockerfile </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KMP字符串匹配</title>
      <link href="/2020/09/07/kmp-zi-fu-chuan-pi-pei/"/>
      <url>/2020/09/07/kmp-zi-fu-chuan-pi-pei/</url>
      
        <content type="html"><![CDATA[<h3 id="KMP字符串匹配">KMP字符串匹配</h3><pre class=" language-language-java"><code class="language-language-java">package com.zjr.wholesale.config;import java.util.ArrayList;import java.util.List;public class KMP {    /*public static String [] str = new String[]{"b","a","c","b","a","d",            "a","b","a","d","a","b","a","b","a","c","a","m","b","a","b","a","c","a","d","d","a","b","a","b","a","c","a","s","d"};    public static String [] ptr = new String[]{"a","b","a","b","a","c","a"};*/    public static String [] str = new String[]{"a","b","c","a","a","b",            "a","b","a","b","a","a"};    public static String [] ptr = new String[]{"a","b","a","b"};    public static void main(String []args) {        List<Integer> array = getPtrArray(ptr);        ifEquals(0,0,array);    }    //递归判断字符串比较    public static void ifEquals(int i,int j,List<Integer> array){        if(i < str.length){            if(j < ptr.length){                if(str[i].equals(ptr[j])){                    i++;                    j++;                    ifEquals(i,j,array);                }else{                    if(j == 0){                        i++;                    }else{                        int length = getArrayValue(j,array);                        if(length != 0){                            i = i - length;                        }                        j = 0;                    }                    ifEquals(i,j,array);                }            }else{                System.out.println("Ptr 匹配成功，下标开始位置为:" + (i - ptr.length));                j = 0;                ifEquals(i,j,array);            }        }else{            System.out.println("字符串匹配结束");        }    }    //获取前缀后缀数组中的长度    public static int getArrayValue(int j,List<Integer> array){        return array.get(j-1);    }    //获取比较字符串的前缀后缀数组    public static List<Integer> getPtrArray(String []str){        int j = 1;        List<Integer> array = new ArrayList<Integer>();        while(j <= str.length){            List<String> nStr = new ArrayList<String>();            for(int i=0;i<j;i++){                nStr.add(str[i]);            }            int number = getNStrNumber(nStr);            array.add(number);            j++;        }        return array;    }    //获取当前列表字符串的前缀后缀个数    public static int getNStrNumber(List<String> nStr){        int length = 0;        if(nStr.size() == 1){            length = 0;        }else{            int i = nStr.size();            boolean flag = true;            while(i > 0 && flag){                i--;                StringBuilder str = new StringBuilder();                StringBuilder ptr = new StringBuilder();                for(int index = 0;index < i;index++){                    str.append(nStr.get(index));                }                for(int lIndex=(nStr.size() - i);lIndex < nStr.size();lIndex++){                    ptr.append(nStr.get(lIndex));                }                if(str.toString().equals(ptr.toString())){                    flag = false;                    length = str.length();                }            }        }        return length;    }}</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>steam及其他免费游戏白嫖(更新)</title>
      <link href="/2020/08/08/steam-ji-qi-ta-mian-fei-you-xi-bai-piao-geng-xin/"/>
      <url>/2020/08/08/steam-ji-qi-ta-mian-fei-you-xi-bai-piao-geng-xin/</url>
      
        <content type="html"><![CDATA[<h3 id="steam及其他免费游戏白嫖-更新">steam及其他免费游戏白嫖(更新)</h3><h4 id="2020-11-05">2020-11-05</h4><p>无主之地3 61元<br><a href="https://www.greenmangaming.com/zh/" target="_blank" rel="noopener">https://www.greenmangaming.com/zh/</a></p><h4 id="2020-09-02">2020-09-02</h4><p>远哭3领取<br><a href="https://register.ubisoft.com/far-cry-3/zh-CN" target="_blank" rel="noopener">https://register.ubisoft.com/far-cry-3/zh-CN</a><br>全境封锁领取<br><a href="https://register.ubisoft.com/the-division/en-US" target="_blank" rel="noopener">https://register.ubisoft.com/the-division/en-US</a><br>全境封锁需挂下梯子，推荐efan</p><h4 id="2020-08-16">2020-08-16</h4><p>NBK 2K20<br><a href="https://www.greenmangaming.com/zh/games/nba-2k20-pc/" target="_blank" rel="noopener">https://www.greenmangaming.com/zh/games/nba-2k20-pc/</a><br>鬼泣5和生化危机2 3优惠<br><a href="https://chinaplay.store/china-keys/" target="_blank" rel="noopener">https://chinaplay.store/china-keys/</a></p><h4 id="2020-08-13">2020-08-13</h4><p>全面战争特洛伊<br>本体领取地址：<a href="https://www.epicgames.com/store/zh-CN/product/a-total-war-saga-troy" target="_blank" rel="noopener">https://www.epicgames.com/store/zh-CN/product/a-total-war-saga-troy</a><br>本体领取时间：8月13日21:00-8月14日21:00<br>DLC领取地址<br>注册地址：<a href="https://access.totalwar.com/auth/email_register/" target="_blank" rel="noopener">https://access.totalwar.com/auth/email_register/</a><br>（如果注册地址打不开去这里https://access.totalwar.com/点击注册）<br>关联账号：<a href="https://access.totalwar.com/auth/control_panel/" target="_blank" rel="noopener">https://access.totalwar.com/auth/control_panel/</a></p><h4 id="2020-08-08">2020-08-08</h4><h5 id="steam">steam</h5><p><a href="https://www.humblebundle.com/store/f1-2018-free-game?hmb_campaign=freegame_2020_homepage_f12018&amp;hmb_medium=banner" target="_blank" rel="noopener">F1 2018</a></p><p>**《F1 2018》**是Codemasters开发制作的一款以世界一级方程式赛车为背景制作的一款赛车竞速类游戏。</p><p><a href="https://store.steampowered.com/app/397540/3/" target="_blank" rel="noopener">无主之地3</a></p><p>无主之地3免费周末试玩</p><h5 id="EPIC">EPIC</h5><p><a href="https://www.epicgames.com/store/zh-CN/free-games" target="_blank" rel="noopener">免费游戏</a></p><p>遗迹灰烬重生还不错</p><h5 id="GMG">GMG</h5><p><a href="https://www.epicgames.com/store/zh-CN/free-games" target="_blank" rel="noopener">GMG夏促</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 游戏 </category>
          
          <category> 白嫖 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> steam </tag>
            
            <tag> epic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>win10系统的桌面美化</title>
      <link href="/2020/08/05/win10-xi-tong-de-zhuo-mian-mei-hua/"/>
      <url>/2020/08/05/win10-xi-tong-de-zhuo-mian-mei-hua/</url>
      
        <content type="html"><![CDATA[<h3 id="win10系统的桌面美化">win10系统的桌面美化</h3><p>首先放个人桌面和开始菜单的预览图。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805114944.png" alt="1596599399928"></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805115313.png" alt=""></p><p>桌面美化的部分网络实时数据，应用图标美化，任务栏透明图标居中，开始菜单以及鼠标美化。</p><h4 id="美化软件以及相关文件">美化软件以及相关文件</h4><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/TrafficMonitor.rar" target="_blank" rel="noopener">实时数据软件</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/StartIsBack%20%20%202.9.exe" target="_blank" rel="noopener">任务栏软件</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/RocketDock-v1.3.5.exe" target="_blank" rel="noopener">应用设置软件</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E7%BE%8E%E5%8C%96.rar" target="_blank" rel="noopener">图标及开始程序按钮美化包</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/Ori%E9%BC%A0%E6%A0%87%E5%85%89%E6%A0%872.0.rar" target="_blank" rel="noopener">奥日鼠标资源</a></p><h4 id="实时数据美化">实时数据美化</h4><p>下载提供的实时数据软件后进行解压。打开TrafficMonitor.exe软件。这时就会出现相关内容。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805120508.png" alt=""></p><h5 id="选项设置">选项设置</h5><p>在实时数据上<strong>右键</strong></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805120752.png" alt=""></p><p>将显示通知区域图标关掉，打开显示CPU和内存利用率的选项。</p><h5 id="样式更改">样式更改</h5><p>刚开始的位置并不在任务栏上，因此需要对配置进行修改。这里在实时数据上<strong>右键打开选项</strong>，按照以下配置进行修改即可。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805145216.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805145222.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805145229.png" alt=""></p><h4 id="任务栏设置">任务栏设置</h4><p>安装StartIsBack   2.9.exe</p><h5 id="属性设置">属性设置</h5><p>安装完成后右键开始菜单选择<strong>属性</strong>项</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150108.png" alt=""></p><h5 id="图标美化">图标美化</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150201.png" alt=""></p><p>下载上面提供的美化图标，点击<strong>加号按钮</strong>添加即可。我这里已经添加过了，所有已经有了新的开始菜单的美化图标。</p><h5 id="任务栏透明">任务栏透明</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150321.png" alt=""></p><p>点击<strong>自定义任务栏特效</strong>，选择任务栏图标居中和动态透明度即可。</p><h4 id="应用图标设置">应用图标设置</h4><p>下载上面提供的应用软件RocketDock-v1.3.5进行安装，安装完成后会出现类似mac的效果，自带默认的图标。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150715.png" alt=""></p><p>因为我这里将我的电脑上的应用添加进来了，所以是完成后的效果。安装完成后将自带的图标移除，然后将自己的软件图标拉入到对应的位置即可。</p><h5 id="配置设置">配置设置</h5><p>在rockeddock上右键程序设置进行配置修改</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150823.png" alt=""></p><p>点击程序设置后按照我提供的配置进行修改即可。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150837.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150842.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805150848.png" alt=""></p><p>修改完成之后就是我现在的效果。</p><h5 id="图标设置">图标设置</h5><p>如果你想要更改图标的样式的话。在图标上右键点击图标设置。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805151023.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805151030.png" alt=""></p><p>下载上面提供的美化图标进行替换即可。</p><h5 id="图标分隔">图标分隔</h5><p>如果图标之间想要留出空隙的话，选择**添加项目中的分隔符****即可。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805151143.png" alt=""></p><p>至此应用图标的美化完成。</p><h4 id="鼠标设置">鼠标设置</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805151400.png" alt=""></p><p>我这里使用的是奥日的图标，如果你想使用的话下载上面的资源即可。</p><h5 id="网站下载">网站下载</h5><p>如果不想使用的话，鉴于国外网站需要翻墙，我这里提供鼠标样式下载的国内网站，自己选择下载即可。</p><p><a href="https://zhutix.com/tag/cursors/" target="_blank" rel="noopener">https://zhutix.com/tag/cursors/</a></p><h5 id="鼠标替换">鼠标替换</h5><p>打开电脑的设置界面，选择设备</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805152038.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805152053.png" alt=""></p><p>切换到鼠标，点击右面的其他鼠标选项</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805152109.png" alt=""></p><p>选择指针模块，浏览自己下载的鼠标资源进行替换即可。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805152118.png" alt=""></p><h3 id="开始菜单切片设置">开始菜单切片设置</h3><p>在win10系统自带的商店下载tile genie软件，下载成功后选择图片切割即可。</p><p>图片切片并不难，主要是需要耗费时间拼。</p><h3 id="win10壁纸设置">win10壁纸设置</h3><h4 id="wallpaper-engine">wallpaper engine</h4><p>这里有条件的小伙伴推荐去steam购买一个壁纸软件wallpaper engine，18块钱你买不了吃亏，你买不了上当。最重要的除了它是一款壁纸软件之外，它还是一块非常牛逼的射击游戏。</p><p>我这里推荐几个壁纸。或者你也可以去哔哩哔哩自行搜索相关的视频。</p><p>The Last of Us</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/eafa3cfba8c7a0f380fb792d3e310ffb.jpg" alt=""></p><p>Time lapse + 3D Digital Clock</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/d4aab6a9232e65e55778219d64bbd9b4.jpg" alt=""></p><p>Lamp</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/97bb304efc12b1e4525274e89bb980bf.jpg" alt=""></p><p>Alone</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fee7c1563c02682d02d17ec00ab878e0.jpg" alt=""></p><p>PCB City</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/668df914429c2e529556b6e350b9e6b0.jpg" alt=""></p><h4 id="免费壁纸">免费壁纸</h4><p>如果不想购买的话我这里也推荐几个静态的免费的壁纸。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-02e8a470854063843938274a14cbe238_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-81b1f2f30d62487bb6f47ab08155cc69_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-8262525f9d0681e6b996bc758229f90c_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-c2c137fa7017fc27b454651dafb36b0f_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-dabcb2e02b94f7c8dfefae1ac7e4f591_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-db782cbaced5180b9037064ff9905afe_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-e35ed2b90501aea545ee73e2decebf6a_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-e3fd094f1f8555c4abcbf98ab9119d70_r.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/v2-ff96a816315c9e7c28e48899c9f8df7e_r.jpg" alt=""></p><h4 id="壁纸网站">壁纸网站</h4><p><a href="https://hddesktopwallpapers.in/" target="_blank" rel="noopener">https://hddesktopwallpapers.in/</a></p><p><a href="http://simpledesktops.com/" target="_blank" rel="noopener">http://simpledesktops.com/</a></p><p><a href="http://www.obzhi.com/" target="_blank" rel="noopener">http://www.obzhi.com/</a></p><p><a href="https://ss.netnr.com/wallpaper" target="_blank" rel="noopener">https://ss.netnr.com/wallpaper</a></p><p><a href="http://spotlight.shijuewuyu.com/" target="_blank" rel="noopener">http://spotlight.shijuewuyu.com/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 美化 </category>
          
          <category> 壁纸 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> win10 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>答辩知识架构及企业OA项目开发汇总</title>
      <link href="/2020/08/02/da-bian-zhi-shi-jia-gou-ji-qi-ye-oa-xiang-mu-kai-fa-hui-zong/"/>
      <url>/2020/08/02/da-bian-zhi-shi-jia-gou-ji-qi-ye-oa-xiang-mu-kai-fa-hui-zong/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Vue框架">1.Vue框架</h3><p><strong>MVVM架构，model，view，viewmodel，model和view之间无法直接交互，通过viewmodel进行交互，创建新的模板，在模板中使用js函数创建监听器，使用vm_patch渲染页面完成数据模板的绑定，当数据更新时自动同步数据。view和model自动同步，开发人员只需要关注业务逻辑，而非操作dom对象。<br>前台使用VUE进行开发，Vue.js是一个构建数据驱动的web界面的渐进式框架。Vue的目标是通过尽可能简单API实现响应的数据绑定和组合的视图组件。核心是一个响应的数据绑定系统。即vue是单页面应用，页面局部刷新，不用每次跳转页面都去请求所有数据和dom对象，这样大大加快了访问速度和提升用户体验。而且他的第三方ui库可以节省很多不必要的开发时间。<br>选用Vue的原因中很重要的一点即为图中的MVVM架构，在MVVM中视图和模型是无法直接进行交互的，而他们中间存在着一个中转站，就是ViewModel。当用户对视图进行相关操作时，和视图相连的ViewModel中的一部分就会通知模型层内容的变化，同理，当模型变化时，ViewModel也可以通知试图层进行改变，这就是Vue中的双向数据绑定。类似于通知者通过关键字来通知订阅者的模式。<br>1.双向数据绑定实现浏览器页面的自动刷新。<br>2.轻量级js库，API简洁易用。<br>3.element-ui库</strong></p><h3 id="2-springboot框架">2.springboot框架</h3><p><strong>后台使用spring boot框架，spring boot 并不是用来替代 Spring 的解决方案，而是和 Spring 框架紧密结合用于提升 Spring 开发者体验的工具。同时它集成了大量常用的第三方库配置，Spring Boot应用中这些第三方库几乎可以是零配置的开箱即用（out-of-the-box），大部分的 Spring Boot 应用都只需要非常少量的配置代码（基于 Java 的配置），开发者能够更加专注于业务逻辑。<br>AOP为spring框架中切面编程的实现，AOP可以降低代码之间的耦合度，方便dang当前系统的扩展和代码复用，在OA系统中，我使用AOP进行日志的相关管理和操作。AOP的自定义标签从AopNamespaceHandler这个类开始解析，进入到AspectJAutoProxyBeanDefinitionParser解析器后，调用BeanDefinitionParser的parser方法将请求转交给registerAspectJAnnotationAutoProxyCreatorIfNecessary去实现。在注册或升级即registerAspectJAnnotationAutoProxyCreatorIfNecessary方法处理完成以后根据proxy-target-class的属性来确定使用什么方式的代理，最后调用registerComponentIfNecessary通知监听器。我在使用中正是运用了此流程，使用自定义的AOP注解，将注解注释在需要记录操作的接口方法上，通过通知类型来获取接口数据，进而和数据库交互存储对应的日志信息。<br>springboot是对spring框架的封装。<br>1.简化配置，无需配置xml<br>2.内置tomcat，可是使用maven打包成jar包之后直接在服务器上运行。<br>3.完美兼容当前成熟的开发组件。</strong></p><h3 id="3-前后端分离">3.前后端分离</h3><p><strong>前台通过ajax向后台restful风格的接口发起请求，获取json格式的数据进行解析展示。<br>1.前后端业务逻辑解耦。<br>2.分担后端服务器承受压力。<br>3.分开部署，解耦操作，更新方便。</strong></p><h3 id="4-JWT">4.JWT</h3><p><strong>由于后台管理系统采用了前后端分离开发的方式，那么就必然存在前端与后端的接口交互，于是就涉及到了安全问题，后端该如何确认收到的请求是合法的客户端发出的。普通的session中存储用户信息的方法在前后端交互中不可避免的会存在资源开销较大的问题，因此，在智慧谷OA系统的开发中，我使用了JWT来进行认证。JWT认证在用户登录时生成token，并且在下次接口请求时携带相同的token，不仅解决了资源开销的问题，而且JWT的信息使用HS256和RS256算法签名，保证了其在传输过程中的安全性。<br>JWT一共由三段内容组成，JWT在认证过程中，使用对称算法时，服务器端就获取到接收的JWT的前两段信息作为数据，用相同的算法和密钥进行签名，得到签名后将服务器端的签名和JWT中的第三段内容进行比较，如果相同认证即为成功。使用非对称加密算法时，由一对密钥构成，私钥由服务器端存储，公钥客户端存储，客户端进行验证的时候，用公钥解密收到的签名,得到JWT三段内容中的前两段，再用这两段内容和JWT进行比较，相同则验证成功。<br>由于JWT不能保证数据在传输中不被窃取，因此除了使用非对称加密算法之外，还需要在JWT的内容中设置合理的过期时间来确保数据的安全性。<br>session方式将用户信息存储到session中，session存储到内存中，当用户数据增多时，增大服务器的开销。<br>token方式将用户数据发送到服务器，服务器使用SHA256或其他算法计算token，客户端接收存储，在请求时将token存储于请求头中。</strong></p><h3 id="5-Activiti">5.Activiti</h3><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/9f2d207d579d4c7ea7114ab042429f0d.png" alt=""><br><strong>Activiti由图中模块构成，Activiti Engine为Activiti的核心，负责数据的处理，Activiti Modeler为模型设计器，Activities Rest支持Restful风格的服务，允许以JSON的格式进行交互，支持跨语言和跨平台。<br>Activiti组件使用Mybatis和数据库进行交互，完全契合当前的智慧谷OA系统，且Activiti具有简单易懂的接口和全面的流程设计器，适合任何人的使用与开发。而Activiti原生支持spring这一特性大大减少了在开发过程中的难度和工作量。<br>引入官方依赖，使用Activiti的可视化流程编辑器整合。<br>编写对应的流程方法和监听器。</strong></p><h3 id="前端代码">前端代码</h3><pre class=" language-language-html"><code class="language-language-html">**配置**<!-- 开发环境 -->  <% if (process.env.VUE_APP_NODE_ENV === 'dev') { %>    <script>window.SITE_CONFIG['apiURL'] = 'http://127.0.0.1:8081/wisdom';</script>  <% } %>  <!-- 集成测试环境 -->  <% if (process.env.VUE_APP_NODE_ENV === 'prod:sit') { %>    <script>window.SITE_CONFIG['apiURL'] = 'http://47.100.126.191:8081/wisdom';</script>  <% } %>  <!-- 验收测试环境 -->  <% if (process.env.VUE_APP_NODE_ENV === 'prod:uat') { %>    <script>window.SITE_CONFIG['apiURL'] = 'http://47.100.126.191:8081/wisdom';</script>  <% } %>  <!-- 生产环境 -->  <% if (process.env.VUE_APP_NODE_ENV === 'prod') { %>    <script>window.SITE_CONFIG['apiURL'] = 'http://47.100.126.191:8081/wisdom';</script>  <% } %></code></pre><p><strong>数据接口</strong></p><pre class=" language-language-javascript"><code class="language-language-javascript">export default {  data () {    /* eslint-disable */    return {      // 设置属性      mixinViewModuleOptions: {        activatedIsNeed: true,    // 此页面是否在激活（进入）时，调用查询数据列表接口？        getDataListURL: '',       // 数据列表接口，API地址        getDataListIsPage: false, // 数据列表接口，是否需要分页？        deleteURL: '',            // 删除接口，API地址        deleteIsBatch: false,     // 删除接口，是否需要批量？        deleteIsBatchKey: 'id',   // 删除接口，批量状态下由那个key进行标记操作？比如：pid，uid...        exportURL: ''             // 导出接口，API地址      },      // 默认属性      dataForm: {},               // 查询条件      dataList: [],               // 数据列表      order: 'desc',                  // 排序，asc／desc      orderField: 'create_date',             // 排序，字段      page: 1,                    // 当前页码      limit: 10,                  // 每页数      total: 0,                   // 总条数      dataListLoading: false,     // 数据列表，loading状态      dataListSelections: [],     // 数据列表，多选项      addOrUpdateVisible: false   // 新增／更新，弹窗visible状态    }    /* eslint-enable */  },  activated () {    if (this.mixinViewModuleOptions.activatedIsNeed) {      this.getDataList()    }  },  methods: {    // 获取数据列表    getDataList () {      this.dataListLoading = true      this.$http.get(        this.mixinViewModuleOptions.getDataListURL,        {          params: {            order: this.order,            orderField: this.orderField,            page: this.mixinViewModuleOptions.getDataListIsPage ? this.page : null,            limit: this.mixinViewModuleOptions.getDataListIsPage ? this.limit : null,            ...this.dataForm          }        }      ).then(({ data: res }) => {        this.dataListLoading = false        if (res.code !== 0) {          this.dataList = []          this.total = 0          return this.$message.error(res.msg)        }        this.dataList = this.mixinViewModuleOptions.getDataListIsPage ? res.data.list : res.data        this.total = this.mixinViewModuleOptions.getDataListIsPage ? res.data.total : 0      }).catch(() => {        this.dataListLoading = false      })    },    // 多选    dataListSelectionChangeHandle (val) {      this.dataListSelections = val    },    // 排序    dataListSortChangeHandle (data) {      if (!data.order || !data.prop) {        this.order = ''        this.orderField = ''        return false      }      this.order = data.order.replace(/ending$/, '')      this.orderField = data.prop.replace(/([A-Z])/g, '_$1').toLowerCase()      this.getDataList()    },    // 分页, 每页条数    pageSizeChangeHandle (val) {      this.page = 1      this.limit = val      this.getDataList()    },    // 分页, 当前页    pageCurrentChangeHandle (val) {      this.page = val      this.getDataList()    },    // 新增 / 修改    addOrUpdateHandle (id) {      this.addOrUpdateVisible = true      this.$nextTick(() => {        this.$refs.addOrUpdate.dataForm.id = id        this.$refs.addOrUpdate.init()      })    },    // 删除    deleteHandle (id) {      if (this.mixinViewModuleOptions.deleteIsBatch && !id && this.dataListSelections.length <= 0) {        return this.$message({          message: this.$t('prompt.deleteBatch'),          type: 'warning',          duration: 500        })      }      this.$confirm(this.$t('prompt.info', { 'handle': this.$t('delete') }), this.$t('prompt.title'), {        confirmButtonText: this.$t('confirm'),        cancelButtonText: this.$t('cancel'),        type: 'warning'      }).then(() => {        this.$http.delete(          `${this.mixinViewModuleOptions.deleteURL}${this.mixinViewModuleOptions.deleteIsBatch ? '' : '/' + id}`,          this.mixinViewModuleOptions.deleteIsBatch ? {            'data': id ? [id] : this.dataListSelections.map(item => item[this.mixinViewModuleOptions.deleteIsBatchKey])          } : {}        ).then(({ data: res }) => {          if (res.code !== 0) {            return this.$message.error(res.msg)          }          this.$message({            message: this.$t('prompt.success'),            type: 'success',            duration: 500,            onClose: () => {              this.getDataList()            }          })        }).catch(() => {})      }).catch(() => {})    },    // 导出    exportHandle () {      var params = qs.stringify({        'token': Cookies.get('token'),        ...this.dataForm      })      window.location.href = `${window.SITE_CONFIG['apiURL']}${this.mixinViewModuleOptions.exportURL}?${params}`    }  }</code></pre><p><strong>缓存</strong></p><pre class=" language-language-javascript"><code class="language-language-javascript">export default new Vuex.Store({  namespaced: true,  state: {    // 导航条, 布局风格, defalut(白色) / colorful(鲜艳)    navbarLayoutType: 'colorful',    // 侧边栏, 布局皮肤, default(白色) / dark(黑色)    sidebarLayoutSkin: 'dark',    // 侧边栏, 折叠状态    sidebarFold: false,    // 侧边栏, 菜单    sidebarMenuList: [],    sidebarMenuActiveName: '',    // 内容, 是否需要刷新    contentIsNeedRefresh: false,    ip: 'http://47.100.126.191:8081',    // 内容, 标签页(默认添加首页)    contentTabs: [      {        ...window.SITE_CONFIG['contentTabDefault'],        'name': 'home',        'title': 'home'      }    ],    contentTabsActiveName: 'home'  },  modules: {    user  },  mutations: {    // 重置vuex本地储存状态    resetStore (state) {      Object.keys(state).forEach((key) => {        state[key] = cloneDeep(window.SITE_CONFIG['storeState'][key])      })    }  }})</code></pre><p><strong>国际化</strong></p><pre class=" language-language-javascript"><code class="language-language-javascript"><script>import Cookies from 'js-cookie'import { messages } from '@/i18n'export default {  watch: {    '$i18n.locale': 'i18nHandle'  },  created () {    this.i18nHandle(this.$i18n.locale)  },  methods: {    i18nHandle (val, oldVal) {      Cookies.set('language', val)      document.querySelector('html').setAttribute('lang', val)      document.title = messages[val].brand.lg      // 非登录页面，切换语言刷新页面      if (this.$route.name !== 'login' && oldVal) {        window.location.reload()      }    }  }}</script></code></pre><h3 id="后台相关代码">后台相关代码</h3><p><strong>请求头token验证</strong></p><pre class=" language-language-java"><code class="language-language-java">public class Oauth2Filter extends AuthenticatingFilter {    @Override    protected AuthenticationToken createToken(ServletRequest request, ServletResponse response) throws Exception {        //获取请求token        String token = getRequestToken((HttpServletRequest) request);        if(StringUtils.isBlank(token)){            return null;        }        return new Oauth2Token(token);    }    @Override    protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) {        if(((HttpServletRequest) request).getMethod().equals(RequestMethod.OPTIONS.name())){            return true;        }        return false;    }    @Override    protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {        //获取请求token，如果token不存在，直接返回401        String token = getRequestToken((HttpServletRequest) request);        if(StringUtils.isBlank(token)){            HttpServletResponse httpResponse = (HttpServletResponse) response;            httpResponse.setContentType("application/json;charset=utf-8");            httpResponse.setHeader("Access-Control-Allow-Credentials", "true");            httpResponse.setHeader("Access-Control-Allow-Origin", HttpContextUtils.getOrigin());            String json = new Gson().toJson(new Result().error(ErrorCode.UNAUTHORIZED));            httpResponse.getWriter().print(json);            return false;        }        return executeLogin(request, response);    }    @Override    protected boolean onLoginFailure(AuthenticationToken token, AuthenticationException e, ServletRequest request, ServletResponse response) {        HttpServletResponse httpResponse = (HttpServletResponse) response;        httpResponse.setContentType("application/json;charset=utf-8");        httpResponse.setHeader("Access-Control-Allow-Credentials", "true");        httpResponse.setHeader("Access-Control-Allow-Origin", HttpContextUtils.getOrigin());        try {            //处理登录失败的异常            Throwable throwable = e.getCause() == null ? e : e.getCause();            Result r = new Result().error(HttpStatus.SC_UNAUTHORIZED, throwable.getMessage());            String json = new Gson().toJson(r);            httpResponse.getWriter().print(json);        } catch (IOException e1) {        }        return false;    }    /**     * 获取请求的token     */    private String getRequestToken(HttpServletRequest httpRequest){        //从header中获取token        String token = httpRequest.getHeader(Constant.TOKEN_HEADER);        //如果header中不存在token，则从参数中获取token        if(StringUtils.isBlank(token)){            token = httpRequest.getParameter(Constant.TOKEN_HEADER);        }        return token;    }}</code></pre><p><strong>验证授权</strong></p><pre class=" language-language-java"><code class="language-language-java">@Componentpublic class Oauth2Realm extends AuthorizingRealm {    @Autowired    private ShiroService shiroService;    @Override    public boolean supports(AuthenticationToken token) {        return token instanceof Oauth2Token;    }    /**     * 授权(验证权限时调用)     */    @Override    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {        UserDetail user = (UserDetail)principals.getPrimaryPrincipal();        //用户权限列表        Set<String> permsSet = shiroService.getUserPermissions(user);        SimpleAuthorizationInfo info = new SimpleAuthorizationInfo();        info.setStringPermissions(permsSet);        return info;    }    /**     * 认证(登录时调用)     */    @Override    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {        String accessToken = (String) token.getPrincipal();        //根据accessToken，查询用户信息        SysUserTokenEntity tokenEntity = shiroService.getByToken(accessToken);        //token失效        if(tokenEntity == null || tokenEntity.getExpireDate().getTime() < System.currentTimeMillis()){            throw new IncorrectCredentialsException(MessageUtils.getMessage(ErrorCode.TOKEN_INVALID));        }        //查询用户信息        SysUserEntity userEntity = shiroService.getUser(tokenEntity.getUserId());        //转换成UserDetail对象        UserDetail userDetail = ConvertUtils.sourceToTarget(userEntity, UserDetail.class);        //获取用户对应的部门数据权限        List<Long> deptIdList = shiroService.getDataScopeList(userDetail.getId());        userDetail.setDeptIdList(deptIdList);        //账号锁定        if(userDetail.getStatus() == 0){            throw new LockedAccountException(MessageUtils.getMessage(ErrorCode.ACCOUNT_LOCK));        }        SimpleAuthenticationInfo info = new SimpleAuthenticationInfo(userDetail, accessToken, getName());        return info;    }}</code></pre><p><strong>token生成</strong></p><pre class=" language-language-java"><code class="language-language-java">public class TokenGenerator {    public static String generateValue() {        return generateValue(UUID.randomUUID().toString());    }    private static final char[] HEX_CODE = "0123456789abcdef".toCharArray();    public static String toHexString(byte[] data) {        if(data == null) {            return null;        }        StringBuilder r = new StringBuilder(data.length*2);        for ( byte b : data) {            r.append(HEX_CODE[(b >> 4) & 0xF]);            r.append(HEX_CODE[(b & 0xF)]);        }        return r.toString();    }    public static String generateValue(String param) {        try {            MessageDigest algorithm = MessageDigest.getInstance("MD5");            algorithm.reset();            algorithm.update(param.getBytes());            byte[] messageDigest = algorithm.digest();            return toHexString(messageDigest);        } catch (Exception e) {            throw new RenException("token invalid", e);        }    }}</code></pre><p><strong>监听器</strong></p><pre class=" language-language-java"><code class="language-language-java">public class LeaderListenImpl implements TaskListener{    @Override    public void notify(DelegateTask delegateTask) {        ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();        HttpServletRequest request = requestAttributes.getRequest();        WebApplicationContext webApplicationContext = WebApplicationContextUtils.getWebApplicationContext(request.getServletContext());        SysUserService userService = webApplicationContext.getBean(SysUserService.class);        List<String> strings = new ArrayList<String>();        List<SysUserDTO> list = userService.getUserByRole(RoleEnum.LEADER.value());        for (SysUserDTO user : list) {            strings.add(user.getUsername());        }        delegateTask.addCandidateUsers(strings);        ApprovalTbService approvalTbService = webApplicationContext.getBean(ApprovalTbService.class);        ApprovalTbDTO approvalTbDTO = (ApprovalTbDTO) delegateTask.getVariable(ActivitiEnum.APPROVAL.value());        approvalTbService.updateType(RoleEnum.LEADERNAME.value(),approvalTbDTO.getId());        if(list.size() > 0){            Long dealId = (Long) delegateTask.getVariable(ActivitiEnum.DEALID.value());            UtilService utilService = webApplicationContext.getBean(UtilService.class);            utilService.updateDeal(dealId,list);        }    }}</code></pre><p><strong>定时任务</strong></p><pre class=" language-language-java"><code class="language-language-java">@Component("downTask")public class DownTask implements ITask{private Logger logger = LoggerFactory.getLogger(getClass());@Autowiredprivate ActivityService activityService;@Overridepublic void run(String params){List<ActivityDTO> allActivity = activityService.getAllActivity(ActivityEnum.DOWN.value());if(allActivity.size() > 0){for(ActivityDTO activityDTO : allActivity){activityService.changeState(activityDTO.getId());logger.debug(activityDTO.getId() + "资源已释放！");}}}}</code></pre><h3 id="企业项目淮安智慧谷OA相关文件">企业项目淮安智慧谷OA相关文件</h3><h4 id="文档文件">文档文件</h4><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%99%BA%E6%85%A7%E8%B0%B7OA-%E5%8E%9F%E5%A7%8B%E9%9C%80%E6%B1%8220200110.doc" target="_blank" rel="noopener">智慧谷OA-原始需求20200110.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%99%BA%E6%85%A7%E8%B0%B7OA-%E5%88%9D%E6%AD%A5%E9%9C%80%E6%B1%8220200112.docx" target="_blank" rel="noopener">智慧谷OA-初步需求20200112.docx</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E7%B3%BB%E7%BB%9F%E5%AE%8C%E5%96%84%E9%9C%80%E6%B1%82.docx" target="_blank" rel="noopener">系统完善需求.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E7%B3%BB%E7%BB%9F%E5%AE%8C%E5%96%84%E9%9C%80%E6%B1%822.docx" target="_blank" rel="noopener">系统完善需求2.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E5%89%A9%E4%BD%99%E5%A4%84%E7%90%86%E9%A1%B9.docx" target="_blank" rel="noopener">剩余处理项.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E5%89%A9%E4%BD%99%E5%A4%84%E7%90%86%E9%A1%B93.docx" target="_blank" rel="noopener">剩余处理项3.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E5%89%A9%E4%BD%99%E5%A4%84%E7%90%86%E9%A1%B94.1.docx" target="_blank" rel="noopener">剩余处理项4.1.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E5%89%A9%E4%BD%99%E5%A4%84%E7%90%86%E9%A1%B95-%E6%9C%80%E6%96%B0.docx" target="_blank" rel="noopener">剩余处理项4.5-最新.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E5%89%A9%E4%BD%99%E5%A4%84%E7%90%86%E9%A1%B94.2--%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%9C.docx" target="_blank" rel="noopener">剩余处理项4.2.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/11.27%E5%AE%8C%E5%96%84%E6%95%B4%E7%90%86.docx" target="_blank" rel="noopener">11.27完善整理.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/11.27%E5%AE%8C%E5%96%84%E6%95%B4%E7%90%86%20-%20%E5%8F%8D%E9%A6%88.docx" target="_blank" rel="noopener">11.27完善整理 - 反馈.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/1.21.docx" target="_blank" rel="noopener">1.9 后台测试反馈.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/1.21.docx" target="_blank" rel="noopener">1.21.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E9%A1%B9%E7%9B%AE%E4%BF%A1%E6%81%AF.xlsx" target="_blank" rel="noopener">资产管理模块需求-二期-暂时不做.docx</a></p><h4 id="表单文件">表单文件</h4><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%9D%A5%E8%AE%BF%E8%A1%A8%E5%8D%95.docx" target="_blank" rel="noopener">来访表单.doc</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%B4%BB%E5%8A%A8%E7%94%B3%E8%AF%B7%E8%A1%A8%E5%8D%95.docx" target="_blank" rel="noopener">活动申请表单.doc</a></p><h4 id="xlsx导入模板">xlsx导入模板</h4><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E9%A1%B9%E7%9B%AE%E4%BF%A1%E6%81%AF.xlsx" target="_blank" rel="noopener">项目信息.xlsx</a></p><p><a href="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E4%BA%BA%E6%89%8D%E4%BF%A1%E6%81%AF.xlsx" target="_blank" rel="noopener">人才信息.xlsx</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 项目开发 </category>
          
          <category> 答辩 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vue </tag>
            
            <tag> JWT </tag>
            
            <tag> Avtiviti </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>红黑树</title>
      <link href="/2020/08/01/hong-hei-shu/"/>
      <url>/2020/08/01/hong-hei-shu/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么要使用红黑树">为什么要使用红黑树</h3><p><strong>当冲突的链表长度超过8个时，链表结构就会转为红黑树结构，这样做的好处是避免在极端条件的情况下冲突链表过长而导致查询效率非常慢</strong><br><strong>红黑树是一种近似平衡的二叉查找树，其主要的优点就是平衡，即左右子树高度几乎一致，以此来防止树退化为链表，通过这种方式来保障查找的时间复杂度为log(n)</strong></p><h3 id="红黑树">红黑树</h3><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626184132.png" alt="红黑树" title="红黑树"></p><table><thead><tr><th>红黑树要求</th></tr></thead><tbody><tr><td>1.每个节点或者是黑色，或者是红色</td></tr><tr><td>2.根节点是黑色</td></tr><tr><td>3.每个叶子结点（这里的叶子结点不是传统的叶子结点，是指为空的叶子结点）是黑色</td></tr><tr><td>4.如果一个结点是红色的，则它的子结点必须是黑色的</td></tr><tr><td>5.从一个结点到该结点的子孙结点的所有路径上包含相同数目的黑结点</td></tr><tr><td>6.新加入到红黑树的节点为红色节点</td></tr></tbody></table><p><strong>在树的结构发生改变时，往往会破坏红色节点的孩子和父亲都不能是红色的和从任意节点到其子树中每个叶子结点的路径有相同数量的黑色节点这两个条件，需要通过调整使得查找树重新满足红黑树的条件</strong></p><h3 id="调整方式">调整方式</h3><h4 id="颜色调整">颜色调整</h4><p><strong>转换节点的颜色</strong></p><h4 id="结构调整">结构调整</h4><h5 id="左旋">左旋</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626185320.png" alt="左旋" title="左旋"></p><table><thead><tr><th>序号</th><th>步骤</th></tr></thead><tbody><tr><td>1</td><td>Y的左孩子设为X的右孩子，即β的父亲为X</td></tr><tr><td>2</td><td>将X的父亲设为Y的父亲</td></tr><tr><td>2.1</td><td>若X的父亲为空，则Y为根节点</td></tr><tr><td>2.2</td><td>X为他父亲的左节点，则Y为X父亲的左节点；X为他父亲的右节点，则Y为X父亲的右节点</td></tr><tr><td>3</td><td>X设为Y的左节点，即X的父亲为Y</td></tr></tbody></table><pre class=" language-language-java"><code class="language-language-java">static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {    //指向父节点的指针 TreeNode<K,V> parent;    //指向左孩子的指针 TreeNode<K,V> left;    //指向右孩子的指针 TreeNode<K,V> right;    //前驱指针，跟next属性相反的指向 TreeNode<K,V> prev;    //是否为红色节点 boolean red;    //左旋方法如下    static <K, V> TreeNode<K, V> rotateLeft(TreeNode<K, V> root,                                            TreeNode<K, V> x) {        TreeNode<K, V> y, parent, β;//x表示要调整的节点，y表示x的右节点，parent表示x的parent节点，β表示x的右孩子的左孩子节点        //判断y，如果y为空则旋转没有意义        if (y != null && (β = y.left) != null) {            //设置的β父亲为x，β为x的右节点            if ((β = y.left) != null)                x.right = β;            //判断x的父亲，为空，y为根节点，根节点的话就设置为黑色            if ((parent = x.parent ) == null)                (root = y).red = false;                //判断x节点是左儿子还是右儿子            else if (parent.left == x)                parent.left = y;            else                parent.right = y;            y.left = x;            x.parent = y;        }        return root;    }}</code></pre><h5 id="右旋">右旋</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626191905.png" alt="右旋" title="右旋"><br><strong>右旋和左旋的原理相同</strong></p><h5 id="插入新节点调整">插入新节点调整</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626192839.png" alt="插入" title="插入"><br><strong>插入新节点为红色是为了不违背特性5</strong><br><strong>当前节点(即，被插入节点)的父节点是红色，且当前节点的祖父节点的另一个子节点（叔叔节点）也是红色</strong><br><strong>(1) 将“父节点”设为黑色。<br>(2) 将“叔叔节点”设为黑色。<br>(3) 将“祖父节点”设为“红色”。<br>(4) 将“祖父节点”设为“当前节点”(红色节点)；即，之后继续对“当前节点”进行操作</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626193021.png" alt="调整" title="调整"><br><strong>“当前节点”和“父节点”都是红色，违背“特性(4)”。将“父节点”设置“黑色”以解决这个问题。<br>但是，将“父节点”由“红色”变成“黑色”之后，违背了“特性(5)”：因为，包含“父节点”的分支的黑色节点的总数增加了1。 解决这个问题的办法是：将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”。关于这里，说明几点：第一，为什么“祖父节点”之前是黑色？这个应该很容易想明白，因为在变换操作之前，该树是红黑树，“父节点”是红色，那么“祖父节点”一定是黑色。 第二，为什么将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；能解决“包含‘父节点’的分支的黑色节点的总数增加了1”的问题。这个道理也很简单。“包含‘父节点’的分支的黑色节点的总数增加了1” 同时也意味着 “包含‘祖父节点’的分支的黑色节点的总数增加了1”，既然这样，我们通过将“祖父节点”由“黑色”变成“红色”以解决“包含‘祖父节点’的分支的黑色节点的总数增加了1”的问题； 但是，这样处理之后又会引起另一个问题“包含‘叔叔’节点的分支的黑色节点的总数减少了1”，现在我们已知“叔叔节点”是“红色”，将“叔叔节点”设为“黑色”就能解决这个问题。 所以，将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；就解决了该问题。<br>按照上面的步骤处理之后：当前节点、父节点、叔叔节点之间都不会违背红黑树特性，但祖父节点却不一定。若此时，祖父节点是根节点，直接将祖父节点设为“黑色”，那就完全解决这个问题了；若祖父节点不是根节点，那我们需要将“祖父节点”设为“新的当前节点”，接着对“新的当前节点”进行分析。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626193519.png" alt=""><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626193543.png" alt=""></p><h5 id="红黑树的查询">红黑树的查询</h5><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200626193712.png" alt="红黑树查询" title="红黑树查询"></p><h5 id="红黑树的删除">红黑树的删除</h5><h6 id="被删除的D节点为红色。这种情况，则与D相关的颜色以及结构关系必然只有如下一种情况">被删除的D节点为红色。这种情况，则与D相关的颜色以及结构关系必然只有如下一种情况</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/317300-20160505091642982-1989959722.png" alt="D节点为红色" title="D节点为红色"><br><strong>因为D为红色，所以P为黑色，同时DR不可能为红色(否则违反性质4)。同时由于性质5，则DR必为Nil，否则就D树来说，经过DR与不经过DR的路径的黑节点数必不相同。现在要删除D节点，只需要直接将D节点删除，并将DR作为P的左子节点</strong></p><h6 id="被删除的D节点为黑色">被删除的D节点为黑色</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/317300-20160505091836732-1760252339.png" alt="D为黑色" title="D为黑色"><br><strong>由于删除的D为黑色，删除后P的左子树的黑节点数必少1，此时刚好DR为黑色，并且删除后DR可以占据D的位置，然后再将DR的颜色改为黑色，刚好可以填补P左子树所减少的黑节点数。</strong></p><h6 id="被删除的D为黑色，且DR为Nil">被删除的D为黑色，且DR为Nil</h6><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/317300-20160505091941513-167154460.png" alt=""><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/317300-20160505092013701-2109174939.png" alt=""><br><strong>转换后，虽然对于P树的左子树的黑节点数仍然会比右子树的黑节点数少1，但此时DR的兄弟(以前的S节点)现在已经变为SL，即已经由红色变为黑色，并且非常重要的此时的DR的兄弟节点SL的子结点(即：DR的两个侄子节点)，要不就是红色节点要不就必为Nil节点，而这种情况正是D为黑色、S也黑色的情况</strong></p><p><strong>所有图片来源于网络。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HashMap </tag>
            
            <tag> 二叉树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo插件及优化</title>
      <link href="/2020/07/31/hexo-cha-jian-tui-jian/"/>
      <url>/2020/07/31/hexo-cha-jian-tui-jian/</url>
      
        <content type="html"><![CDATA[<h3 id="hexo常用插件及优化">hexo常用插件及优化</h3><p>这里使用的主题为matery，闪烁之狐，matery主题的<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">github</a>仓库，作者提供的文档里已经给出了像代码高亮，拼音转换等插件，所以本文介绍的是除了作者提供的之外的其他简约插件。</p><h4 id="matery使用需知">matery使用需知</h4><h5 id="背景粒子和彩带设置">背景粒子和彩带设置</h5><p>matery的主题提供了这几项功能，但是默认是没有开启的，因此使用该主题需要手动开启，在themes/hexo-theme-matery目录下的_config.yml配置文件中修改以下几项。</p><pre class=" language-language-yml"><code class="language-language-yml">#背景canvas-nestcanvas_nest:  enable: true  color: 0,0,255 # 线条颜色, 默认: '0,0,0' ；三个数字分别为(R,G,B)，注意用,分$  pointColor: 0,0,255 # 交点颜色, 默认: '0,0,0' ；三个数字分别为(R,G,B)，注意$  opacity: 0.7 # 线条透明度（0~1）, 默认: 0.5  zIndex: -1 # 背景的 z-index 属性，css 属性用于控制所在层的位置, 默认: -1.  count: 99 # 线条的总数量, 默认: 99  # 背景静止彩带.ribbon:  enable: true  size: 150 # 彩带大小, 默认: 90.  alpha: 0.6 # 彩带透明度 (0 ~ 1), 默认: 0.6.  zIndex: -1 # 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1.  clickChange: false  # 设置是否每次点击都更换彩带.# 背景动态彩带.ribbon_dynamic:  enable: true</code></pre><p>enable选项设置为true即可。</p><p>因为我这里的hexo博客是放在我的阿里云服务器上的，因此如果要修改内容我推荐smartty软件进行远程并修改文件相对方便。</p><h5 id="导航栏以及底部的配色修改">导航栏以及底部的配色修改</h5><p>matery的自带的导航栏鲜艳的配色有点问题，所以这里可以自己修改一下。</p><p>在主题下source下的css目录下，修改matery.css。分别修改第268和280行。</p><pre class=" language-language-css"><code class="language-language-css">.bg-color {    background-image: linear-gradient(to right, #4cbf30 0%, #0f9d58 100%);}.progress-bar {    height: 4px;    position: fixed;    bottom: 0;    z-index: 300;    background: linear-gradient(to right, #4cbf30 0%, #0f9d58 100%);    opacity: 0.8;}</code></pre><p>我这里修改为#4cbf30 0%, #0f9d58 100%</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%B8%90%E5%8F%98.png" alt=""></p><p>你也可以修改为自己看着好看的颜色，我这里放上几个配色的网站，可以由自己选择。</p><p><a href="https://coolors.co/" target="_blank" rel="noopener">https://coolors.co/</a></p><p><a href="http://www.materialpalette.com/" target="_blank" rel="noopener">http://www.materialpalette.com/</a></p><h5 id="轮播图和笔记展示图">轮播图和笔记展示图</h5><p>如果你在写笔记的时候不想自己上传封面的话而使用默认的图片的话，这里可以对封面的图片进行重新上传，选择好看的即可。</p><p>分别替换掉主题下source下medias中的banner和featureimages这两个文件夹下的图片即可。</p><p>同时如果想要增加featureimages下的图片，还需同步更改主题下_config.yml配置文件里的featureImages模块</p><pre class=" language-language-yml"><code class="language-language-yml"># The post featured images that needs to be displayed when there is no image.# 无文章特色图片时需要显示的文章特色图片.featureImages:- /medias/featureimages/0.jpg- /medias/featureimages/1.jpg- /medias/featureimages/2.jpg- /medias/featureimages/3.jpg- /medias/featureimages/4.jpg- /medias/featureimages/5.jpg- /medias/featureimages/6.jpg- /medias/featureimages/7.jpg- /medias/featureimages/8.jpg- /medias/featureimages/9.jpg- /medias/featureimages/10.jpg- /medias/featureimages/11.jpg- /medias/featureimages/12.jpg- /medias/featureimages/13.jpg- /medias/featureimages/14.jpg- /medias/featureimages/15.jpg- /medias/featureimages/16.jpg- /medias/featureimages/17.jpg- /medias/featureimages/18.jpg- /medias/featureimages/19.jpg- /medias/featureimages/20.jpg- /medias/featureimages/21.jpg- /medias/featureimages/22.jpg- /medias/featureimages/23.jpg</code></pre><p>同样，这里提供几个图片网站。替换图片时，图片的分辨率无需修改。</p><p><a href="https://wallhaven.cc/(%E8%B6%85%E6%A3%92)" target="_blank" rel="noopener">https://wallhaven.cc/(超棒)</a></p><p><a href="https://konachan.net/post" target="_blank" rel="noopener">https://konachan.net/post</a></p><h3 id="看板娘插件">看板娘插件</h3><p>注：如果你使用了matery的卜算子来统计数据的话，官方的helper-live的看板娘插件会使卜算子失效，同时官方提供的插件功能也较少，我这里推荐使用<a href="https://github.com/stevenjoezhang/live2d-widget" target="_blank" rel="noopener">该插件</a>。</p><p>如果是新手可以直接使用作者提供的js即可。</p><p><a href="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js</a></p><p>如果使用这个插件，会覆盖原matery主题的吸底模式的音乐播放插件。这里提供两种方式解决。</p><h4 id="fork新的仓库">fork新的仓库</h4><p>通过fork作者新的仓库，修改对应的css和js文件，这里在作者的github仓库下有对应的介绍，因此不再赘述。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fork.png" alt=""></p><p>修改适合自己的css和js文件后，发布新的版本，使用github的cdn加速获取js即可。</p><p>src=“<a href="https://cdn.jsdelivr.net/gh/username/live2d-widget@latest/autoload.js" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/username/live2d-widget@latest/autoload.js</a>”</p><p>将这里的用户名替换自己的即可。若直接访问js文件访问失败403的话，因为缓存的原因可能不能显示，但是在自己的博客使用没有问题。</p><h4 id="修改吸底模式的音乐插件。">修改吸底模式的音乐插件。</h4><p>因为修改看板娘的文件之后还要修改回到顶端插件，如果你设置了在线聊天也要修改。如果你这里不想麻烦的话，直接将音乐插件的吸底模式取消即可。</p><p>在主题下的_config.yml配置文件中，修改music模块。将吸底模式关闭即可。</p><pre class=" language-language-yml"><code class="language-language-yml"># Whether to display the musics.# 是否在首页显示音乐.music:  enable: true  title: #非吸底模式有效    enable: true    show: 听听音乐  autoHide: true    # hide automaticaly  server: netease   #requiremusic platform: netease, tencent, kugou, xiami, baidu  type: playlist    #require song, playlist, album, search, artist  id: 503838841     #requiresong id / playlist id / album id / search keyword  fixed: false       # 开启吸底模式  autoplay: false   # 是否自动播放  theme: '#42b983'  loop: 'all'       # 音频循环播放, 可选值: 'all', 'one', 'none'  order: 'random'   # 音频循环顺序, 可选值: 'list', 'random'  preload: 'auto'   # 预加载，可选值: 'none', 'metadata', 'auto'  volume: 0.7       # 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效  listFolded: true  # 列表默认折叠  hideLrc: true     # 隐藏歌词</code></pre><h3 id="小地球插件">小地球插件</h3><p>小地球插件的地址<br><a href="https://www.revolvermaps.com/?target=setupgl">https://www.revolvermaps.com/?target=setupgl</a></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E5%B0%8F%E5%9C%B0%E7%90%83.png" alt=""></p><p>复制上面的srcipt标签放在自己的博客中合适的位置即可。</p><h3 id="天气插件">天气插件</h3><p>生成js地址</p><p><a href="https://cj.weather.com.cn/plugin/index" target="_blank" rel="noopener">https://cj.weather.com.cn/plugin/index</a></p><p>根据自己的需求选择对应的插件即可，我这里选择的是简约插件，UI比较漂亮</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%A0%B9%E6%8D%AE%E8%87%AA%E5%B7%B1%E9%80%89%E6%8B%A9%E5%AF%B9%E5%BA%94%E6%A8%A1%E5%9D%97.png" alt=""></p><p>选择对应的选项，在下方生成对应的js文件即可。</p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E7%94%9F%E6%88%90js.png" alt=""></p><p>复制你的js文件放到你的主题下的相应的文件里即可，这里不对主题的文件进行介绍了，有一些vue基础就可以看懂了。</p><h3 id="阿里云部署博客">阿里云部署博客</h3><p>如果使用github来部署的话访问较慢，所以我这里推荐阿里云服务器部署，学生机10块钱一个月很便宜。如果是上班族的话，找个亲戚或朋友家的大学生不是计算机专业的认证一下就好。</p><h4 id="安装node-以ubuntu16-04为例">安装node(以ubuntu16.04为例)</h4><p>更新一下服务器的源</p><pre><code>apt-get updateapt-get install -y python-software-properties software-properties-commonadd-apt-repository ppa:chris-lea/node.jsapt-get update</code></pre><p>node安装</p><pre><code>apt-get install nodejsapt install nodejs-legacyapt install npm</code></pre><p>将npm的源设置为淘宝源，下载加速。</p><pre><code>npm config set registry https://registry.npm.taobao.orgnpm config list</code></pre><p>安装版本管理器</p><pre><code>npm install n -g</code></pre><h4 id="安装git">安装git</h4><pre><code>apt-get install git</code></pre><p>配置用户名和邮箱</p><pre><code>git config --global user.name "你的用户名"git config --global user.email "你的注册邮箱"</code></pre><h4 id="安装nginx">安装nginx</h4><pre><code>apt-get install nginx</code></pre><p>安装完nginx后，这里以ubuntu16.04为例，nginx的目录为/ect/nginx。我们修改配置文件只需要修改sites-available下的default即可。我这里在之前学习的时候都是不一样的路径，所以我这里以ubuntu为例。</p><pre><code>server {        listen 80 default_server;        listen [::]:80 default_server;        listen 443 ssl;        ssl off;        ssl_certificate cert/4287626_www.dingdm.club.pem;        ssl_certificate_key cert/4287626_www.dingdm.club.key;        ssl_session_timeout 5m;        #ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aN$        ssl_ciphers ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS;        ssl_protocols TLSv1 TLSv1.1 TLSv1.2;        ssl_prefer_server_ciphers on;        root /home/www/hexo;        server_name dingdm.club;}</code></pre><p>server_name为你的域名，root为你的静态网站所在的路径，可以自己创建，<strong>注意这里的路径不是你的hexo博客的路径，如果你在服务器上创建博客的话</strong>。</p><p>ssl的配置为开启https的配置，这里先不赘述，不需要的话可以将ssl相关的配置注释掉即可。</p><h4 id="初始化git仓库">初始化git仓库</h4><p>添加git用户，使用root账户也可以。</p><pre><code>adduser git</code></pre><p>修改文件内容赋予权限</p><pre><code>chmod 740 /etc/sudoersnano /etc/sudoers</code></pre><p>找到root用户那，在下方添加一条</p><pre><code>git ALL=(ALL) ALL</code></pre><p>将文件权限恢复</p><pre><code>chmod 400 /etc/sudoers</code></pre><p>切换到git用户，并创建ssh文件夹</p><pre><code>su gitcd ~mkdir .sshcd .ssh</code></pre><p>生成密钥公钥并配置</p><pre><code>ssh-keygencp id_rsa.pub authorized_keyschmod 600 ~/.ssh/authorized_keyschmod 700 ~/.ssh</code></pre><p>配置完成之后可以使用<code>ssh -v git@ip</code>连接测试</p><p>新建仓库并设置钩子脚本</p><pre><code>cd ~git init --bare blog.gitnano ~/blog.git/hooks/post-receive</code></pre><p>输入以下内容</p><pre><code>git --work-tree=/home/www/hexo --git-dir=/home/git/blog.git checkout -f</code></pre><p>/home/www/hexo为你刚刚在nginx的配置文件里的root路径，/home/git/blog.git为你的仓库地址</p><h4 id="hexo部署">hexo部署</h4><p>在你的hexo博客下的_config.yml配置文件中修改deploy模块</p><pre class=" language-language-yml"><code class="language-language-yml">deploy:  type: git  repo: git@公网IP:/home/git/blog.git    </code></pre><p>修改完成之后运行<code>hexo clean &amp;&amp; hexo deploy</code>部署你的hexo博客即可。</p><p>部署完成之后，使用/etc/init.d/nginx configtest检查一下nginx的配置是否有问题，如果没问题，运行<code>/etc/init.d/nginx restart</code>重启nginx服务即可。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
          <category> 美化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> git </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一次steam个人资料动态设置</title>
      <link href="/2020/07/30/yi-ci-steam-ge-ren-zi-liao-dong-tai-she-zhi/"/>
      <url>/2020/07/30/yi-ci-steam-ge-ren-zi-liao-dong-tai-she-zhi/</url>
      
        <content type="html"><![CDATA[<h3 id="首先放上预览图">首先放上预览图</h3><p><a href="https://steamcommunity.com/profiles/76561198366411843" target="_blank" rel="noopener">https://steamcommunity.com/profiles/76561198366411843</a></p><h3 id="资料背景设置">资料背景设置</h3><p><strong>需要steam等级到达十级，拥有自己的艺术展柜。</strong><br><strong>首先需要在steam的社区-市场购买自己想要的个人资料背景，当然目前处于夏季促销，可以通过点数购买比较好看的背景。如果没有游戏的话，可以先买游戏，买完背景之后在退款就好了。</strong><br><strong>这里登陆不上市场需要挂加速器，UU加速器的免费steam加速就可以。</strong><br><strong>我这里选择的是Meteor Shower</strong><br><img src="https://steamcdn-a.akamaihd.net/steamcommunity/public/images/items/645690/06e52a6057125c91e4d88fd00e1f486bdee9a654.jpg" alt=""></p><h4 id="资料背景切割">资料背景切割</h4><p><a href="http://sapic.github.io/" target="_blank" rel="noopener">http://sapic.github.io/</a><br><strong>在当前页面上进行steam的登录，他会自动检索你的库存中的背景资料，自己选择即可。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628231317.png" alt="下载" title="下载"><br><strong>自行下载对应的文件然后解压就可以看到对应的三个文件。</strong></p><h4 id="PS进行动态设置">PS进行动态设置</h4><p><strong>首先打开静态背景图片，然后调出时间轴，创建帧动画。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628231525.png" alt="静态图片" title="静态图片"><br><strong>导入你自己的动态图片，然后选择所有帧之后，在选择拷贝所有帧。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628231735.png" alt="选择所有帧" title="选择所有帧"><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628231748.png" alt="拷贝所有帧" title="拷贝所有帧"><br><strong>选择静态背景，粘贴所有的帧，并置于图层之上</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628231911.png" alt="粘贴所有帧" title="粘贴所有帧"><br><img src="http://144.202.57.217:8080/uploadImages/63ac0df5b0ee466e83c0945faa381be8.png" alt="图层之上" title="图层之上"><br><strong>调整动画位置，设置动画循环，导出即可。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628232116.png" alt="循环" title="循环"><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628232133.png" alt="导出" title="导出"><br><strong>预览没有问题，导出即可。注意gif的大小不应超过8M</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628232148.png" alt="保存" title="保存"></p><h4 id="背景资料上传">背景资料上传</h4><p><a href="https://steamcommunity.com/login/home/?goto=%2Fsharedfiles%2Fedititem%2F767%2F3%2F" target="_blank" rel="noopener">https://steamcommunity.com/login/home/?goto=%2Fsharedfiles%2Fedititem%2F767%2F3%2F</a><br><strong>打开网页版的steam进行背景上传。</strong><br><strong>艺术作品的名字为空，防止上传完成之后出现名字。直接复制即可。</strong><br><code>⁡⁡⁡ ⁡⁠</code><br><strong>上传自己的除了头像外的其他两张图片，侧边和自己的gif。图片需要一张一张传</strong><br><strong>注意</strong><br><strong>上传完图片和添加完名字后，打开网页的console控制台。chrome右键检查即可。输入以下代码。</strong></p><pre class=" language-language-javascript"><code class="language-language-javascript">var num= document.getElementsByName("image_width")[0].value;document.getElementsByName("image_height")[0].value = num-(num-1);document.getElementsByName("image_width")[0].value= num*100;</code></pre><p><strong>这里代码的运行结果不为0即可，为0的话重复运行。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628232812.png" alt="代码" title="代码"></p><h4 id="展柜设置">展柜设置</h4><p><strong>选择刚刚上传的图片保存即可。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200628232908.png" alt="艺术展柜" title="艺术展柜"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 游戏 </category>
          
          <category> 美化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> steam </tag>
            
            <tag> ps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次服务器部署内容</title>
      <link href="/2020/07/28/ji-yi-ci-fu-wu-qi-bu-shu-nei-rong/"/>
      <url>/2020/07/28/ji-yi-ci-fu-wu-qi-bu-shu-nei-rong/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>由于目前国内的域名需要备案，因此购买国外的域名，同时由于国外的域名解析到国内的服务器也需要备案，因此服务器也使用国外的，当然，需要一个备案的国内域名用于七牛CDN使用。</strong></p><h1>1.服务器域名选择</h1><p><strong>之前一直用的搬瓦工的服务器，但是现在搬瓦工的服务器配置高了，价格很贵，不是很划算，于是就用了vultr的服务器，五美元一个月，价格还可以，除了建站之外，当作梯子也是划算的。<br>vultr的平台需要先充值，最低额度十美元，支持支付宝付款还是可以的。</strong></p><h2 id="1-1-服务器购买">1.1 服务器购买</h2><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%89%E6%8B%A9.png" alt=""><br><strong>服务器类型选择Cloud Compute就可以，用来个人需求绰绰有余。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/fuwuqixuanze" alt=""><br><strong>国家及城市根据自己需求选择就可以。<br>服务器操作系统我推荐Ubuntu16.04</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624211528.png" alt=""><br><strong>其他默认就好，购买完成就会看到自己的服务器</strong></p><h3 id="1-1-1">1.1.1</h3><p><strong>这里推荐两个软件，smarrty用于远程连接，Filezilla用于向服务器传输文件。<br>安装包我放在资源里面，可以在下载中自行下载。</strong></p><h2 id="1-2-域名购买">1.2 域名购买</h2><p><strong>域名购买我推荐nameSilo，不必要购买特别贵的，自己可以使用的就好<br>购买时挂梯子应该打开会快一点</strong><br><strong>官方网址<br><a href="https://www.namesilo.com/" target="_blank" rel="noopener">https://www.namesilo.com/</a><br>搜索自己想要的域名，然后进行购买</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624220852.png" alt=""><br><strong>选择第一个就可以了</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624220934.png" alt=""><br><strong>选择对应的配置和优惠码，优惠码百度一下，很多的，基本都是减一美元</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624221137.png" alt=""></p><h3 id="1-2-1-域名解析">1.2.1 域名解析</h3><p><strong>域名购买完成之后，点击manage my domains进入自己的域名管理页面</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624221243.png" alt=""><br><strong>下面显示的就是你刚刚购买的域名，然后点击蓝色的小球，dns设置</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624221256.png" alt=""><br><strong>进入到域名解析页面你会看到五条记录，将第三条和第四条删掉，然后编辑第一条和第二条。<br>第一条编辑的时候在ipv4栏输入你的服务器ip地址<br>第二条编辑的时候在1的基础上还要在hostname输入www</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624221324.png" alt=""></p><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200624221337.png" alt=""></p><h1>2.服务器配置及部署</h1><h2 id="2-1-服务器配置">2.1 服务器配置</h2><h3 id="2-1-1-java环境安装">2.1.1 java环境安装</h3><p><strong>由于我的项目是基于java开发的，所以安装Java环境。</strong><br><strong>首先卸载之前安装或参与的项，如果没有请忽略</strong><br><code>apt-get remove openjdk</code><br><strong>可以通过Oracle官网下载对应的jdk和在线安装两种方式进行安装</strong><br><strong>可以通过<code>apt install openjdk-8-jdk</code>的方式在线安装<br>也可以通过https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html<br>官网下载对应的版本的jdk进行解压安装</strong></p><h4 id="2-1-1-1-配置环境变量">2.1.1.1 配置环境变量</h4><p><code>vim  ~/.bashrc</code><strong>也可以使用nano或vi 如果未安装vim</strong><br><strong>文件末尾添加以下内容</strong></p><pre class=" language-language-shell"><code class="language-language-shell">export JAVA_HOME=/usr/local/jdk1.8.0_211export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=.:${JAVA_HOME}/bin:$PATH</code></pre><p><strong>记得更换为自己文件所在的路径</strong><br><strong>保存变量并更新应用</strong><br><code>source ~/.bashrc</code><br><strong>使用</strong><code>java -version</code><strong>查看自己的java版本即可</strong></p><h3 id="2-1-2-docker安装">2.1.2 docker安装</h3><p><strong>由于项目使用到的redis，rabbitmq，mongodb都是基于docker容器的，因此先进行配置。</strong><br><strong>卸载存在的docker的旧版本</strong><br><code>apt-get remove docker docker-engine docker-ce docker.io</code><br><strong>更新apt包</strong><br><code>apt-get update</code><br><strong>安装以下包通过https使用存储库</strong><br><code>apt-get install -y apt-transport-https ca-certificates curl software-properties-common</code><br><strong>添加docker的GPG密钥</strong><br><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</code><br><strong>设置stable存储库</strong><br><code>add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"</code><br><strong>更新apt包</strong><br><code>apt-get update</code><br><strong>安装docker</strong><br><code>apt-get install -y docker-ce</code><br><strong>验证服务是否启动</strong><br><code>systemctl status docker</code><br><strong>docker未启动，则手动启动docker</strong><br><code>systemctl start docker</code><br><strong>docker安装完成</strong></p><h4 id="2-1-2-1-docker安装mongodb">2.1.2.1 docker安装mongodb</h4><p><strong>下载镜像</strong><br><code>docker pull registry.docker-cn.com/library/mongo</code><br><strong>创建数据文件夹</strong><br><code>mkdir /data/mongodb0</code><br><strong>启动容器</strong><br><code>docker run --name mongodb-server0 -v /data/mongodb0:/data/db -p 27017:27017 -d 镜像ID --auth</code><br><strong>-v后面的参数表示把数据文件挂载到宿主机的路径<br>-p把mongo端口映射到宿主机的指定端口<br>–auth表示连接mongodb需要授权</strong></p><h4 id="2-1-2-2-docker安装redis">2.1.2.2 docker安装redis</h4><p><strong>拉取镜像</strong><br><code>docker pull redis</code><br><strong>启动镜像</strong><br><code>docker run -p 6379:6379 -d redis:latest redis-server</code><br><strong>-p 6379:6379 : 将容器的6379端口映射到主机的6379端口<br>-v $PWD/data:/data : 将主机中当前目录下的data挂载到容器的/data<br>redis-server --appendonly yes : 在容器执行redis-server启动命令，并打开redis持久化配置</strong><br><strong>docker的相关命令在其他笔记中已经说明过</strong><br><strong>redis的操作软件在附件中也已经上传，可以自行下载</strong></p><h4 id="2-1-2-3-docker安装rabbitMq">2.1.2.3 docker安装rabbitMq</h4><p><strong>拉取镜像</strong><br><code>docker pull rabbitmq:management</code><br><strong>运行镜像</strong><br><code>docker run -d -p 5672:5672 -p 15672:15672 --name rabbitmq rabbitmq:management</code><br><strong>访问管理页面<br>http://[宿主机IP]:15672</strong></p><h3 id="2-1-3-nginx安装">2.1.3 nginx安装</h3><p><strong>因为我使用了两个域名，一个为8080端口，一个为8060端口，因此需要使用nginx来进行端口的转发。</strong><br>安装nginx<br><code>apt-get install nginx</code><br><strong>安装好以后，nginx默认安装位置在etc目录下，这里我在使用的时候出现了nginx文件限制的问题，因此在这里统一进行说明。<br>在nginx下的nginx.conf配置文件中的http段对文件大小进行设置。</strong><br><code>client_max_body_size 100m;</code><br><strong>在sites-available下进行对应的端口转发配置</strong></p><pre class=" language-language-shell"><code class="language-language-shell">server {listen 80 default_server;listen [::]:80 default_server;root /var/www/html;index index.html index.htm index.nginx-debian.html;server_name dinggc.info;location / {      proxy_set_header   X-Real-IP        $remote_addr;      proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;      proxy_set_header   Host             $http_host;      proxy_set_header   Upgrade          $http_upgrade;      proxy_set_header   X-NginX-Proxy    true;      proxy_set_header   Connection "upgrade";      proxy_http_version 1.1;      proxy_pass         http://localhost:8080;   }}</code></pre><pre class=" language-language-shell"><code class="language-language-shell">server {listen 80;   # 修改为你的域名   server_name dinggc.online;   location / {      proxy_set_header   X-Real-IP        $remote_addr;      proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;      proxy_set_header   Host             $http_host;      proxy_set_header   Upgrade          $http_upgrade;      proxy_set_header   X-NginX-Proxy    true;      proxy_set_header   Connection "upgrade";      proxy_http_version 1.1;      proxy_pass         http://localhost:8060;   }}</code></pre><p><strong>使用</strong><code>/etc/init.d/nginx configtest</code><strong>进行配置测试如果成功则使用</strong><code>/etc/init.d/nginx reload</code><strong>重启nginx即可</strong><br><strong>至此，相关的服务器配置安装完成。</strong></p><h1>3 项目部署</h1><p><strong>使用filezilla软件将相关文件上传到服务器之后在线运行即可。<br>若使用nohup后台运行的话，注意不要直接关掉smartty，而应该使用exit命令退出命令行。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> ubuntu </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>springboot的启动流程</title>
      <link href="/2020/07/28/springboot-de-qi-dong-liu-cheng/"/>
      <url>/2020/07/28/springboot-de-qi-dong-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h3 id="spring-boot的启动流程图">spring boot的启动流程图</h3><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/qidongliucheng.png" alt="启动流程图" title="启动流程图"><br><strong>springboot的流程启动主要为三个部分</strong></p><ul><li>SpringApplication的初始化，配置基础的环境变量，构造器，监听器等。</li><li>流程启动，主要为启动流程的监听，加载配置环境，创建上下文</li><li>自动化配置</li></ul><p><strong>springboot的主启动类入口</strong></p><pre class=" language-language-java"><code class="language-language-java">@SpringBootApplication(exclude = SecurityAutoConfiguration.class)@MapperScan(value = "cn.dingdm.website.mapper")@EnableCachingpublic class WebsiteApplication {    public static void main(String[] args) {        SpringApplication.run(WebsiteApplication.class, args);    }}</code></pre><p><strong>SpringBootApplication注解详解</strong></p><pre class=" language-language-java"><code class="language-language-java">@Target({ElementType.TYPE}) //注解的适用范围，其中TYPE用于描述类、接口（包括包注解类型）或enum声明@Retention(RetentionPolicy.RUNTIME) // 注解的生命周期，保留到class文件中（三个生命周期）@Documented // 表明这个注解应该被javadoc记录@Inherited // 子类可以继承该注解@SpringBootConfiguration // 继承了Configuration，表示当前是注解类@EnableAutoConfiguration // 开启springboot的注解功能，springboot的四大神器之一，其借助@import的帮助@ComponentScan( // 扫描路径设置    excludeFilters = {@Filter(    type = FilterType.CUSTOM,    classes = {TypeExcludeFilter.class}), @Filter(    type = FilterType.CUSTOM,    classes = {AutoConfigurationExcludeFilter.class})})public @interface SpringBootApplication {</code></pre><p><strong>ElementType枚举类</strong></p><pre class=" language-language-java"><code class="language-language-java">public enum ElementType {    TYPE,    FIELD,    METHOD,    PARAMETER,    CONSTRUCTOR,    LOCAL_VARIABLE,    ANNOTATION_TYPE,    PACKAGE,    TYPE_PARAMETER,    TYPE_USE}</code></pre><p><strong>生命周期枚举类</strong></p><pre class=" language-language-java"><code class="language-language-java">public enum RetentionPolicy {    SOURCE,    CLASS,    RUNTIME}</code></pre><p><strong>1、RetentionPolicy.SOURCE：注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃；<br>2、RetentionPolicy.CLASS：注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期；<br>3、RetentionPolicy.RUNTIME：注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在；</strong><br><strong>首先要明确生命周期长度&nbsp;SOURCE &lt; CLASS &lt; RUNTIME ，所以前者能作用的地方后者一定也能作用。一般如果需要在运行时去动态获取注解信息，那只能用&nbsp;RUNTIME 注解；如果要在编译时进行一些预处理操作，比如生成一些辅助代码（如&nbsp;ButterKnife），就用&nbsp;CLASS注解；如果只是做一些检查性的操作，比如&nbsp;@Override 和&nbsp;@SuppressWarnings，则可选用&nbsp;SOURCE 注解。</strong></p><table><thead><tr><th>注解</th><th>父类</th><th>作用</th></tr></thead><tbody><tr><td>SpringBootConfiguration</td><td>Configuration</td><td>标注当前类是Java config配置类，会被扫描并加载到ioc容器</td></tr><tr><td>ComponentScan</td><td></td><td>扫描默认包或指定包下面符合条件的组件并加载</td></tr><tr><td>EnableAutoConfiguration</td><td></td><td>从classpath中搜寻所有的META-INF/spring.factories配置文件，并将其中org.springframework.boot.autoconfig.EnableAutoConfiguration对应的配置项通过反射实例化为对应的标注了@Configuration的java config形式的ioc容器配置类，汇总并加载到ioc容器</td></tr><tr><td></td><td>@AutoConfigurationPackages</td><td>注册当前主程序类的同级以及子级的包中的符合条件的Bean的定义</td></tr><tr><td></td><td>@Import(AutoConfigurationImportSelector.class)</td><td>扫描各个组件jar META-INIF目录下的spring.dactories文件，将下面的包名.类名中的工厂类全部加载到IOC容器中；将所有符合条件的bean的定义加载到ioc容器中</td></tr></tbody></table><h3 id="springboot启动">springboot启动</h3><p><strong>执行主函数main方法</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/main.png" alt="main函数" title="main函数"><br><strong>创建SpringApplication对象对象，并运行SpringApplication对象的run方法</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/chuangjianduixiang.png" alt="创建对象" title="创建对象"><br><strong>加载接口</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/jiazaijiekou.png" alt="加载接口" title="加载接口"></p><pre class=" language-language-java"><code class="language-language-java">        this.primarySources = new LinkedHashSet(Arrays.asList(primarySources));        this.webApplicationType = this.deduceWebApplicationType();       // 扫描当前路径下META-INF/spring.factories文件的，加载ApplicationContextInitializer接口实例    this.setInitializers(this.getSpringFactoriesInstances(ApplicationContextInitializer.class));// 扫描当前路径下META-INF/spring.factories文件的，加载ApplicationListener接口实例        this.setListeners(this.getSpringFactoriesInstances(ApplicationListener.class));        this.mainApplicationClass = this.deduceMainApplicationClass();</code></pre><p><strong>ApplicationContextInitializer 这个类当springboot上下文Context初始化完成后会调用<br>ApplicationListener 当springboot启动时事件change后都会触发</strong><br><strong>这两个接口均可自己进行定义</strong><br><strong>实现接口后自己配置自己的实现类</strong></p><pre class=" language-language-java"><code class="language-language-java">org.springframework.context.ApplicationContextInitializer=\org.admin.starter.test.listener.StarterApplicationContextInitializerorg.springframework.context.ApplicationListener=\org.admin.starter.test.listener.StarterApplicationListener</code></pre><p><strong>实现run方法</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/run.png" alt="run方法" title="run方法"></p><pre class=" language-language-java"><code class="language-language-java">public ConfigurableApplicationContext run(String... args) {        // 计时器StopWatch stopWatch = new StopWatch();stopWatch.start();ConfigurableApplicationContext context = null;Collection<SpringBootExceptionReporter> exceptionReporters = new ArrayList<>();        // 设置环境变量        configureHeadlessProperty();        // 获取事件监听器SpringApplicationRunListener类型，并且执行starting()方法SpringApplicationRunListeners listeners = getRunListeners(args);listeners.starting();try {            // 参数args封装成DefaultApplicationArgumentsApplicationArguments applicationArguments = new DefaultApplicationArguments(args);            // 把环境跟spring上下文绑定好，并且执行environmentPrepared()方法ConfigurableEnvironment environment = prepareEnvironment(listeners,applicationArguments);            // 判断一些环境的值，设置一些环境的值configureIgnoreBeanInfo(environment);            // 打印bannerBanner printedBanner = printBanner(environment);            // 根据项目类型创建上下文context = createApplicationContext();            // 获取异常报告事件监听exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class,new Class[] { ConfigurableApplicationContext.class }, context);            // 准备上下文，执行完成后调用contextPrepared()方法,contextLoaded()方法prepareContext(context, environment, listeners, applicationArguments,printedBanner);            // spring启动的代码，这里就回去里面就回去扫描并且初始化单实列bean了            // 这个refreshContext()加载了bean，启动了内置web容器refreshContext(context);afterRefresh(context, applicationArguments);stopWatch.stop();if (this.logStartupInfo) {new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch);}           // 执行ApplicationRunListeners中的started()方法listeners.started(context);            <!--执行Runner（ApplicationRunner和CommandLineRunner）-->callRunners(context, applicationArguments);}catch (Throwable ex) {handleRunFailure(context, listeners, exceptionReporters, ex);throw new IllegalStateException(ex);}listeners.running(context);return context;}</code></pre><p><strong>根据项目类型创建上下文，并且注入几个核心组件类。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/chuangjianshangxiawen.png" alt="创建上下文" title="创建上下文"><br><strong>refreshContext(context)方法启动spring的代码加载了bean，还启动了内置web容器</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/jiazaibean.png" alt="加载bean" title="加载bean"></p><pre class=" language-language-java"><code class="language-language-java">public void refresh() throws BeansException, IllegalStateException {        synchronized(this.startupShutdownMonitor) {            this.prepareRefresh();            ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory();            this.prepareBeanFactory(beanFactory);            try {                this.postProcessBeanFactory(beanFactory);                this.invokeBeanFactoryPostProcessors(beanFactory);                this.registerBeanPostProcessors(beanFactory);                this.initMessageSource();                this.initApplicationEventMulticaster();                this.onRefresh();                this.registerListeners();                this.finishBeanFactoryInitialization(beanFactory);                this.finishRefresh();            } catch (BeansException var9) {                if (this.logger.isWarnEnabled()) {                    this.logger.warn("Exception encountered during context initialization - cancelling refresh attempt: " + var9);                }                this.destroyBeans();                this.cancelRefresh(var9);                throw var9;            } finally {                this.resetCommonCaches();            }        }    }</code></pre><p><strong>onRefresh钩子方法</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/gouzi.png" alt=""><br>钩子方法，它会钩到它子类重写onRefresh()方法。所以去看子类里面的onRefresh()<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/zileishixian.png" alt="子类实现" title="子类实现"><br><strong>内置容器</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/neizhirongqi.png" alt="内置容器" title="内置容器"><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/neizhirongqi2.png" alt="内置容器" title="内置容器"><br><strong>容器选择</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/rongqixuanze.png" alt="容器选择" title="容器选择"><br><strong>tomcat容器</strong><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/tomcat.png" alt="tomcat" title="tomcat"></p><h3 id="springboot的自动化配置">springboot的自动化配置</h3><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/zidonghua.png" alt="自动化配置" title="自动化配置"><br><strong>该配置模块的主要使用到了SpringFactoriesLoader，即Spring工厂加载器，该对象提供了loadFactoryNames方法，入参为factoryClass和classLoader，即需要传入上图中的工厂类名称和对应的类加载器，方法会根据指定的classLoader，加载该类加器搜索路径下的指定文件，即spring.factories文件，传入的工厂类为接口，而文件中对应的类则是接口的实现类，或最终作为实现类，所以文件中一般为如下图这种一对多的类名集合，获取到这些实现类的类名后，loadFactoryNames方法返回类名集合，方法调用方得到这些集合后，再通过反射获取这些类的类对象、构造方法，最终生成实例。</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/jiekoupeizhi.png" alt="接口配置" title="接口配置"><br><strong>EnableAutoConfiguration最终实现了ImportSelector(选择器)和BeanClassLoaderAware(bean类加载器中间件)，重点关注一下AutoConfigurationImportSelector的selectImports方法</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/leixinxiliebiao.png" alt="类信息列表" title="类信息列表"><br><strong>该方法在springboot启动流程——bean实例化前被执行，返回要实例化的类信息列表。我们知道，如果获取到类信息，spring自然可以通过类加载器将类加载到jvm中，现在我们已经通过spring-boot的starter依赖方式依赖了我们需要的组件，那么这些组建的类信息在select方法中也是可以被获取到的</strong><br><strong>方法中的getCandidateConfigurations方法，通过方法注释了解到，其返回一个自动配置类的类名列表，方法调用了loadFactoryNames方法</strong></p><pre class=" language-language-java"><code class="language-language-java">    protected List<String> getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) {        List<String> configurations = SpringFactoriesLoader.loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader());        Assert.notEmpty(configurations, "No auto configuration classes found in META-INF/spring.factories. If you are using a custom packaging, make sure that file is correct.");        return configurations;    }</code></pre><p><strong>自动配置器会根据传入的factoryClass.getName()到项目系统路径下所有的spring.factories文件中找到相应的key，从而加载里面的类</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/jiazailei.png" alt="加载类" title="加载类"></p><pre class=" language-language-java"><code class="language-language-java">private static Map<String, List<String>> loadSpringFactories(@Nullable ClassLoader classLoader) {        MultiValueMap<String, String> result = (MultiValueMap)cache.get(classLoader);        if (result != null) {            return result;        } else {            try {                Enumeration<URL> urls = classLoader != null ? classLoader.getResources("META-INF/spring.factories") : ClassLoader.getSystemResources("META-INF/spring.factories");                LinkedMultiValueMap result = new LinkedMultiValueMap();                while(urls.hasMoreElements()) {                    URL url = (URL)urls.nextElement();                    UrlResource resource = new UrlResource(url);                    Properties properties = PropertiesLoaderUtils.loadProperties(resource);                    Iterator var6 = properties.entrySet().iterator();                    while(var6.hasNext()) {                        Entry<?, ?> entry = (Entry)var6.next();                        List<String> factoryClassNames = Arrays.asList(StringUtils.commaDelimitedListToStringArray((String)entry.getValue()));                        result.addAll((String)entry.getKey(), factoryClassNames);                    }                }                cache.put(classLoader, result);                return result;            } catch (IOException var9) {                throw new IllegalArgumentException("Unable to load factories from location [META-INF/spring.factories]", var9);            }        }    }</code></pre><p><strong>自动配置</strong><br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/zidongpeizhi2.png" alt="自动配置" title="自动配置"></p><pre class=" language-language-java"><code class="language-language-java">@Configuration@ConditionalOnClass({EnableAspectJAutoProxy.class, Aspect.class, Advice.class, AnnotatedElement.class})@ConditionalOnProperty(    prefix = "spring.aop",    name = {"auto"},    havingValue = "true",    matchIfMissing = true)public class AopAutoConfiguration {    public AopAutoConfiguration() {    }    @Configuration    @EnableAspectJAutoProxy(        proxyTargetClass = true    )    @ConditionalOnProperty(        prefix = "spring.aop",        name = {"proxy-target-class"},        havingValue = "true",        matchIfMissing = true    )    public static class CglibAutoProxyConfiguration {        public CglibAutoProxyConfiguration() {        }    }    @Configuration    @EnableAspectJAutoProxy(        proxyTargetClass = false    )    @ConditionalOnProperty(        prefix = "spring.aop",        name = {"proxy-target-class"},        havingValue = "false",        matchIfMissing = false    )    public static class JdkDynamicAutoProxyConfiguration {        public JdkDynamicAutoProxyConfiguration() {        }    }}</code></pre><p><strong>@Configuration，是一个通过注解标注的springBean</strong><br><strong>@ConditionalOnClass({EnableAspectJAutoProxy.class, Aspect.class, Advice.class, AnnotatedElement.class})这个注解的意思是：EnableAspectJAutoProxy.class, Aspect.class，Advice.class，AnnotatedElement.class这几个类时才解析AopAutoConfiguration配置类，否则不解析这一个配置类</strong><br><strong>因为maven依赖的传递性，我们只要依赖starter就可以依赖到所有需要自动配置的类，实现开箱即用的功能。也体现出Springboot简化了Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> springboot </tag>
            
            <tag> tomcat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络</title>
      <link href="/2020/05/08/ji-suan-ji-wang-luo/"/>
      <url>/2020/05/08/ji-suan-ji-wang-luo/</url>
      
        <content type="html"><![CDATA[<h3 id="应用进程跨越网络的通信">应用进程跨越网络的通信</h3><p><strong>解决的问题</strong>：一些特定的应用需要互联网但是不能使用标准化的互联网应用协议<br>系统调用：<br>流程：应用进程启动系统调用，控制进程传递给了系统调用接口，接口将控制权交给操作系统。即应用进程的控制权和操作系统的权限转换的接口。<br>应用进程只要使用标准的系统调用函数就可得到操作系统的服务。所以系统调用接口又称为应用编程接口。<br>其中TCP/IP协议软件驻留在操作系统中。<br>在编写java代码时有一些公用的API：<br>通过传入简单的字符串，java通过API来进行切割字符串</p><pre class=" language-language-java"><code class="language-language-java">/**  * String 类得split方法解析。  * 实际是调用Pattern类得split方法  */  public class AboutSplit {      public String[] split(String regex, int limit) {          return Pattern.compile(regex).split("本String字符串", limit);      }      /**      * 关于limit：limit决定数组的长度      * 当limit为0时数组长度不受限制,并且将剔除尾部空串      * 当limit为负时数组长度不受限制      * 当limit长度大于0，数组长度将<=limit，而且数组的最后一个元素将包含匹配符后面的所有字符      */      public String[] split(CharSequence input, int limit) {          int index = 0;          boolean matchLimited = limit > 0;          ArrayList<String> matchList = new ArrayList<String>();          Matcher m = Pattern.matcher(input); //此pattern为Pattern.compile(regex)返回的Pattern对象            // Add segments before each match found          while(m.find()) {              if (!matchLimited || matchList.size() < limit - 1) { //默认limit为0                  String match = input.subSequence(index, m.start()).toString();  //比如a:b:c，返回a。m.start()返回第一个匹配字符的索引即1                  matchList.add(match);   //存入List                  index = m.end();    //返回最后一个匹配字符的后一个字符的索引，这里是2              } else if (matchList.size() == limit - 1) { // 最后一个元素                  String match = input.subSequence(index,                                                   input.length()).toString();                  matchList.add(match);                  index = m.end();              }          }            // 没有找到匹配串          if (index == 0)              return new String[] {input.toString()};            // 把最后一部分的片段添加进来          if (!matchLimited || matchList.size() < limit)              matchList.add(input.subSequence(index, input.length()).toString());            // Construct result          int resultSize = matchList.size();          if (limit == 0) //如果limit为0              while (resultSize > 0 && matchList.get(resultSize-1).equals(""))                  resultSize--;   //剔除尾部空串          String[] result = new String[resultSize];          return matchList.subList(0, resultSize).toArray(result);      }  }  </code></pre><p>目前的应用编程接口API：套接字接口(插口接口)<br>这里需要注意一点，websocket在连接的时候有一个握手阶段，但是这和TCP的三次握手又是不一样的。TCP的三次握手是为了保证连接可靠，当TCP三次握手成功的时候，websocket的握手阶段才真正开始。TCP三次握手传送的是TCP报文，而websocket的握手传送的是HTTP报文，这个是不太一样的地方。</p><p>握手开始的时候，我们需要现发送一个HTTP 1.1的请求头部：</p><pre class=" language-language-shell"><code class="language-language-shell">GET /chat HTTP/1.1Host: server.example.comUpgrade: websocketConnection: UpgradeSec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==Origin: http://example.comSec-WebSocket-Protocol: chat, superchatSec-WebSocket-Version: 13</code></pre><p>服务端返回的成功握手请求头部如下：</p><pre class=" language-language-java"><code class="language-language-java">HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=Sec-WebSocket-Protocol: chat</code></pre><p>Upgrade:WebSocket表示这是一个特殊的 HTTP 请求，请求的目的就是要将客户端和服务器端的通讯协议从 HTTP 协议升级到 WebSocket 协议。</p><h3 id="常用的系统调用">常用的系统调用</h3><p>TCP服务：<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/f0afb365fe254a1485a3460b13776aef.png" alt=""></p><h3 id="鉴别">鉴别</h3><p><strong>概念</strong>：验证通信的对方的确是自己要通信的对象，所传送的报文是完整的。</p><h4 id="报文鉴别">报文鉴别</h4><p>密码散列函数(Hash函数)<br>特点：输入长度可以很长，但其输出长度是固定的，并且较短。<br>不同的散列值肯定对应不同的输入，不同的输入却可能得到相同的散列值。<br>一个优良的Hash函数必须满足如下几个性质:</p><ul><li>任意y，找x，使得H(x) = y，非常困难</li><li>给定x1, 找x2, 使得H(x1) == H(x2), 非常困难</li><li>找任意的x1, x2, 使得H(x1) == H(x2), 非常困难 生日定理</li></ul><p><strong>常用的Hash算法</strong><br>MD5<br>SHA1<br>SHA224<br>SHA256<br>SHA384<br>SHA512<br>SM3</p><pre class=" language-language-java"><code class="language-language-java">#!/usr/bin/python#coding:utf8import hashlibdef b2s(bytes):    rets = []    for c in bytes:        rets.append("%02X" % (ord(c)))    r = ' '.join(rets)    return rdef md5(data):    hashobj = hashlib.md5()    hashobj.update(data)    return hashobj.digest()def sha1(data):    hashobj = hashlib.sha1()    hashobj.update(data)    return hashobj.digest()def sha224(data):    hashobj = hashlib.sha224()    hashobj.update(data)    return hashobj.digest()def sha256(data):    hashobj = hashlib.sha256()    hashobj.update(data)    return hashobj.digest()def sha384(data):    hashobj = hashlib.sha384()    hashobj.update(data)    return hashobj.digest()def sha512(data):    hashobj = hashlib.sha512()    hashobj.update(data)    return hashobj.digest()def main():    hash_fun_list = (md5, sha1, sha224, sha256, sha384, sha512)    data = '\x12\x34'    for fun in hash_fun_list:        print fun        r = fun(data)        print b2s(r)if __name__ == '__main__':    main()</code></pre><h4 id="MD5">MD5</h4><p>算法过程：<br>先将报文按模2^64计算余数，追加在报文后面<br>报文和余数之填充1~512位，填充的首位时1，后面都是0<br>报文分割为一个个512位的数据块，每个512位的报文数据再分成4个128位的数据块依次送到不同的散列函数进行4轮计算，每一轮又按32位的小数据块进行运算</p><pre class=" language-language-java"><code class="language-language-java">public class MD5 {    static final String hexs[]={"0","1","2","3","4","5","6","7","8","9","A","B","C","D","E","F"};    //标准的幻数    private static final long A=0x67452301L;    private static final long B=0xefcdab89L;    private static final long C=0x98badcfeL;    private static final long D=0x10325476L;    //下面这些S11-S44实际上是一个4*4的矩阵，在四轮循环运算中用到    static final int S11 = 7;    static final int S12 = 12;    static final int S13 = 17;    static final int S14 = 22;    static final int S21 = 5;    static final int S22 = 9;    static final int S23 = 14;    static final int S24 = 20;    static final int S31 = 4;    static final int S32 = 11;    static final int S33 = 16;    static final int S34 = 23;    static final int S41 = 6;    static final int S42 = 10;    static final int S43 = 15;    static final int S44 = 21;    //java不支持无符号的基本数据（unsigned）    private long [] result={A,B,C,D};//存储hash结果，共4×32=128位，初始化值为（幻数的级联）    public static void main(String []args){        MD5 md=new MD5();        System.out.println("md5(abc)="+md.digest("abc"));    }    private String digest(String inputStr){        byte [] inputBytes=inputStr.getBytes();        int byteLen=inputBytes.length;//长度（字节）        int groupCount=0;//完整分组的个数        groupCount=byteLen/64;//每组512位（64字节）        long []groups=null;//每个小组(64字节)再细分后的16个小组(4字节)        //处理每一个完整 分组        for(int step=0;step<groupCount;step++){            groups=divGroup(inputBytes,step*64);            trans(groups);//处理分组，核心算法        }        //处理完整分组后的尾巴        int rest=byteLen%64;//512位分组后的余数        byte [] tempBytes=new byte[64];        if(rest<=56){            for(int i=0;i<rest;i++)                tempBytes[i]=inputBytes[byteLen-rest+i];            if(rest<56){                tempBytes[rest]=(byte)(1<<7);                for(int i=1;i<56-rest;i++)                    tempBytes[rest+i]=0;            }            long len=(long)(byteLen<<3);            for(int i=0;i<8;i++){                tempBytes[56+i]=(byte)(len&0xFFL);                len=len>>8;            }            groups=divGroup(tempBytes,0);            trans(groups);//处理分组        }else{            for(int i=0;i<rest;i++)                tempBytes[i]=inputBytes[byteLen-rest+i];            tempBytes[rest]=(byte)(1<<7);            for(int i=rest+1;i<64;i++)                tempBytes[i]=0;            groups=divGroup(tempBytes,0);            trans(groups);//处理分组            for(int i=0;i<56;i++)                tempBytes[i]=0;            long len=(long)(byteLen<<3);            for(int i=0;i<8;i++){                tempBytes[56+i]=(byte)(len&0xFFL);                len=len>>8;            }            groups=divGroup(tempBytes,0);            trans(groups);//处理分组        }        //将Hash值转换成十六进制的字符串        String resStr="";        long temp=0;        for(int i=0;i<4;i++){            for(int j=0;j<4;j++){                temp=result[i]&0x0FL;                String a=hexs[(int)(temp)];                result[i]=result[i]>>4;                temp=result[i]&0x0FL;                resStr+=hexs[(int)(temp)]+a;                result[i]=result[i]>>4;            }        }        return resStr;    }    /**     * 从inputBytes的index开始取512位，作为新的分组     * 将每一个512位的分组再细分成16个小组，每个小组64位（8个字节）     * @param inputBytes     * @param index     * @return     */    private static long[] divGroup(byte[] inputBytes,int index){        long [] temp=new long[16];        for(int i=0;i<16;i++){            temp[i]=b2iu(inputBytes[4*i+index])|                (b2iu(inputBytes[4*i+1+index]))<<8|                (b2iu(inputBytes[4*i+2+index]))<<16|                (b2iu(inputBytes[4*i+3+index]))<<24;        }        return temp;    }    /**     * 这时不存在符号位（符号位存储不再是代表正负），所以需要处理一下     * @param b     * @return     */    public static long b2iu(byte b){        return b < 0 ? b & 0x7F + 128 : b;     }    /**     * 主要的操作，四轮循环     * @param groups[]--每一个分组512位（64字节）     */    private void trans(long[] groups) {        long a = result[0], b = result[1], c = result[2], d = result[3];        /*第一轮*/        a = FF(a, b, c, d, groups[0], S11, 0xd76aa478L); /* 1 */        d = FF(d, a, b, c, groups[1], S12, 0xe8c7b756L); /* 2 */        c = FF(c, d, a, b, groups[2], S13, 0x242070dbL); /* 3 */        b = FF(b, c, d, a, groups[3], S14, 0xc1bdceeeL); /* 4 */        a = FF(a, b, c, d, groups[4], S11, 0xf57c0fafL); /* 5 */        d = FF(d, a, b, c, groups[5], S12, 0x4787c62aL); /* 6 */        c = FF(c, d, a, b, groups[6], S13, 0xa8304613L); /* 7 */        b = FF(b, c, d, a, groups[7], S14, 0xfd469501L); /* 8 */        a = FF(a, b, c, d, groups[8], S11, 0x698098d8L); /* 9 */        d = FF(d, a, b, c, groups[9], S12, 0x8b44f7afL); /* 10 */        c = FF(c, d, a, b, groups[10], S13, 0xffff5bb1L); /* 11 */        b = FF(b, c, d, a, groups[11], S14, 0x895cd7beL); /* 12 */        a = FF(a, b, c, d, groups[12], S11, 0x6b901122L); /* 13 */        d = FF(d, a, b, c, groups[13], S12, 0xfd987193L); /* 14 */        c = FF(c, d, a, b, groups[14], S13, 0xa679438eL); /* 15 */        b = FF(b, c, d, a, groups[15], S14, 0x49b40821L); /* 16 */        /*第二轮*/        a = GG(a, b, c, d, groups[1], S21, 0xf61e2562L); /* 17 */        d = GG(d, a, b, c, groups[6], S22, 0xc040b340L); /* 18 */        c = GG(c, d, a, b, groups[11], S23, 0x265e5a51L); /* 19 */        b = GG(b, c, d, a, groups[0], S24, 0xe9b6c7aaL); /* 20 */        a = GG(a, b, c, d, groups[5], S21, 0xd62f105dL); /* 21 */        d = GG(d, a, b, c, groups[10], S22, 0x2441453L); /* 22 */        c = GG(c, d, a, b, groups[15], S23, 0xd8a1e681L); /* 23 */        b = GG(b, c, d, a, groups[4], S24, 0xe7d3fbc8L); /* 24 */        a = GG(a, b, c, d, groups[9], S21, 0x21e1cde6L); /* 25 */        d = GG(d, a, b, c, groups[14], S22, 0xc33707d6L); /* 26 */        c = GG(c, d, a, b, groups[3], S23, 0xf4d50d87L); /* 27 */        b = GG(b, c, d, a, groups[8], S24, 0x455a14edL); /* 28 */        a = GG(a, b, c, d, groups[13], S21, 0xa9e3e905L); /* 29 */        d = GG(d, a, b, c, groups[2], S22, 0xfcefa3f8L); /* 30 */        c = GG(c, d, a, b, groups[7], S23, 0x676f02d9L); /* 31 */        b = GG(b, c, d, a, groups[12], S24, 0x8d2a4c8aL); /* 32 */        /*第三轮*/        a = HH(a, b, c, d, groups[5], S31, 0xfffa3942L); /* 33 */        d = HH(d, a, b, c, groups[8], S32, 0x8771f681L); /* 34 */        c = HH(c, d, a, b, groups[11], S33, 0x6d9d6122L); /* 35 */        b = HH(b, c, d, a, groups[14], S34, 0xfde5380cL); /* 36 */        a = HH(a, b, c, d, groups[1], S31, 0xa4beea44L); /* 37 */        d = HH(d, a, b, c, groups[4], S32, 0x4bdecfa9L); /* 38 */        c = HH(c, d, a, b, groups[7], S33, 0xf6bb4b60L); /* 39 */        b = HH(b, c, d, a, groups[10], S34, 0xbebfbc70L); /* 40 */        a = HH(a, b, c, d, groups[13], S31, 0x289b7ec6L); /* 41 */        d = HH(d, a, b, c, groups[0], S32, 0xeaa127faL); /* 42 */        c = HH(c, d, a, b, groups[3], S33, 0xd4ef3085L); /* 43 */        b = HH(b, c, d, a, groups[6], S34, 0x4881d05L); /* 44 */        a = HH(a, b, c, d, groups[9], S31, 0xd9d4d039L); /* 45 */        d = HH(d, a, b, c, groups[12], S32, 0xe6db99e5L); /* 46 */        c = HH(c, d, a, b, groups[15], S33, 0x1fa27cf8L); /* 47 */        b = HH(b, c, d, a, groups[2], S34, 0xc4ac5665L); /* 48 */        /*第四轮*/        a = II(a, b, c, d, groups[0], S41, 0xf4292244L); /* 49 */        d = II(d, a, b, c, groups[7], S42, 0x432aff97L); /* 50 */        c = II(c, d, a, b, groups[14], S43, 0xab9423a7L); /* 51 */        b = II(b, c, d, a, groups[5], S44, 0xfc93a039L); /* 52 */        a = II(a, b, c, d, groups[12], S41, 0x655b59c3L); /* 53 */        d = II(d, a, b, c, groups[3], S42, 0x8f0ccc92L); /* 54 */        c = II(c, d, a, b, groups[10], S43, 0xffeff47dL); /* 55 */        b = II(b, c, d, a, groups[1], S44, 0x85845dd1L); /* 56 */        a = II(a, b, c, d, groups[8], S41, 0x6fa87e4fL); /* 57 */        d = II(d, a, b, c, groups[15], S42, 0xfe2ce6e0L); /* 58 */        c = II(c, d, a, b, groups[6], S43, 0xa3014314L); /* 59 */        b = II(b, c, d, a, groups[13], S44, 0x4e0811a1L); /* 60 */        a = II(a, b, c, d, groups[4], S41, 0xf7537e82L); /* 61 */        d = II(d, a, b, c, groups[11], S42, 0xbd3af235L); /* 62 */        c = II(c, d, a, b, groups[2], S43, 0x2ad7d2bbL); /* 63 */        b = II(b, c, d, a, groups[9], S44, 0xeb86d391L); /* 64 */        /*加入到之前计算的结果当中*/        result[0] += a;        result[1] += b;        result[2] += c;        result[3] += d;        result[0]=result[0]&0xFFFFFFFFL;        result[1]=result[1]&0xFFFFFFFFL;        result[2]=result[2]&0xFFFFFFFFL;        result[3]=result[3]&0xFFFFFFFFL;    }    /**     * 下面是处理要用到的线性函数     */    private static long F(long x, long y, long z) {        return (x & y) | ((~x) & z);    }    private static long G(long x, long y, long z) {        return (x & z) | (y & (~z));    }    private static long H(long x, long y, long z) {        return x ^ y ^ z;    }    private static long I(long x, long y, long z) {        return y ^ (x | (~z));    }    private static long FF(long a, long b, long c, long d, long x, long s,            long ac) {        a += (F(b, c, d)&0xFFFFFFFFL) + x + ac;        a = ((a&0xFFFFFFFFL)<< s) | ((a&0xFFFFFFFFL) >>> (32 - s));        a += b;        return (a&0xFFFFFFFFL);    }    private static long GG(long a, long b, long c, long d, long x, long s,            long ac) {        a += (G(b, c, d)&0xFFFFFFFFL) + x + ac;        a = ((a&0xFFFFFFFFL) << s) | ((a&0xFFFFFFFFL) >>> (32 - s));        a += b;        return (a&0xFFFFFFFFL);    }    private static long HH(long a, long b, long c, long d, long x, long s,            long ac) {        a += (H(b, c, d)&0xFFFFFFFFL) + x + ac;        a = ((a&0xFFFFFFFFL) << s) | ((a&0xFFFFFFFFL) >>> (32 - s));        a += b;        return (a&0xFFFFFFFFL);    }    private static long II(long a, long b, long c, long d, long x, long s,            long ac) {        a += (I(b, c, d)&0xFFFFFFFFL) + x + ac;        a = ((a&0xFFFFFFFFL) << s) | ((a&0xFFFFFFFFL) >>> (32 - s));        a += b;        return (a&0xFFFFFFFFL);    }}</code></pre><h4 id="报文鉴别使用散列函数">报文鉴别使用散列函数</h4><hr><ul><li>用户A首先根据自己的明文X计算出散列H(X)，</li><li>用户A把散列H拼接在明文X的后面，生成了扩展的报文(X,H)，然后发送给B</li><li>用户B收到了这个扩展的报文(X,H)，因为散列的长度H是已知道的固定值，因此可以把收到的散列H和明文X分离开。B通过散列函数的计算，计算出明文X的散列H(X)。</li></ul><p>散列加密防止伪造报文<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/5181df2368ec4c359bc18a6737b3212f.png" alt=""></p><h3 id="密钥分配">密钥分配</h3><p>密钥管理：密码的产生、分配、注入、验证和使用</p><h4 id="对称密钥的分配">对称密钥的分配</h4><p>目前密钥分发的方式：<br>　　1、A选择一个密钥后以物理的方式传递给B<br>　　2、第三方选择密钥后物理地传给A和B<br>　　3、如果A和B先前或最近使用过一个密钥，则一方可以将 新密钥用旧密钥发送给另一方<br>　　4、如果A和B到第三方C有加密连接，则C可以在加密连接上发送密钥给A、B<br>分析：<br>1和2都需要人工交付，对链路加密（设备一对一连接）可行，对网络通信则不可行，因为网络通信涉及大量密钥<br>对于3，一旦攻击者获得一个密钥，则后序所有密钥便都不再安全。<br>4需要第三方即密钥分发中心，在网络通信中得到了广泛的应用<br>举例：</p><ol><li>你带好身份证和相应的材料去派出所，向JCSS说明你要办居住证，需要开据相应的证明。</li><li>JCSS根据他们的内部系统，核实了你提供的材料并开据了证明，上面盖有派出所的红章。</li><li>你再拿着这个证明再去社区事务中心，社区事务中心的工作人员看到了JCSS提供的证明，就可以确定你的身份，便开始给你办理业务。<br>图解：<br><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/QQ%E6%88%AA%E5%9B%BE20200805112435.png" alt=""></li></ol><ul><li>A携带A、B的身份以及唯一标识通过明文的方式向KDC发送请求</li><li>KDC通过通话密钥和收到的请求判断消息到达之前是否被篡改，然后把响应的信息用A的密钥加密返回给Ａ</li><li>KDC通过通话密钥和Ａ的身份，将Ｂ的信息用B的密钥加密然后送给Ａ</li><li>A、B进行通话验证，Ｂ使用会话密钥和唯一标识发送给Ａ，Ａ对标识进行函数变换发送给Ｂ验证</li><li>Ａ发送信息给B</li></ul><h4 id="公钥的分配">公钥的分配</h4><p><img src="https://raw.githubusercontent.com/SprBoot/image/refs/heads/main/0dec14b8796f425091844c2b97ae1400.png" alt=""></p><ul><li>Ａ向管理员发送请求，申请Ｂ的公钥</li><li>管理员通过自己的加密方式将请求响应给Ａ，Ａ通过管理员加密的方式解密来获得消息</li><li>Ａ通过Ｂ的公钥和自身的标识以及随机数加密消息发送给Ｂ<br>缺点：管理员的缺陷</li></ul><h3 id="安全协议">安全协议</h3><h4 id="SSH">SSH</h4><p><strong>概念</strong><br>传统的网络服务程序，如：ftp、pop和telnet在本质上都是不安全的，因为它们在网络上用明文传送口令和数据， 别有用心的人非常容易就可以截获这些口令和数据。而且，这些服务程序的安全验证方式也是有其弱点的， 就是很容易受到“中间人”（man-in-the-middle）这种方式的攻击。所谓“中间人”的攻击方式， 就是“中间人”冒充真正的服务器接收你的传给服务器的数据，然后再冒充你把数据传给真正的服务器。 服务器和你之间的数据传送被“中间人”一转手做了手脚之后，就会出现很严重的问题。<br><strong>两种级别的安全验证</strong><br>第一种级别（基于口令的安全验证）只要你知道自己帐号和口令，就可以登录到远程主机。所有传输的数据都会被加密， 但是不能保证你正在连接的服务器就是你想连接的服务器。可能会有别的服务器在冒充真正的服务器， 也就是受到“中间人”这种方式的攻击。<br>第二种级别（基于密匙的安全验证）需要依靠密匙，也就是你必须为自己创建一对密匙，并把公用密匙放在需要访问的服务器上。 如果你要连接到SSH服务器上，客户端软件就会向服务器发出请求，请求用你的密匙进行安全验证。服务器收到请求之后， 先在你在该服务器的家目录下寻找你的公用密匙，然后把它和你发送过来的公用密匙进行比较。如果两个密匙一致， 服务器就用公用密匙加密“质询”（challenge）并把它发送给客户端软件。 客户端软件收到“质询”之后就可以用你的私人密匙解密再把它发送给服务器。</p><h4 id="SSL">SSL</h4><p>举例：<br>当顾客想从 Web 站点购买某个产品时，顾客和 Web 站点都要进行认证。顾客通常是以提供名字和密码的方式来认证他自己。 另一方面，Web 站点通过交换一块签名数据和一个有效的 X.509 证书（作为 SSL 握手的一部分）来认证它自己。 顾客的浏览器验证该证书并用所附的公用密钥验证签名数据。一旦双方都认证了，则交易就可以开始了。<br>安全套接字层（Secure Sockets Layer（SSL）） ，SSL 是一种安全协议，它为网络（例如因特网）的通信提供私密性。SSL 使应用程序在通信时不用担心被窃听和篡改。 SSL 实际上是共同工作的两个协议：“SSL 记录协议”（SSL Record Protocol）和“SSL 握手协议” （SSL Handshake Protocol）。“SSL 记录协议”是两个协议中较低级别的协议，它为较高级别的协议， 例如 SSL 握手协议对数据的变长的记录进行加密和解密。SSL 握手协议处理应用程序凭证的交换和验证。<br>名为 A 和 B 的两台对等机想安全地进行通信。在我们简单的 p2p 应用程序的环境中，对等机 A 想查询对等机 B 上的一个资源。 每个对等机都有包含其专用密钥的一个数据库（名为 keystore）和包含其公用密钥的证书。密码保护数据库的内容。 该数据库还包含一个或多个来自被信任的对等机的自签名证书。 对等机 A 发起这项事务，每台对等机相互认证，两台对等机协商采用的密码及其长度并建立一个安全通道。完成这些操作之后， 每个对等机都知道它正在跟谁交谈并且知道通道是安全的。 SSL (Secure socket Layer)安全套接层协议主要是使用公开密钥体制和X.509数字证书技术保护信息传输的机密性和完整性， 它不能保证信息的不可抵赖性，主要适用于点对点之间的信息传输，常用Web Server方式。</p><h4 id="PKI">PKI</h4><p>PKI体系结构采用证书管理公钥，通过第三方的可信机构CA， 把用户的公钥和用户的其他标识信息（如名称、e-mail、身份证号等）捆绑在一起，在Internet网上验证用户的身份， PKI体系结构把公钥密码和对称密码结合起来，在Internet网上实现密钥的自动管理，保证网上数据的机密性、完整性。</p><h4 id="SET">SET</h4><p>SET(Secure Electronic Transaction)安全电子交易协议是由美国Visa和MasterCard两大信用卡组织提出的应用于 Internet上的以信用卡为基础的电子支付系统协议。它采用公钥密码体制和X.509数字证书标准， 主要应用于B to C模式中保障支付信息的安全性。SET协议本身比较复杂，设计比较严格，安全性高， 它能保证信息传输的机密性、真实性、完整性和不可否认性。SET协议是PKI框架下的一个典型实现，同时也在不断升级和完善， 如SET 2.0将支持借记卡电子交易。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
          <category> 加密算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
            <tag> TCP </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
